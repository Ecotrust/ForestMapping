{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# data prep and model-tuning\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# types of models we'll fit\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.multioutput import RegressorChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5089 entries, 0 to 5088\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   uuid                 5089 non-null   object \n",
      " 1   lat                  5089 non-null   float64\n",
      " 2   lon                  5089 non-null   float64\n",
      " 3   ecoregion3           5089 non-null   object \n",
      " 4   agency               5089 non-null   object \n",
      " 5   distance_to_water_m  5089 non-null   float64\n",
      " 6   plot_size_ac         5089 non-null   float64\n",
      " 7   meas_yr              5089 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 318.2+ KB\n"
     ]
    }
   ],
   "source": [
    "PLOT_DATA = '../data/processed/plot_features.csv'\n",
    "KEEP_PLOT_COLS = ['uuid', 'lat', 'lon', 'ecoregion3', 'agency', 'distance_to_water_m', 'plot_size_ac', 'meas_yr']\n",
    "plot_data = pd.read_csv(PLOT_DATA)[KEEP_PLOT_COLS]\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5411 entries, 0 to 5410\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   uuid       5411 non-null   object \n",
      " 1   elevation  5411 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 84.7+ KB\n"
     ]
    }
   ],
   "source": [
    "LIDAR_DATA = '../data/processed/lidar_features.csv'\n",
    "lidar_data = pd.read_csv(LIDAR_DATA)[['uuid', 'elevation']]\n",
    "lidar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4272 entries, 0 to 5410\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   uuid                 4272 non-null   object \n",
      " 1   lat                  4272 non-null   float64\n",
      " 2   lon                  4272 non-null   float64\n",
      " 3   ecoregion3           4272 non-null   object \n",
      " 4   agency               4272 non-null   object \n",
      " 5   distance_to_water_m  4272 non-null   float64\n",
      " 6   plot_size_ac         4272 non-null   float64\n",
      " 7   meas_yr              4272 non-null   int64  \n",
      " 8   elevation            4272 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(3)\n",
      "memory usage: 333.8+ KB\n"
     ]
    }
   ],
   "source": [
    "plot_data = plot_data.merge(lidar_data, left_on=['uuid'], right_on=['uuid'], how='inner').drop_duplicates(subset=['uuid'])\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 49410 entries, ('ba510248', 2010) to ('c4f7f099', 2025)\n",
      "Data columns (total 75 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   tpa                  49410 non-null  int64  \n",
      " 1   ba                   49410 non-null  int64  \n",
      " 2   sdi                  49410 non-null  int64  \n",
      " 3   ccf                  49410 non-null  int64  \n",
      " 4   qmd                  49410 non-null  float64\n",
      " 5   tcuft                49410 non-null  int64  \n",
      " 6   topht                49410 non-null  int64  \n",
      " 7   number_of_strata     49410 non-null  int64  \n",
      " 8   total_cover          49410 non-null  int64  \n",
      " 9   structure_class      49410 non-null  object \n",
      " 10  canopy_baseheight    49410 non-null  int64  \n",
      " 11  canopy_bulkdensity   49410 non-null  float64\n",
      " 12  aboveground_biomass  49410 non-null  int64  \n",
      " 13  aboveground_carbon   49410 non-null  int64  \n",
      " 14  gs_tpa               49410 non-null  int64  \n",
      " 15  AF                   49410 non-null  int64  \n",
      " 16  AS                   49410 non-null  int64  \n",
      " 17  BM                   49410 non-null  int64  \n",
      " 18  BO                   49410 non-null  int64  \n",
      " 19  CH                   49410 non-null  int64  \n",
      " 20  CW                   49410 non-null  int64  \n",
      " 21  DF                   49410 non-null  int64  \n",
      " 22  DG                   49410 non-null  int64  \n",
      " 23  ES                   49410 non-null  int64  \n",
      " 24  GC                   49410 non-null  int64  \n",
      " 25  GF                   49410 non-null  int64  \n",
      " 26  IC                   49410 non-null  int64  \n",
      " 27  JP                   49410 non-null  int64  \n",
      " 28  KP                   49410 non-null  int64  \n",
      " 29  LO                   49410 non-null  int64  \n",
      " 30  LP                   49410 non-null  int64  \n",
      " 31  MA                   49410 non-null  int64  \n",
      " 32  MC                   49410 non-null  int64  \n",
      " 33  MH                   49410 non-null  int64  \n",
      " 34  NF                   49410 non-null  int64  \n",
      " 35  OH                   49410 non-null  int64  \n",
      " 36  OS                   49410 non-null  int64  \n",
      " 37  OT                   49410 non-null  int64  \n",
      " 38  PC                   49410 non-null  int64  \n",
      " 39  PL                   49410 non-null  int64  \n",
      " 40  PP                   49410 non-null  int64  \n",
      " 41  PY                   49410 non-null  int64  \n",
      " 42  RA                   49410 non-null  int64  \n",
      " 43  RC                   49410 non-null  int64  \n",
      " 44  RF                   49410 non-null  int64  \n",
      " 45  SF                   49410 non-null  int64  \n",
      " 46  SH                   49410 non-null  int64  \n",
      " 47  SP                   49410 non-null  int64  \n",
      " 48  SS                   49410 non-null  int64  \n",
      " 49  TO                   49410 non-null  int64  \n",
      " 50  WA                   49410 non-null  int64  \n",
      " 51  WB                   49410 non-null  int64  \n",
      " 52  WF                   49410 non-null  int64  \n",
      " 53  WH                   49410 non-null  int64  \n",
      " 54  WI                   49410 non-null  int64  \n",
      " 55  WJ                   49410 non-null  int64  \n",
      " 56  WL                   49410 non-null  int64  \n",
      " 57  WO                   49410 non-null  int64  \n",
      " 58  WP                   49410 non-null  int64  \n",
      " 59  YC                   49410 non-null  int64  \n",
      " 60  TRUE_FIR             49410 non-null  int64  \n",
      " 61  OTHER_HARDWOOD       49410 non-null  int64  \n",
      " 62  MAPLE                49410 non-null  int64  \n",
      " 63  OAK                  49410 non-null  int64  \n",
      " 64  DOUGLAS_FIR          49410 non-null  int64  \n",
      " 65  SPRUCE               49410 non-null  int64  \n",
      " 66  CEDAR                49410 non-null  int64  \n",
      " 67  PONDEROSA_PINE       49410 non-null  int64  \n",
      " 68  OTHER_SOFTWOOD       49410 non-null  int64  \n",
      " 69  LODGEPOLE_PINE       49410 non-null  int64  \n",
      " 70  HEMLOCK              49410 non-null  int64  \n",
      " 71  RED_ALDER            49410 non-null  int64  \n",
      " 72  TANOAK               49410 non-null  int64  \n",
      " 73  JUNIPER              49410 non-null  int64  \n",
      " 74  LARCH                49410 non-null  int64  \n",
      "dtypes: float64(2), int64(72), object(1)\n",
      "memory usage: 28.5+ MB\n"
     ]
    }
   ],
   "source": [
    "INVENTORY = '../data/processed/inventory_features.csv'\n",
    "inv_data = pd.read_csv(INVENTORY, index_col=['uuid', 'year'])\n",
    "inv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 10178 entries, ('00027724', 2019) to ('fff7e1c3', 2020)\n",
      "Data columns (total 53 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   S2_R_LEAFOFF           10178 non-null  float64\n",
      " 1   S2_G_LEAFOFF           10178 non-null  float64\n",
      " 2   S2_B_LEAFOFF           10178 non-null  float64\n",
      " 3   S2_NIR_LEAFOFF         10178 non-null  float64\n",
      " 4   S2_SWIR1_LEAFOFF       10178 non-null  float64\n",
      " 5   S2_SWIR2_LEAFOFF       10178 non-null  float64\n",
      " 6   S2_RE1_LEAFOFF         10178 non-null  float64\n",
      " 7   S2_RE2_LEAFOFF         10178 non-null  float64\n",
      " 8   S2_RE3_LEAFOFF         10178 non-null  float64\n",
      " 9   S2_RE4_LEAFOFF         10178 non-null  float64\n",
      " 10  S2_R_LEAFON            10178 non-null  float64\n",
      " 11  S2_G_LEAFON            10178 non-null  float64\n",
      " 12  S2_B_LEAFON            10178 non-null  float64\n",
      " 13  S2_NIR_LEAFON          10178 non-null  float64\n",
      " 14  S2_SWIR1_LEAFON        10178 non-null  float64\n",
      " 15  S2_SWIR2_LEAFON        10178 non-null  float64\n",
      " 16  S2_RE1_LEAFON          10178 non-null  float64\n",
      " 17  S2_RE2_LEAFON          10178 non-null  float64\n",
      " 18  S2_RE3_LEAFON          10178 non-null  float64\n",
      " 19  S2_RE4_LEAFON          10178 non-null  float64\n",
      " 20  S2_NDVI_LEAFON         10178 non-null  float64\n",
      " 21  S2_SAVI_LEAFON         10178 non-null  float64\n",
      " 22  S2_BRIGHTNESS_LEAFON   10178 non-null  float64\n",
      " 23  S2_GREENNESS_LEAFON    10178 non-null  float64\n",
      " 24  S2_WETNESS_LEAFON      10178 non-null  float64\n",
      " 25  S2_NDVI_LEAFOFF        10178 non-null  float64\n",
      " 26  S2_SAVI_LEAFOFF        10178 non-null  float64\n",
      " 27  S2_BRIGHTNESS_LEAFOFF  10178 non-null  float64\n",
      " 28  S2_GREENNESS_LEAFOFF   10178 non-null  float64\n",
      " 29  S2_WETNESS_LEAFOFF     10178 non-null  float64\n",
      " 30  S2_dR                  10178 non-null  float64\n",
      " 31  S2_dG                  10178 non-null  float64\n",
      " 32  S2_dB                  10178 non-null  float64\n",
      " 33  S2_dNIR                10178 non-null  float64\n",
      " 34  S2_dSWIR1              10178 non-null  float64\n",
      " 35  S2_dSWIR2              10178 non-null  float64\n",
      " 36  S2_dRE1                10178 non-null  float64\n",
      " 37  S2_dRE2                10178 non-null  float64\n",
      " 38  S2_dNDVI               10178 non-null  float64\n",
      " 39  S2_dSAVI               10178 non-null  float64\n",
      " 40  S2_dBRIGHTNESS         10178 non-null  float64\n",
      " 41  S2_dGREENNESS          10178 non-null  float64\n",
      " 42  S2_dWETNESS            10178 non-null  float64\n",
      " 43  S2_dRE3                10178 non-null  float64\n",
      " 44  S2_dRE4                10178 non-null  float64\n",
      " 45  LT_DUR_NBR             10178 non-null  int64  \n",
      " 46  LT_DUR_SWIR1           10178 non-null  int64  \n",
      " 47  LT_MAG_NBR             10178 non-null  int64  \n",
      " 48  LT_MAG_SWIR1           10178 non-null  int64  \n",
      " 49  LT_RATE_NBR            10178 non-null  int64  \n",
      " 50  LT_RATE_SWIR1          10178 non-null  int64  \n",
      " 51  LT_YSD_NBR             10178 non-null  int64  \n",
      " 52  LT_YSD_SWIR1           10178 non-null  int64  \n",
      "dtypes: float64(45), int64(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "SATELLITE = '../data/processed/satellite_features.csv'\n",
    "sat = pd.read_csv(SATELLITE, index_col=['uuid', 'year'])\n",
    "S2_COLS = [col for col in sat.columns if col.startswith('S2')]\n",
    "LANDTRENDR_COLS = [col for col in sat.columns if col.startswith('LT')]\n",
    "sat = sat[S2_COLS + LANDTRENDR_COLS].dropna()\n",
    "sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out some of the training data\n",
    "We can exclude some of the training data based on how far separated the inventory data (interpolated using FVS simulations) is from the year the imagery was collected. Similarly, we can screen out training examples that had relatively low density of lidar returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9385 entries, 0 to 9384\n",
      "Columns: 130 entries, uuid to LARCH\n",
      "dtypes: float64(47), int64(81), object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "sat_and_inv = sat.merge(inv_data, how='inner', left_index=True, right_index=True).reset_index()\n",
    "sat_and_inv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,952 samples\n",
      "Columns: ['uuid' 'year' 'S2_R_LEAFOFF' 'S2_G_LEAFOFF' 'S2_B_LEAFOFF'\n",
      " 'S2_NIR_LEAFOFF' 'S2_SWIR1_LEAFOFF' 'S2_SWIR2_LEAFOFF' 'S2_RE1_LEAFOFF'\n",
      " 'S2_RE2_LEAFOFF' 'S2_RE3_LEAFOFF' 'S2_RE4_LEAFOFF' 'S2_R_LEAFON'\n",
      " 'S2_G_LEAFON' 'S2_B_LEAFON' 'S2_NIR_LEAFON' 'S2_SWIR1_LEAFON'\n",
      " 'S2_SWIR2_LEAFON' 'S2_RE1_LEAFON' 'S2_RE2_LEAFON' 'S2_RE3_LEAFON'\n",
      " 'S2_RE4_LEAFON' 'S2_NDVI_LEAFON' 'S2_SAVI_LEAFON' 'S2_BRIGHTNESS_LEAFON'\n",
      " 'S2_GREENNESS_LEAFON' 'S2_WETNESS_LEAFON' 'S2_NDVI_LEAFOFF'\n",
      " 'S2_SAVI_LEAFOFF' 'S2_BRIGHTNESS_LEAFOFF' 'S2_GREENNESS_LEAFOFF'\n",
      " 'S2_WETNESS_LEAFOFF' 'S2_dR' 'S2_dG' 'S2_dB' 'S2_dNIR' 'S2_dSWIR1'\n",
      " 'S2_dSWIR2' 'S2_dRE1' 'S2_dRE2' 'S2_dNDVI' 'S2_dSAVI' 'S2_dBRIGHTNESS'\n",
      " 'S2_dGREENNESS' 'S2_dWETNESS' 'S2_dRE3' 'S2_dRE4' 'LT_DUR_NBR'\n",
      " 'LT_DUR_SWIR1' 'LT_MAG_NBR' 'LT_MAG_SWIR1' 'LT_RATE_NBR' 'LT_RATE_SWIR1'\n",
      " 'LT_YSD_NBR' 'LT_YSD_SWIR1' 'tpa' 'ba' 'sdi' 'ccf' 'qmd' 'tcuft' 'topht'\n",
      " 'number_of_strata' 'total_cover' 'structure_class' 'canopy_baseheight'\n",
      " 'canopy_bulkdensity' 'aboveground_biomass' 'aboveground_carbon' 'gs_tpa'\n",
      " 'AF' 'AS' 'BM' 'BO' 'CH' 'CW' 'DF' 'DG' 'ES' 'GC' 'GF' 'IC' 'JP' 'KP'\n",
      " 'LO' 'LP' 'MA' 'MC' 'MH' 'NF' 'OH' 'OS' 'OT' 'PC' 'PL' 'PP' 'PY' 'RA'\n",
      " 'RC' 'RF' 'SF' 'SH' 'SP' 'SS' 'TO' 'WA' 'WB' 'WF' 'WH' 'WI' 'WJ' 'WL'\n",
      " 'WO' 'WP' 'YC' 'TRUE_FIR' 'OTHER_HARDWOOD' 'MAPLE' 'OAK' 'DOUGLAS_FIR'\n",
      " 'SPRUCE' 'CEDAR' 'PONDEROSA_PINE' 'OTHER_SOFTWOOD' 'LODGEPOLE_PINE'\n",
      " 'HEMLOCK' 'RED_ALDER' 'TANOAK' 'JUNIPER' 'LARCH' 'lat' 'lon' 'ecoregion3'\n",
      " 'agency' 'distance_to_water_m' 'plot_size_ac' 'meas_yr' 'elevation']\n"
     ]
    }
   ],
   "source": [
    "df = sat_and_inv.merge(plot_data, how='inner', left_on=['uuid'], right_on=['uuid']).dropna()\n",
    "print('{:,d} samples'.format(len(df)))\n",
    "print('Columns:', df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7952 entries, 0 to 7951\n",
      "Columns: 138 entries, uuid to elevation\n",
      "dtypes: float64(52), int64(82), object(4)\n",
      "memory usage: 8.4+ MB\n"
     ]
    }
   ],
   "source": [
    "OUTLIERS = '../data/interim/outlier_uuids.csv'\n",
    "outliers = pd.read_csv(OUTLIERS)\n",
    "# filter out the height outliers\n",
    "df = df[~df.uuid.isin(outliers.outlier_uuid)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7477 entries, 0 to 7951\n",
      "Columns: 138 entries, uuid to elevation\n",
      "dtypes: float64(52), int64(82), object(4)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[(df.topht > 0)&(df.total_cover >= 10)&(df.qmd > 0)]\n",
    "df.loc[df.qmd > 50, 'qmd'] = 50\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/sentinel_structure_training_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect how many samples we have for different years, regions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>3979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>3498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "year       \n",
       "2019   3979\n",
       "2020   3498"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['year'])[['uuid']].count().rename({'uuid':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meas_yr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>392</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>739</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>451</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>444</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>650</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year     2019  2020\n",
       "meas_yr            \n",
       "2010      487     0\n",
       "2011      392   392\n",
       "2013      739   741\n",
       "2014      451   452\n",
       "2015      444   445\n",
       "2016      650   651\n",
       "2017      483   484\n",
       "2018      333   333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df, \n",
    "               values='uuid', \n",
    "               aggfunc='count', \n",
    "               index=['meas_yr'], \n",
    "               columns=['year'], \n",
    "               fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>year</th>\n",
       "      <th>plot_size_ac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecoregion3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>704</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>1760</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>columbia_plateau</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>415</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>411</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    uuid  year  plot_size_ac\n",
       "ecoregion3                                                                  \n",
       "blue_mountains                                        87     2             1\n",
       "cascades                                             704     2             3\n",
       "coast_range                                         1760     2             2\n",
       "columbia_plateau                                      10     2             1\n",
       "eastern_cascades_slopes_and_foothills                415     2             2\n",
       "klamath_mountains_california_high_north_coast_r...   291     2             2\n",
       "north_cascades                                       411     2             2\n",
       "northern_rockies                                     103     2             1\n",
       "puget_lowland                                        159     2             1\n",
       "willamette_valley                                     45     2             3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoreg_counts = df.groupby(by=['ecoregion3'])[['uuid', 'year', 'plot_size_ac']].nunique()\n",
    "ecoreg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available features\n",
    "The different types of predictor variables we can use to predict a forest attribute, including climate, lidar-derived, soil, and satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S2_R_LEAFOFF</th>\n",
       "      <th>S2_G_LEAFOFF</th>\n",
       "      <th>S2_B_LEAFOFF</th>\n",
       "      <th>S2_NIR_LEAFOFF</th>\n",
       "      <th>S2_SWIR1_LEAFOFF</th>\n",
       "      <th>S2_SWIR2_LEAFOFF</th>\n",
       "      <th>S2_RE1_LEAFOFF</th>\n",
       "      <th>S2_RE2_LEAFOFF</th>\n",
       "      <th>S2_RE3_LEAFOFF</th>\n",
       "      <th>S2_RE4_LEAFOFF</th>\n",
       "      <th>...</th>\n",
       "      <th>S2_dRE3</th>\n",
       "      <th>S2_dRE4</th>\n",
       "      <th>LT_DUR_NBR</th>\n",
       "      <th>LT_DUR_SWIR1</th>\n",
       "      <th>LT_MAG_NBR</th>\n",
       "      <th>LT_MAG_SWIR1</th>\n",
       "      <th>LT_RATE_NBR</th>\n",
       "      <th>LT_RATE_SWIR1</th>\n",
       "      <th>LT_YSD_NBR</th>\n",
       "      <th>LT_YSD_SWIR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>531.694289</td>\n",
       "      <td>605.416149</td>\n",
       "      <td>505.358747</td>\n",
       "      <td>2237.596022</td>\n",
       "      <td>707.922248</td>\n",
       "      <td>375.124320</td>\n",
       "      <td>857.882566</td>\n",
       "      <td>1819.585928</td>\n",
       "      <td>2082.188337</td>\n",
       "      <td>2248.629412</td>\n",
       "      <td>...</td>\n",
       "      <td>411.260546</td>\n",
       "      <td>461.857058</td>\n",
       "      <td>18.199010</td>\n",
       "      <td>14.429183</td>\n",
       "      <td>76.050823</td>\n",
       "      <td>279.592751</td>\n",
       "      <td>16.830814</td>\n",
       "      <td>311.751103</td>\n",
       "      <td>28.655343</td>\n",
       "      <td>26.740671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>871.376845</td>\n",
       "      <td>871.553734</td>\n",
       "      <td>890.183177</td>\n",
       "      <td>867.859630</td>\n",
       "      <td>309.202564</td>\n",
       "      <td>224.237164</td>\n",
       "      <td>881.365459</td>\n",
       "      <td>863.581571</td>\n",
       "      <td>837.038646</td>\n",
       "      <td>805.090546</td>\n",
       "      <td>...</td>\n",
       "      <td>916.861539</td>\n",
       "      <td>867.560133</td>\n",
       "      <td>12.229163</td>\n",
       "      <td>14.951374</td>\n",
       "      <td>191.083720</td>\n",
       "      <td>636.569985</td>\n",
       "      <td>35.910229</td>\n",
       "      <td>525.947378</td>\n",
       "      <td>10.411304</td>\n",
       "      <td>9.833718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.797190</td>\n",
       "      <td>26.922216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>348.355904</td>\n",
       "      <td>91.010674</td>\n",
       "      <td>42.869246</td>\n",
       "      <td>70.049804</td>\n",
       "      <td>257.344712</td>\n",
       "      <td>311.294348</td>\n",
       "      <td>379.008394</td>\n",
       "      <td>...</td>\n",
       "      <td>-5838.540354</td>\n",
       "      <td>-5129.971255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-734.000000</td>\n",
       "      <td>-3098.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>-390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>137.034631</td>\n",
       "      <td>224.795087</td>\n",
       "      <td>120.027203</td>\n",
       "      <td>1692.104687</td>\n",
       "      <td>524.976136</td>\n",
       "      <td>247.182184</td>\n",
       "      <td>433.972900</td>\n",
       "      <td>1328.277211</td>\n",
       "      <td>1572.159461</td>\n",
       "      <td>1742.031190</td>\n",
       "      <td>...</td>\n",
       "      <td>168.669551</td>\n",
       "      <td>183.927594</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-134.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>211.648345</td>\n",
       "      <td>307.034381</td>\n",
       "      <td>188.857381</td>\n",
       "      <td>2100.699373</td>\n",
       "      <td>649.669441</td>\n",
       "      <td>309.616828</td>\n",
       "      <td>564.841457</td>\n",
       "      <td>1624.619934</td>\n",
       "      <td>1924.779642</td>\n",
       "      <td>2118.979181</td>\n",
       "      <td>...</td>\n",
       "      <td>482.779503</td>\n",
       "      <td>503.855434</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>497.018692</td>\n",
       "      <td>532.594164</td>\n",
       "      <td>418.635884</td>\n",
       "      <td>2588.109086</td>\n",
       "      <td>809.862578</td>\n",
       "      <td>421.175325</td>\n",
       "      <td>863.546154</td>\n",
       "      <td>2041.937584</td>\n",
       "      <td>2380.962769</td>\n",
       "      <td>2586.793360</td>\n",
       "      <td>...</td>\n",
       "      <td>830.845174</td>\n",
       "      <td>867.817089</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8672.332090</td>\n",
       "      <td>9067.047264</td>\n",
       "      <td>9251.702460</td>\n",
       "      <td>7712.457830</td>\n",
       "      <td>3020.551859</td>\n",
       "      <td>2062.405424</td>\n",
       "      <td>8807.240741</td>\n",
       "      <td>8250.834024</td>\n",
       "      <td>7530.579326</td>\n",
       "      <td>7091.325768</td>\n",
       "      <td>...</td>\n",
       "      <td>4035.860442</td>\n",
       "      <td>4245.146064</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>3133.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>3133.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S2_R_LEAFOFF  S2_G_LEAFOFF  S2_B_LEAFOFF  S2_NIR_LEAFOFF  \\\n",
       "count   7477.000000   7477.000000   7477.000000     7477.000000   \n",
       "mean     531.694289    605.416149    505.358747     2237.596022   \n",
       "std      871.376845    871.553734    890.183177      867.859630   \n",
       "min        9.797190     26.922216      1.000000      348.355904   \n",
       "25%      137.034631    224.795087    120.027203     1692.104687   \n",
       "50%      211.648345    307.034381    188.857381     2100.699373   \n",
       "75%      497.018692    532.594164    418.635884     2588.109086   \n",
       "max     8672.332090   9067.047264   9251.702460     7712.457830   \n",
       "\n",
       "       S2_SWIR1_LEAFOFF  S2_SWIR2_LEAFOFF  S2_RE1_LEAFOFF  S2_RE2_LEAFOFF  \\\n",
       "count       7477.000000       7477.000000     7477.000000     7477.000000   \n",
       "mean         707.922248        375.124320      857.882566     1819.585928   \n",
       "std          309.202564        224.237164      881.365459      863.581571   \n",
       "min           91.010674         42.869246       70.049804      257.344712   \n",
       "25%          524.976136        247.182184      433.972900     1328.277211   \n",
       "50%          649.669441        309.616828      564.841457     1624.619934   \n",
       "75%          809.862578        421.175325      863.546154     2041.937584   \n",
       "max         3020.551859       2062.405424     8807.240741     8250.834024   \n",
       "\n",
       "       S2_RE3_LEAFOFF  S2_RE4_LEAFOFF  ...      S2_dRE3      S2_dRE4  \\\n",
       "count     7477.000000     7477.000000  ...  7477.000000  7477.000000   \n",
       "mean      2082.188337     2248.629412  ...   411.260546   461.857058   \n",
       "std        837.038646      805.090546  ...   916.861539   867.560133   \n",
       "min        311.294348      379.008394  ... -5838.540354 -5129.971255   \n",
       "25%       1572.159461     1742.031190  ...   168.669551   183.927594   \n",
       "50%       1924.779642     2118.979181  ...   482.779503   503.855434   \n",
       "75%       2380.962769     2586.793360  ...   830.845174   867.817089   \n",
       "max       7530.579326     7091.325768  ...  4035.860442  4245.146064   \n",
       "\n",
       "        LT_DUR_NBR  LT_DUR_SWIR1   LT_MAG_NBR  LT_MAG_SWIR1  LT_RATE_NBR  \\\n",
       "count  7477.000000   7477.000000  7477.000000   7477.000000  7477.000000   \n",
       "mean     18.199010     14.429183    76.050823    279.592751    16.830814   \n",
       "std      12.229163     14.951374   191.083720    636.569985    35.910229   \n",
       "min       1.000000      1.000000  -734.000000  -3098.000000  -116.000000   \n",
       "25%       6.000000      1.000000    -5.000000   -134.000000     0.000000   \n",
       "50%      19.000000      4.000000    40.000000    151.000000     3.000000   \n",
       "75%      27.000000     35.000000   126.000000    521.000000    17.000000   \n",
       "max      36.000000     36.000000  1071.000000   3133.000000   268.000000   \n",
       "\n",
       "       LT_RATE_SWIR1   LT_YSD_NBR  LT_YSD_SWIR1  \n",
       "count    7477.000000  7477.000000   7477.000000  \n",
       "mean      311.751103    28.655343     26.740671  \n",
       "std       525.947378    10.411304      9.833718  \n",
       "min      -390.000000     0.000000      0.000000  \n",
       "25%        -5.000000    28.000000     22.000000  \n",
       "50%        30.000000    34.000000     31.000000  \n",
       "75%       397.000000    35.000000     34.000000  \n",
       "max      3133.000000    35.000000     35.000000  \n",
       "\n",
       "[8 rows x 53 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[S2_COLS + LANDTRENDR_COLS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and targets\n",
    "This is the first step in determining what features we want to use, and what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = S2_COLS + LANDTRENDR_COLS + ['elevation', 'lat', 'lon'] + ['ecoregion3'] \n",
    "Y_COLS = ['total_cover', 'topht', 'qmd', 'tcuft']\n",
    "\n",
    "Y_NAMES = [col.upper() for col in Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S2_R_LEAFOFF',\n",
       " 'S2_G_LEAFOFF',\n",
       " 'S2_B_LEAFOFF',\n",
       " 'S2_NIR_LEAFOFF',\n",
       " 'S2_SWIR1_LEAFOFF',\n",
       " 'S2_SWIR2_LEAFOFF',\n",
       " 'S2_RE1_LEAFOFF',\n",
       " 'S2_RE2_LEAFOFF',\n",
       " 'S2_RE3_LEAFOFF',\n",
       " 'S2_RE4_LEAFOFF',\n",
       " 'S2_R_LEAFON',\n",
       " 'S2_G_LEAFON',\n",
       " 'S2_B_LEAFON',\n",
       " 'S2_NIR_LEAFON',\n",
       " 'S2_SWIR1_LEAFON',\n",
       " 'S2_SWIR2_LEAFON',\n",
       " 'S2_RE1_LEAFON',\n",
       " 'S2_RE2_LEAFON',\n",
       " 'S2_RE3_LEAFON',\n",
       " 'S2_RE4_LEAFON',\n",
       " 'S2_NDVI_LEAFON',\n",
       " 'S2_SAVI_LEAFON',\n",
       " 'S2_BRIGHTNESS_LEAFON',\n",
       " 'S2_GREENNESS_LEAFON',\n",
       " 'S2_WETNESS_LEAFON',\n",
       " 'S2_NDVI_LEAFOFF',\n",
       " 'S2_SAVI_LEAFOFF',\n",
       " 'S2_BRIGHTNESS_LEAFOFF',\n",
       " 'S2_GREENNESS_LEAFOFF',\n",
       " 'S2_WETNESS_LEAFOFF',\n",
       " 'S2_dR',\n",
       " 'S2_dG',\n",
       " 'S2_dB',\n",
       " 'S2_dNIR',\n",
       " 'S2_dSWIR1',\n",
       " 'S2_dSWIR2',\n",
       " 'S2_dRE1',\n",
       " 'S2_dRE2',\n",
       " 'S2_dNDVI',\n",
       " 'S2_dSAVI',\n",
       " 'S2_dBRIGHTNESS',\n",
       " 'S2_dGREENNESS',\n",
       " 'S2_dWETNESS',\n",
       " 'S2_dRE3',\n",
       " 'S2_dRE4',\n",
       " 'LT_DUR_NBR',\n",
       " 'LT_DUR_SWIR1',\n",
       " 'LT_MAG_NBR',\n",
       " 'LT_MAG_SWIR1',\n",
       " 'LT_RATE_NBR',\n",
       " 'LT_RATE_SWIR1',\n",
       " 'LT_YSD_NBR',\n",
       " 'LT_YSD_SWIR1',\n",
       " 'elevation',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'ecoregion3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_cover</th>\n",
       "      <th>topht</th>\n",
       "      <th>qmd</th>\n",
       "      <th>tcuft</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecoregion3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>33.2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2016.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>70.6</td>\n",
       "      <td>110.7</td>\n",
       "      <td>16.6</td>\n",
       "      <td>9783.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>63.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>6119.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>65.4</td>\n",
       "      <td>103.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>8039.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>60.8</td>\n",
       "      <td>93.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>7284.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>43.0</td>\n",
       "      <td>72.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3542.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>42.4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>2741.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>68.7</td>\n",
       "      <td>92.1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6325.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>64.2</td>\n",
       "      <td>113.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>9516.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    total_cover  topht   qmd  \\\n",
       "ecoregion3                                                                     \n",
       "blue_mountains                                             33.2   61.5  13.2   \n",
       "coast_range                                                70.6  110.7  16.6   \n",
       "north_cascades                                             63.0   87.5  12.3   \n",
       "cascades                                                   65.4  103.5  13.8   \n",
       "klamath_mountains_california_high_north_coast_r...         60.8   93.9  11.2   \n",
       "eastern_cascades_slopes_and_foothills                      43.0   72.9  12.3   \n",
       "northern_rockies                                           42.4   75.0  11.7   \n",
       "puget_lowland                                              68.7   92.1  12.8   \n",
       "willamette_valley                                          64.2  113.0  18.8   \n",
       "\n",
       "                                                     tcuft  \n",
       "ecoregion3                                                  \n",
       "blue_mountains                                      2016.9  \n",
       "coast_range                                         9783.9  \n",
       "north_cascades                                      6119.7  \n",
       "cascades                                            8039.2  \n",
       "klamath_mountains_california_high_north_coast_r...  7284.3  \n",
       "eastern_cascades_slopes_and_foothills               3542.5  \n",
       "northern_rockies                                    2741.1  \n",
       "puget_lowland                                       6325.9  \n",
       "willamette_valley                                   9516.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_cover</th>\n",
       "      <th>topht</th>\n",
       "      <th>qmd</th>\n",
       "      <th>tcuft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "      <td>7477.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.105256</td>\n",
       "      <td>98.350140</td>\n",
       "      <td>14.383718</td>\n",
       "      <td>7631.583523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.035939</td>\n",
       "      <td>38.580756</td>\n",
       "      <td>7.930582</td>\n",
       "      <td>6280.335812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.045915</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>8.842887</td>\n",
       "      <td>2849.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>12.913312</td>\n",
       "      <td>6076.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>17.886864</td>\n",
       "      <td>10763.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51330.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_cover        topht          qmd         tcuft\n",
       "count  7477.000000  7477.000000  7477.000000   7477.000000\n",
       "mean     63.105256    98.350140    14.383718   7631.583523\n",
       "std      21.035939    38.580756     7.930582   6280.335812\n",
       "min      10.000000    11.000000     2.045915     17.000000\n",
       "25%      50.000000    70.000000     8.842887   2849.000000\n",
       "50%      67.000000    96.000000    12.913312   6076.000000\n",
       "75%      79.000000   124.000000    17.886864  10763.000000\n",
       "max     100.000000   267.000000    50.000000  51330.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_REGIONS = ['blue_mountains', 'coast_range', 'north_cascades', 'cascades',\n",
    "               'klamath_mountains_california_high_north_coast_range', \n",
    "               'eastern_cascades_slopes_and_foothills', 'northern_rockies',\n",
    "               'puget_lowland', 'willamette_valley']\n",
    "display(df.groupby('ecoregion3')[Y_COLS].mean().round(1).loc[USE_REGIONS])\n",
    "display(df[Y_COLS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "X, Y = df[X_COLS], df[Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7477 entries, 0 to 7476\n",
      "Data columns (total 57 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   S2_R_LEAFOFF           7477 non-null   float64\n",
      " 1   S2_G_LEAFOFF           7477 non-null   float64\n",
      " 2   S2_B_LEAFOFF           7477 non-null   float64\n",
      " 3   S2_NIR_LEAFOFF         7477 non-null   float64\n",
      " 4   S2_SWIR1_LEAFOFF       7477 non-null   float64\n",
      " 5   S2_SWIR2_LEAFOFF       7477 non-null   float64\n",
      " 6   S2_RE1_LEAFOFF         7477 non-null   float64\n",
      " 7   S2_RE2_LEAFOFF         7477 non-null   float64\n",
      " 8   S2_RE3_LEAFOFF         7477 non-null   float64\n",
      " 9   S2_RE4_LEAFOFF         7477 non-null   float64\n",
      " 10  S2_R_LEAFON            7477 non-null   float64\n",
      " 11  S2_G_LEAFON            7477 non-null   float64\n",
      " 12  S2_B_LEAFON            7477 non-null   float64\n",
      " 13  S2_NIR_LEAFON          7477 non-null   float64\n",
      " 14  S2_SWIR1_LEAFON        7477 non-null   float64\n",
      " 15  S2_SWIR2_LEAFON        7477 non-null   float64\n",
      " 16  S2_RE1_LEAFON          7477 non-null   float64\n",
      " 17  S2_RE2_LEAFON          7477 non-null   float64\n",
      " 18  S2_RE3_LEAFON          7477 non-null   float64\n",
      " 19  S2_RE4_LEAFON          7477 non-null   float64\n",
      " 20  S2_NDVI_LEAFON         7477 non-null   float64\n",
      " 21  S2_SAVI_LEAFON         7477 non-null   float64\n",
      " 22  S2_BRIGHTNESS_LEAFON   7477 non-null   float64\n",
      " 23  S2_GREENNESS_LEAFON    7477 non-null   float64\n",
      " 24  S2_WETNESS_LEAFON      7477 non-null   float64\n",
      " 25  S2_NDVI_LEAFOFF        7477 non-null   float64\n",
      " 26  S2_SAVI_LEAFOFF        7477 non-null   float64\n",
      " 27  S2_BRIGHTNESS_LEAFOFF  7477 non-null   float64\n",
      " 28  S2_GREENNESS_LEAFOFF   7477 non-null   float64\n",
      " 29  S2_WETNESS_LEAFOFF     7477 non-null   float64\n",
      " 30  S2_dR                  7477 non-null   float64\n",
      " 31  S2_dG                  7477 non-null   float64\n",
      " 32  S2_dB                  7477 non-null   float64\n",
      " 33  S2_dNIR                7477 non-null   float64\n",
      " 34  S2_dSWIR1              7477 non-null   float64\n",
      " 35  S2_dSWIR2              7477 non-null   float64\n",
      " 36  S2_dRE1                7477 non-null   float64\n",
      " 37  S2_dRE2                7477 non-null   float64\n",
      " 38  S2_dNDVI               7477 non-null   float64\n",
      " 39  S2_dSAVI               7477 non-null   float64\n",
      " 40  S2_dBRIGHTNESS         7477 non-null   float64\n",
      " 41  S2_dGREENNESS          7477 non-null   float64\n",
      " 42  S2_dWETNESS            7477 non-null   float64\n",
      " 43  S2_dRE3                7477 non-null   float64\n",
      " 44  S2_dRE4                7477 non-null   float64\n",
      " 45  LT_DUR_NBR             7477 non-null   int64  \n",
      " 46  LT_DUR_SWIR1           7477 non-null   int64  \n",
      " 47  LT_MAG_NBR             7477 non-null   int64  \n",
      " 48  LT_MAG_SWIR1           7477 non-null   int64  \n",
      " 49  LT_RATE_NBR            7477 non-null   int64  \n",
      " 50  LT_RATE_SWIR1          7477 non-null   int64  \n",
      " 51  LT_YSD_NBR             7477 non-null   int64  \n",
      " 52  LT_YSD_SWIR1           7477 non-null   int64  \n",
      " 53  elevation              7477 non-null   float64\n",
      " 54  lat                    7477 non-null   float64\n",
      " 55  lon                    7477 non-null   float64\n",
      " 56  ecoregion3             7477 non-null   object \n",
      "dtypes: float64(48), int64(8), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df[X_COLS].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets by ecoregion\n",
    "We want to explore model transferability between regions, so we'll train models independently on subsets of the data within a single ecoregion, as well as a model that is trained on all available ecoregions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregions = list(np.sort([reg for reg in pd.unique(df.ecoregion3) if ecoreg_counts.loc[reg]['uuid'] > 20]))\n",
    "\n",
    "eco_X_idx = [X.loc[X.ecoregion3 == eco].index.values for eco in ecoregions]\n",
    "\n",
    "eco_X_dfs = [X.loc[X.ecoregion3 == eco].drop(['ecoregion3'], axis=1) for eco in ecoregions]\n",
    "eco_Y_dfs = [Y.loc[idx] for idx in eco_X_idx]\n",
    "\n",
    "# append a \"global\" model that contains data from all ecoregions\n",
    "ecoregions.append('all')\n",
    "ecoregion_names = ['_'.join(x.split('_')[0:2]) for x in ecoregions]\n",
    "eco_X_dfs.append(X.drop(['ecoregion3'], axis=1))\n",
    "eco_Y_dfs.append(Y)\n",
    "\n",
    "ecoregion_display_names = [' '.join(x.upper().split('_')[:2]) for x in ecoregions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_class_bins = [10,40,70,100]\n",
    "cover_class_labels = ['OPEN', 'MODERATE', 'CLOSED']\n",
    "height_class_bins = np.arange(0,300,20)\n",
    "height_class_labels = [f'{x}-{x+20}' for x in height_class_bins[:-1]]\n",
    "diameter_class_bins = [1, 5, 10, 15, 20, 999]\n",
    "diameter_class_labels = ['SEED/SAP', 'SMALL', 'MEDIUM', 'LARGE', 'VERY_LARGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "We'll use Root Mean Square Error to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, pred):\n",
    "    return np.sqrt((np.square(obs-pred)).mean())\n",
    "\n",
    "def nrmse(obs, pred):\n",
    "    return rmse(pred,obs) / obs.mean()\n",
    "\n",
    "def mae(obs, pred):   \n",
    "    return abs(pred - obs).mean()\n",
    "\n",
    "def mape(obs, pred):    \n",
    "    return abs(pred - obs).mean() / obs.mean()\n",
    "\n",
    "def bias(obs, pred):   \n",
    "    return (pred - obs).mean()\n",
    "\n",
    "def rel_bias(obs, pred):\n",
    "    return bias(pred,obs) / obs.mean()\n",
    "\n",
    "def bin_accuracy(obs, pred, bins, fuzzy_tol=0):\n",
    "    pred_binned = np.digitize(pred, bins)\n",
    "    obs_binned = np.digitisze(obs, bins)\n",
    "    diff = abs(pred_binned - obs_binned)\n",
    "    \n",
    "    return (diff <= fuzzy_tol).sum() / len(diff)\n",
    "\n",
    "def confidence_interval_half(X, confidence=0.95):\n",
    "    n = len(X)\n",
    "    se = stats.sem(X)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function will calculate RMSE scores for each regionally-trained model and the global model on each ecoregion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit some models\n",
    "For each type of model, we'll employ cross-validation to tune model hyperparameters, generating a tuned model for each ecoregion as well as a tuned model using all training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Lasso': Lasso(), \n",
    "    'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_jobs=-1), \n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(), \n",
    "}\n",
    "\n",
    "FIT_PARAMS = {\n",
    "    'ElasticNet': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "        'l1_ratio': np.arange(0.0, 1.0, 0.1),\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [1,2,3,4,5,10,20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'max_depth': [5, 20, None],\n",
    "        'max_samples': [0.5, None]\n",
    "    },\n",
    "    'HistGradientBoostingRegressor': {\n",
    "        'max_iter': [50, 100, 200],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTER_FOLDS = 5\n",
    "NUM_INNER_FOLDS = 3\n",
    "SCORE_FUNCS = [rmse, nrmse, mae, mape, bias, rel_bias]\n",
    "score_names = [func.__name__ for func in SCORE_FUNCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_insider_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['cv_results'] = {}\n",
    "            for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "                fold_num = fold_idx + 1\n",
    "                results[region][model_name]['cv_results'][fold_num] = {}\n",
    "                results[region][model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "                results[region][model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[region][model_name]['cv_results'][fold_num][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_global_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for model_name in model_names:\n",
    "        results[model_name] = {}\n",
    "        results[model_name]['fitted_model'] = None\n",
    "        results[model_name]['best_params'] = None\n",
    "        results[model_name]['cv_results'] = {}\n",
    "        for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "            fold_num = fold_idx + 1\n",
    "            results[model_name]['cv_results'][fold_num] = {}\n",
    "            results[model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "            results[model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "            for region in regions:\n",
    "                results[model_name]['cv_results'][fold_num][region] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[model_name]['cv_results'][fold_num][region][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_outsider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['predict_time'] = None\n",
    "            for score_func in score_funcs:\n",
    "                score_func_name = score_func.__name__\n",
    "                results[region][model_name][score_func_name] = {\n",
    "                    y: None for y in target_vars\n",
    "                }\n",
    "    return results\n",
    "\n",
    "def build_visiting_insider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for target_region in regions:\n",
    "        results[target_region] = {}\n",
    "        for train_region in [r for r in regions if r != target_region]:\n",
    "            results[target_region][train_region] = {}\n",
    "            for model_name in model_names:\n",
    "                results[target_region][train_region][model_name] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[target_region][train_region][model_name][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_insider_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X = eco_X_dfs[i]\n",
    "        Y = eco_Y_dfs[i]\n",
    "        outer_groups = df.loc[X.index, 'uuid'].values\n",
    "        \n",
    "        outer_fold_num = 1\n",
    "        for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "            X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "            Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "            inner_groups = df.loc[X_train.index, 'uuid'].values\n",
    "            \n",
    "            inner_search = GridSearchCV(pipe, search_params, \n",
    "                                        scoring='neg_mean_squared_error', \n",
    "                                        n_jobs=-1, cv=cv_inner, refit=True)\n",
    "            \n",
    "            inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "            \n",
    "            inner_best_model = inner_result.best_estimator_\n",
    "            start_time = time.time()\n",
    "            Y_pred = inner_best_model.predict(X_test)\n",
    "            end_time = time.time()\n",
    "            total_predict_time = end_time - start_time\n",
    "            avg_predict_time = total_predict_time / len(X_test)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(Y_test, Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    insider_results[ecoregion][model_name]['cv_results'][outer_fold_num][score_func_name][y_var] = scores.loc[y_var]\n",
    "                    \n",
    "            print(outer_fold_num, end='... ')\n",
    "            outer_fold_num += 1\n",
    "        print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "        # done with scoring of models, now time to tune a model using the whole dataset\n",
    "        outer_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_outer, refit=True)\n",
    "        outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "        \n",
    "        # now fit on the entire dataset, not just training set\n",
    "        model = outer_result.best_estimator_\n",
    "        model.set_params(**outer_result.best_params_)\n",
    "        X = df.loc[df.ecoregion3 == ecoregion, X_COLS].drop(['ecoregion3'], axis=1)\n",
    "        y = df.loc[df.ecoregion3 == ecoregion, Y_COLS]\n",
    "        model.fit(X, y)\n",
    "\n",
    "        eco_name = '_'.join(ecoregion.split('_')[:2])\n",
    "        outfile = f'{eco_name}-sentinel-{model_name}-chained.pkl'\n",
    "        outpath = os.path.join('../models/structure_models', outfile)\n",
    "        with open(outpath, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "        insider_results[ecoregion][model_name]['fitted_model'] = model\n",
    "        insider_results[ecoregion][model_name]['best_params'] = outer_result.best_params_\n",
    "        \n",
    "        cv_results_dict = {ecoregion: insider_results[ecoregion][model_name]['cv_results'] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return cv_results_dict\n",
    "\n",
    "def tune_outsider_model(model_name, num_folds=5):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    groupkfold = GroupKFold(num_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X_train = X.loc[X.ecoregion3 != ecoregion].drop('ecoregion3', axis=1)\n",
    "        Y_train = Y.loc[X_train.index]\n",
    "        X_test = X.loc[X.ecoregion3 == ecoregion].drop('ecoregion3', axis=1)\n",
    "        Y_test = Y.loc[X_test.index]\n",
    "        groups = df.loc[X_train.index]['ecoregion3'].values\n",
    "        \n",
    "        search = GridSearchCV(pipe, search_params, \n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1, cv=groupkfold, refit=True)\n",
    "            \n",
    "        result = search.fit(X_train, Y_train, groups=groups)\n",
    "        print('Done fitting, now scoring', end='... ')\n",
    "        outsider_results[ecoregion][model_name]['best_params'] = result.best_params_\n",
    "        outsider_results[ecoregion][model_name]['fitted_model'] = result.best_estimator_\n",
    "        \n",
    "        best_model = result.best_estimator_       \n",
    "        start_time = time.time()\n",
    "        Y_pred = best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        outsider_results[ecoregion][model_name]['predict_time'] = avg_predict_time\n",
    "            \n",
    "        for score_func in SCORE_FUNCS:\n",
    "            score_func_name = score_func.__name__\n",
    "            scores = score_func(Y_test, Y_pred)\n",
    "            for y_var in scores.index:\n",
    "                outsider_results[ecoregion][model_name][score_func_name][y_var] = scores.loc[y_var]\n",
    "        \n",
    "        results_dict = {ecoregion: outsider_results[ecoregion][model_name] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def tune_global_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    print(f'Scoring with {NUM_OUTER_FOLDS} folds... ', end='')\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    test_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    X = df[X_COLS].drop('ecoregion3', axis=1)\n",
    "    Y = df[Y_COLS]\n",
    "    outer_groups = df['uuid'].values\n",
    "        \n",
    "    outer_fold_num = 1\n",
    "    for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "        X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "        Y_train, Y_test = Y.loc[train_ix], Y.loc[test_ix]\n",
    "        inner_groups = df.loc[train_ix, 'uuid'].values\n",
    "\n",
    "        inner_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "        inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "\n",
    "        inner_best_model = inner_result.best_estimator_\n",
    "        start_time = time.time()\n",
    "        Y_pred = inner_best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "\n",
    "        for ecoregion in test_regions:\n",
    "            region_mask = (df.loc[test_ix, 'ecoregion3'] == ecoregion).values\n",
    "            regional_X_test = X_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_test = Y_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_pred = inner_best_model.predict(regional_X_test)\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(regional_Y_test, regional_Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    global_results[model_name]['cv_results'][outer_fold_num][ecoregion][score_func_name][y_var] = scores.loc[y_var]\n",
    "\n",
    "        print(outer_fold_num, end='... ')\n",
    "        outer_fold_num += 1\n",
    "    \n",
    "    print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "    # done with scoring of models, now time to tune a model using the whole dataset\n",
    "    outer_search = GridSearchCV(pipe, search_params, \n",
    "                                scoring='neg_mean_squared_error', \n",
    "                                n_jobs=-1, cv=cv_outer, refit=True)\n",
    "    outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "\n",
    "     # now fit on the entire dataset, not just training set\n",
    "    model = outer_result.best_estimator_\n",
    "    model.set_params(**outer_result.best_params_)\n",
    "    X = df[X_COLS].drop(['ecoregion3'], axis=1)\n",
    "    y = df[Y_COLS]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    outfile = f'global-sentinel-{model_name}-chained.pkl'\n",
    "    outpath = os.path.join('../models/structure_models', outfile)\n",
    "    with open(outpath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print('All done.')\n",
    "\n",
    "    global_results[model_name]['fitted_model'] = outer_result.best_estimator_\n",
    "    global_results[model_name]['best_params'] = outer_result.best_params_\n",
    "\n",
    "    results_dict = global_results[model_name]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Global Models\n",
    "These models get to see data from every ecoregion during training and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = build_global_results_dictionary(ecoregions[:-1], MODELS.keys(), NUM_OUTER_FOLDS, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_global_results(results):\n",
    "    data = []\n",
    "    for fold in range(NUM_OUTER_FOLDS):\n",
    "        for ecoregion in ecoregions[:-1]:\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold+1, ecoregion, target, score_name, results['cv_results'][fold+1][ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n"
     ]
    }
   ],
   "source": [
    "elastic_global = tune_global_model('ElasticNet')\n",
    "lasso_global = tune_global_model('Lasso')\n",
    "knn_global = tune_global_model('KNeighborsRegressor')\n",
    "rf_global = tune_global_model('RandomForestRegressor')\n",
    "gbm_global = tune_global_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>rmse</td>\n",
       "      <td>10.313029</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.299894</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mae</td>\n",
       "      <td>8.691027</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.252728</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>bias</td>\n",
       "      <td>-1.658816</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC      SCORE       MODEL\n",
       "0        1  Blue Mountains  total_cover   rmse  10.313029  ElasticNet\n",
       "1        1  Blue Mountains  total_cover  nrmse   0.299894  ElasticNet\n",
       "2        1  Blue Mountains  total_cover    mae   8.691027  ElasticNet\n",
       "3        1  Blue Mountains  total_cover   mape   0.252728  ElasticNet\n",
       "4        1  Blue Mountains  total_cover   bias  -1.658816  ElasticNet"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_global, lasso_global, knn_global, rf_global, gbm_global]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_global_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_global_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_global_results['ecoregion'] = all_global_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_global_results.columns = [col.upper() for col in all_global_results.columns]\n",
    "# all_global_results = all_global_results.rename({'SCORE': 'MAE'}, axis=1)\n",
    "all_global_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_global_results.to_csv('../data/processed/nestedcv_chained_global_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Outsider Models\n",
    "These models have data from the ecoregion they're tested on held out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsider_results = build_outsider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_outsider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for target in Y_COLS:\n",
    "            for score_name in score_names:\n",
    "                data.append((np.nan, ecoregion, target, score_name, results[ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... "
     ]
    }
   ],
   "source": [
    "elastic_outsider = tune_outsider_model('ElasticNet')\n",
    "lasso_outsider = tune_outsider_model('Lasso')\n",
    "knn_outsider = tune_outsider_model('KNeighborsRegressor')\n",
    "rf_outsider = tune_outsider_model('RandomForestRegressor')\n",
    "gbm_outsider = tune_outsider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_outsider, lasso_outsider, knn_outsider, rf_outsider, gbm_outsider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_outsider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_outsider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_outsider_results['ecoregion'] = all_outsider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_outsider_results.columns = [col.upper() for col in all_outsider_results.columns]\n",
    "all_outsider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outsider_results.to_csv('../data/processed/nestedcv_chained_outsider_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Insider Models\n",
    "These models are trained with observations from a single ecoregion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_results = build_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), 5, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_insider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for fold_num in results[ecoregion].keys():\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold_num, ecoregion, target, score_name, results[ecoregion][fold_num][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n"
     ]
    }
   ],
   "source": [
    "elastic_insider = tune_insider_model('ElasticNet')\n",
    "lasso_insider = tune_insider_model('Lasso')\n",
    "knn_insider = tune_insider_model('KNeighborsRegressor')\n",
    "rf_insider = tune_insider_model('RandomForestRegressor')\n",
    "gbm_insider = tune_insider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>rmse</td>\n",
       "      <td>8.713998</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.251769</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mae</td>\n",
       "      <td>7.248796</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.209436</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>bias</td>\n",
       "      <td>1.291720</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC     SCORE       MODEL\n",
       "0        1  Blue Mountains  total_cover   rmse  8.713998  ElasticNet\n",
       "1        1  Blue Mountains  total_cover  nrmse  0.251769  ElasticNet\n",
       "2        1  Blue Mountains  total_cover    mae  7.248796  ElasticNet\n",
       "3        1  Blue Mountains  total_cover   mape  0.209436  ElasticNet\n",
       "4        1  Blue Mountains  total_cover   bias  1.291720  ElasticNet"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_insider, lasso_insider, knn_insider, rf_insider, gbm_insider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_insider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_insider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_insider_results['ecoregion'] = all_insider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_insider_results.columns = [col.upper() for col in all_insider_results.columns]\n",
    "all_insider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insider_results.to_csv('../data/processed/nestedcv_chained_insider_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trained Insider Models to Score Visiting Insider Models\n",
    "These models are trained on a single region, and scored on other regions they've never seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_results = build_visiting_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET_ECOREGION</th>\n",
       "      <th>TRAIN_ECOREGION</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>25.945215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>topht</td>\n",
       "      <td>27.145965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>qmd</td>\n",
       "      <td>15.284737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>tcuft</td>\n",
       "      <td>3832.520344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>0.781861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TARGET_ECOREGION TRAIN_ECOREGION       MODEL METRIC       TARGET  \\\n",
       "0   Blue Mountains        Cascades  ElasticNet   rmse  total_cover   \n",
       "1   Blue Mountains        Cascades  ElasticNet   rmse        topht   \n",
       "2   Blue Mountains        Cascades  ElasticNet   rmse          qmd   \n",
       "3   Blue Mountains        Cascades  ElasticNet   rmse        tcuft   \n",
       "4   Blue Mountains        Cascades  ElasticNet  nrmse  total_cover   \n",
       "\n",
       "         SCORE  \n",
       "0    25.945215  \n",
       "1    27.145965  \n",
       "2    15.284737  \n",
       "3  3832.520344  \n",
       "4     0.781861  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitor_results = []\n",
    "for target_region in ecoregions[:-1]:\n",
    "    for train_region in [r for r in ecoregions[:-1] if r != target_region]:\n",
    "        for model_name in MODELS.keys():\n",
    "            model = insider_results[train_region][model_name]['fitted_model']\n",
    "            targ_idx = X.loc[X.ecoregion3 == target_region].index.values\n",
    "            targ_X = X.loc[targ_idx].drop(['ecoregion3'], axis=1)\n",
    "            pred = model.predict(targ_X)\n",
    "            obs = Y.loc[targ_idx]\n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(obs, pred)\n",
    "                for y, score in scores.iteritems():\n",
    "                    visitor_results.append(\n",
    "                        (' '.join(target_region.title().replace('_',' ').split()),\n",
    "                         ' '.join(train_region.title().replace('_',' ').split()),\n",
    "                         model_name, score_func_name, y, score))\n",
    "visitor_df = pd.DataFrame(visitor_results, \n",
    "                          columns = ['TARGET_ECOREGION', 'TRAIN_ECOREGION', \n",
    "                                     'MODEL', 'METRIC', 'TARGET', 'SCORE'])\n",
    "visitor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_df.to_csv('../data/processed/nestedcv_chained_visitor_results_satellite_structure.csv', \n",
    "                  header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest_mapping]",
   "language": "python",
   "name": "conda-env-forest_mapping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
