{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# data prep and model-tuning\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# types of models we'll fit\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.multioutput import RegressorChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5089 entries, 0 to 5088\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   UUID                 5089 non-null   object \n",
      " 1   LAT                  5089 non-null   float64\n",
      " 2   LON                  5089 non-null   float64\n",
      " 3   ECOREGION3           5089 non-null   object \n",
      " 4   AGENCY               5089 non-null   object \n",
      " 5   DISTANCE_TO_WATER_M  5089 non-null   float64\n",
      " 6   PLOT_SIZE_AC         5089 non-null   float64\n",
      " 7   MEAS_YR              5089 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 318.2+ KB\n"
     ]
    }
   ],
   "source": [
    "PLOT_DATA = '../data/processed/plot_features.csv'\n",
    "KEEP_PLOT_COLS = ['uuid', 'lat', 'lon', 'ecoregion3', 'agency', 'distance_to_water_m', 'plot_size_ac', 'meas_yr']\n",
    "plot_data = pd.read_csv(PLOT_DATA)[KEEP_PLOT_COLS]\n",
    "plot_data.columns = [col.upper() for col in plot_data.columns]\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5411 entries, 0 to 5410\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   UUID         5411 non-null   object \n",
      " 1   elevation    5411 non-null   float64\n",
      " 2   lidar_cover  5411 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 126.9+ KB\n"
     ]
    }
   ],
   "source": [
    "LIDAR_DATA = '../data/processed/lidar_features.csv'\n",
    "lidar_data = pd.read_csv(LIDAR_DATA)[['uuid', 'elevation', 'cover']].rename({'uuid': 'UUID', 'cover': 'lidar_cover'}, axis=1)\n",
    "lidar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4272 entries, 0 to 5410\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   UUID                 4272 non-null   object \n",
      " 1   LAT                  4272 non-null   float64\n",
      " 2   LON                  4272 non-null   float64\n",
      " 3   ECOREGION3           4272 non-null   object \n",
      " 4   AGENCY               4272 non-null   object \n",
      " 5   DISTANCE_TO_WATER_M  4272 non-null   float64\n",
      " 6   PLOT_SIZE_AC         4272 non-null   float64\n",
      " 7   MEAS_YR              4272 non-null   int64  \n",
      " 8   ELEVATION            4272 non-null   float64\n",
      " 9   LIDAR_COVER          4272 non-null   float64\n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 367.1+ KB\n"
     ]
    }
   ],
   "source": [
    "plot_data = plot_data.merge(lidar_data, left_on=['UUID'], right_on=['UUID'], how='inner').drop_duplicates(subset=['UUID'])\n",
    "plot_data.columns = [col.upper() for col in plot_data.columns]\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 49410 entries, ('ba510248', 2010) to ('c4f7f099', 2025)\n",
      "Data columns (total 75 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   TPA                  49410 non-null  int64  \n",
      " 1   BA                   49410 non-null  int64  \n",
      " 2   SDI                  49410 non-null  int64  \n",
      " 3   CCF                  49410 non-null  int64  \n",
      " 4   QMD                  49410 non-null  float64\n",
      " 5   TCUFT                49410 non-null  int64  \n",
      " 6   TOPHT                49410 non-null  int64  \n",
      " 7   NUMBER_OF_STRATA     49410 non-null  int64  \n",
      " 8   TOTAL_COVER          49410 non-null  int64  \n",
      " 9   STRUCTURE_CLASS      49410 non-null  object \n",
      " 10  CANOPY_BASEHEIGHT    49410 non-null  int64  \n",
      " 11  CANOPY_BULKDENSITY   49410 non-null  float64\n",
      " 12  ABOVEGROUND_BIOMASS  49410 non-null  int64  \n",
      " 13  ABOVEGROUND_CARBON   49410 non-null  int64  \n",
      " 14  GS_TPA               49410 non-null  int64  \n",
      " 15  AF                   49410 non-null  int64  \n",
      " 16  AS                   49410 non-null  int64  \n",
      " 17  BM                   49410 non-null  int64  \n",
      " 18  BO                   49410 non-null  int64  \n",
      " 19  CH                   49410 non-null  int64  \n",
      " 20  CW                   49410 non-null  int64  \n",
      " 21  DF                   49410 non-null  int64  \n",
      " 22  DG                   49410 non-null  int64  \n",
      " 23  ES                   49410 non-null  int64  \n",
      " 24  GC                   49410 non-null  int64  \n",
      " 25  GF                   49410 non-null  int64  \n",
      " 26  IC                   49410 non-null  int64  \n",
      " 27  JP                   49410 non-null  int64  \n",
      " 28  KP                   49410 non-null  int64  \n",
      " 29  LO                   49410 non-null  int64  \n",
      " 30  LP                   49410 non-null  int64  \n",
      " 31  MA                   49410 non-null  int64  \n",
      " 32  MC                   49410 non-null  int64  \n",
      " 33  MH                   49410 non-null  int64  \n",
      " 34  NF                   49410 non-null  int64  \n",
      " 35  OH                   49410 non-null  int64  \n",
      " 36  OS                   49410 non-null  int64  \n",
      " 37  OT                   49410 non-null  int64  \n",
      " 38  PC                   49410 non-null  int64  \n",
      " 39  PL                   49410 non-null  int64  \n",
      " 40  PP                   49410 non-null  int64  \n",
      " 41  PY                   49410 non-null  int64  \n",
      " 42  RA                   49410 non-null  int64  \n",
      " 43  RC                   49410 non-null  int64  \n",
      " 44  RF                   49410 non-null  int64  \n",
      " 45  SF                   49410 non-null  int64  \n",
      " 46  SH                   49410 non-null  int64  \n",
      " 47  SP                   49410 non-null  int64  \n",
      " 48  SS                   49410 non-null  int64  \n",
      " 49  TO                   49410 non-null  int64  \n",
      " 50  WA                   49410 non-null  int64  \n",
      " 51  WB                   49410 non-null  int64  \n",
      " 52  WF                   49410 non-null  int64  \n",
      " 53  WH                   49410 non-null  int64  \n",
      " 54  WI                   49410 non-null  int64  \n",
      " 55  WJ                   49410 non-null  int64  \n",
      " 56  WL                   49410 non-null  int64  \n",
      " 57  WO                   49410 non-null  int64  \n",
      " 58  WP                   49410 non-null  int64  \n",
      " 59  YC                   49410 non-null  int64  \n",
      " 60  TRUE_FIR             49410 non-null  int64  \n",
      " 61  OTHER_HARDWOOD       49410 non-null  int64  \n",
      " 62  MAPLE                49410 non-null  int64  \n",
      " 63  OAK                  49410 non-null  int64  \n",
      " 64  DOUGLAS_FIR          49410 non-null  int64  \n",
      " 65  SPRUCE               49410 non-null  int64  \n",
      " 66  CEDAR                49410 non-null  int64  \n",
      " 67  PONDEROSA_PINE       49410 non-null  int64  \n",
      " 68  OTHER_SOFTWOOD       49410 non-null  int64  \n",
      " 69  LODGEPOLE_PINE       49410 non-null  int64  \n",
      " 70  HEMLOCK              49410 non-null  int64  \n",
      " 71  RED_ALDER            49410 non-null  int64  \n",
      " 72  TANOAK               49410 non-null  int64  \n",
      " 73  JUNIPER              49410 non-null  int64  \n",
      " 74  LARCH                49410 non-null  int64  \n",
      "dtypes: float64(2), int64(72), object(1)\n",
      "memory usage: 28.6+ MB\n"
     ]
    }
   ],
   "source": [
    "INVENTORY = '../data/processed/inventory_features.csv'\n",
    "inv_data = pd.read_csv(INVENTORY).rename({'uuid': 'UUID', 'year':'YEAR'}, axis=1).set_index(['UUID', 'YEAR'])\n",
    "inv_data.columns = [col.upper() for col in inv_data.columns]\n",
    "inv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 10178 entries, ('00027724', 2019) to ('fff7e1c3', 2020)\n",
      "Data columns (total 53 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   S2_R_LEAFOFF           10178 non-null  float64\n",
      " 1   S2_G_LEAFOFF           10178 non-null  float64\n",
      " 2   S2_B_LEAFOFF           10178 non-null  float64\n",
      " 3   S2_NIR_LEAFOFF         10178 non-null  float64\n",
      " 4   S2_SWIR1_LEAFOFF       10178 non-null  float64\n",
      " 5   S2_SWIR2_LEAFOFF       10178 non-null  float64\n",
      " 6   S2_RE1_LEAFOFF         10178 non-null  float64\n",
      " 7   S2_RE2_LEAFOFF         10178 non-null  float64\n",
      " 8   S2_RE3_LEAFOFF         10178 non-null  float64\n",
      " 9   S2_RE4_LEAFOFF         10178 non-null  float64\n",
      " 10  S2_R_LEAFON            10178 non-null  float64\n",
      " 11  S2_G_LEAFON            10178 non-null  float64\n",
      " 12  S2_B_LEAFON            10178 non-null  float64\n",
      " 13  S2_NIR_LEAFON          10178 non-null  float64\n",
      " 14  S2_SWIR1_LEAFON        10178 non-null  float64\n",
      " 15  S2_SWIR2_LEAFON        10178 non-null  float64\n",
      " 16  S2_RE1_LEAFON          10178 non-null  float64\n",
      " 17  S2_RE2_LEAFON          10178 non-null  float64\n",
      " 18  S2_RE3_LEAFON          10178 non-null  float64\n",
      " 19  S2_RE4_LEAFON          10178 non-null  float64\n",
      " 20  S2_NDVI_LEAFON         10178 non-null  float64\n",
      " 21  S2_SAVI_LEAFON         10178 non-null  float64\n",
      " 22  S2_BRIGHTNESS_LEAFON   10178 non-null  float64\n",
      " 23  S2_GREENNESS_LEAFON    10178 non-null  float64\n",
      " 24  S2_WETNESS_LEAFON      10178 non-null  float64\n",
      " 25  S2_NDVI_LEAFOFF        10178 non-null  float64\n",
      " 26  S2_SAVI_LEAFOFF        10178 non-null  float64\n",
      " 27  S2_BRIGHTNESS_LEAFOFF  10178 non-null  float64\n",
      " 28  S2_GREENNESS_LEAFOFF   10178 non-null  float64\n",
      " 29  S2_WETNESS_LEAFOFF     10178 non-null  float64\n",
      " 30  S2_dR                  10178 non-null  float64\n",
      " 31  S2_dG                  10178 non-null  float64\n",
      " 32  S2_dB                  10178 non-null  float64\n",
      " 33  S2_dNIR                10178 non-null  float64\n",
      " 34  S2_dSWIR1              10178 non-null  float64\n",
      " 35  S2_dSWIR2              10178 non-null  float64\n",
      " 36  S2_dRE1                10178 non-null  float64\n",
      " 37  S2_dRE2                10178 non-null  float64\n",
      " 38  S2_dNDVI               10178 non-null  float64\n",
      " 39  S2_dSAVI               10178 non-null  float64\n",
      " 40  S2_dBRIGHTNESS         10178 non-null  float64\n",
      " 41  S2_dGREENNESS          10178 non-null  float64\n",
      " 42  S2_dWETNESS            10178 non-null  float64\n",
      " 43  S2_dRE3                10178 non-null  float64\n",
      " 44  S2_dRE4                10178 non-null  float64\n",
      " 45  LT_DUR_NBR             10178 non-null  int64  \n",
      " 46  LT_DUR_SWIR1           10178 non-null  int64  \n",
      " 47  LT_MAG_NBR             10178 non-null  int64  \n",
      " 48  LT_MAG_SWIR1           10178 non-null  int64  \n",
      " 49  LT_RATE_NBR            10178 non-null  int64  \n",
      " 50  LT_RATE_SWIR1          10178 non-null  int64  \n",
      " 51  LT_YSD_NBR             10178 non-null  int64  \n",
      " 52  LT_YSD_SWIR1           10178 non-null  int64  \n",
      "dtypes: float64(45), int64(8)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "SATELLITE = '../data/processed/satellite_features.csv'\n",
    "sat = pd.read_csv(SATELLITE).rename({'uuid': 'UUID', 'year': 'YEAR'}, axis=1).set_index(['UUID', 'YEAR'])\n",
    "S2_COLS = [col for col in sat.columns if col.startswith('S2')]\n",
    "LANDTRENDR_COLS = [col for col in sat.columns if col.startswith('LT')]\n",
    "sat = sat[S2_COLS + LANDTRENDR_COLS].dropna()\n",
    "sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out some of the training data\n",
    "We can exclude some of the training data based on how far separated the inventory data (interpolated using FVS simulations) is from the year the imagery was collected. Similarly, we can screen out training examples that had relatively low density of lidar returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9385 entries, 0 to 9384\n",
      "Columns: 130 entries, UUID to LARCH\n",
      "dtypes: float64(47), int64(81), object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "sat_and_inv = sat.merge(inv_data, how='inner', left_index=True, right_index=True).reset_index()\n",
    "sat_and_inv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,952 samples\n",
      "Columns: ['UUID' 'YEAR' 'S2_R_LEAFOFF' 'S2_G_LEAFOFF' 'S2_B_LEAFOFF'\n",
      " 'S2_NIR_LEAFOFF' 'S2_SWIR1_LEAFOFF' 'S2_SWIR2_LEAFOFF' 'S2_RE1_LEAFOFF'\n",
      " 'S2_RE2_LEAFOFF' 'S2_RE3_LEAFOFF' 'S2_RE4_LEAFOFF' 'S2_R_LEAFON'\n",
      " 'S2_G_LEAFON' 'S2_B_LEAFON' 'S2_NIR_LEAFON' 'S2_SWIR1_LEAFON'\n",
      " 'S2_SWIR2_LEAFON' 'S2_RE1_LEAFON' 'S2_RE2_LEAFON' 'S2_RE3_LEAFON'\n",
      " 'S2_RE4_LEAFON' 'S2_NDVI_LEAFON' 'S2_SAVI_LEAFON' 'S2_BRIGHTNESS_LEAFON'\n",
      " 'S2_GREENNESS_LEAFON' 'S2_WETNESS_LEAFON' 'S2_NDVI_LEAFOFF'\n",
      " 'S2_SAVI_LEAFOFF' 'S2_BRIGHTNESS_LEAFOFF' 'S2_GREENNESS_LEAFOFF'\n",
      " 'S2_WETNESS_LEAFOFF' 'S2_dR' 'S2_dG' 'S2_dB' 'S2_dNIR' 'S2_dSWIR1'\n",
      " 'S2_dSWIR2' 'S2_dRE1' 'S2_dRE2' 'S2_dNDVI' 'S2_dSAVI' 'S2_dBRIGHTNESS'\n",
      " 'S2_dGREENNESS' 'S2_dWETNESS' 'S2_dRE3' 'S2_dRE4' 'LT_DUR_NBR'\n",
      " 'LT_DUR_SWIR1' 'LT_MAG_NBR' 'LT_MAG_SWIR1' 'LT_RATE_NBR' 'LT_RATE_SWIR1'\n",
      " 'LT_YSD_NBR' 'LT_YSD_SWIR1' 'TPA' 'BA' 'SDI' 'CCF' 'QMD' 'TCUFT' 'TOPHT'\n",
      " 'NUMBER_OF_STRATA' 'TOTAL_COVER' 'STRUCTURE_CLASS' 'CANOPY_BASEHEIGHT'\n",
      " 'CANOPY_BULKDENSITY' 'ABOVEGROUND_BIOMASS' 'ABOVEGROUND_CARBON' 'GS_TPA'\n",
      " 'AF' 'AS' 'BM' 'BO' 'CH' 'CW' 'DF' 'DG' 'ES' 'GC' 'GF' 'IC' 'JP' 'KP'\n",
      " 'LO' 'LP' 'MA' 'MC' 'MH' 'NF' 'OH' 'OS' 'OT' 'PC' 'PL' 'PP' 'PY' 'RA'\n",
      " 'RC' 'RF' 'SF' 'SH' 'SP' 'SS' 'TO' 'WA' 'WB' 'WF' 'WH' 'WI' 'WJ' 'WL'\n",
      " 'WO' 'WP' 'YC' 'TRUE_FIR' 'OTHER_HARDWOOD' 'MAPLE' 'OAK' 'DOUGLAS_FIR'\n",
      " 'SPRUCE' 'CEDAR' 'PONDEROSA_PINE' 'OTHER_SOFTWOOD' 'LODGEPOLE_PINE'\n",
      " 'HEMLOCK' 'RED_ALDER' 'TANOAK' 'JUNIPER' 'LARCH' 'LAT' 'LON' 'ECOREGION3'\n",
      " 'AGENCY' 'DISTANCE_TO_WATER_M' 'PLOT_SIZE_AC' 'MEAS_YR' 'ELEVATION'\n",
      " 'LIDAR_COVER']\n"
     ]
    }
   ],
   "source": [
    "df = sat_and_inv.merge(plot_data, how='inner', left_on=['UUID'], right_on=['UUID']).dropna()\n",
    "print('{:,d} samples'.format(len(df)))\n",
    "print('Columns:', df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7952 entries, 0 to 7951\n",
      "Columns: 139 entries, UUID to LIDAR_COVER\n",
      "dtypes: float64(53), int64(82), object(4)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "OUTLIERS = '../data/interim/outlier_uuids.csv'\n",
    "outliers = pd.read_csv(OUTLIERS)\n",
    "# filter out the height outliers\n",
    "df = df[~df.UUID.isin(outliers.outlier_uuid)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7708 entries, 0 to 7951\n",
      "Columns: 140 entries, UUID to qmd\n",
      "dtypes: float64(54), int64(82), object(4)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[(df.TOPHT > 0)&(df.TOTAL_COVER >= 0)&(df.QMD > 0)]\n",
    "# df = df.loc[(df.TOPHT > 0)&(df.LIDAR_COVER >= 10)&(df.QMD > 0)]\n",
    "df.loc[df.QMD > 50, 'qmd'] = 50\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/sentinel_structure_training_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect how many samples we have for different years, regions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>4099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>3609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UUID\n",
       "YEAR      \n",
       "2019  4099\n",
       "2020  3609"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['YEAR'])[['UUID']].count().rename({'uuid':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>YEAR</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_YR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>392</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>752</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>459</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>491</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>499</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "YEAR     2019  2020\n",
       "MEAS_YR            \n",
       "2010      490     0\n",
       "2011      392   392\n",
       "2013      752   752\n",
       "2014      459   459\n",
       "2015      491   491\n",
       "2016      674   674\n",
       "2017      499   499\n",
       "2018      342   342"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df, \n",
    "               values='UUID', \n",
    "               aggfunc='count', \n",
    "               index=['MEAS_YR'], \n",
    "               columns=['YEAR'], \n",
    "               fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>PLOT_SIZE_AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECOREGION3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>713</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>1771</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>columbia_plateau</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>434</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>296</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>432</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    UUID  YEAR  PLOT_SIZE_AC\n",
       "ECOREGION3                                                                  \n",
       "blue_mountains                                       110     2             1\n",
       "cascades                                             713     2             3\n",
       "coast_range                                         1771     2             2\n",
       "columbia_plateau                                      13     2             1\n",
       "eastern_cascades_slopes_and_foothills                434     2             2\n",
       "klamath_mountains_california_high_north_coast_r...   296     2             2\n",
       "north_cascades                                       432     2             2\n",
       "northern_rockies                                     122     2             1\n",
       "puget_lowland                                        163     2             1\n",
       "willamette_valley                                     45     2             3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoreg_counts = df.groupby(by=['ECOREGION3'])[['UUID', 'YEAR', 'PLOT_SIZE_AC']].nunique()\n",
    "ecoreg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available features\n",
    "The different types of predictor variables we can use to predict a forest attribute, including climate, lidar-derived, soil, and satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S2_R_LEAFOFF</th>\n",
       "      <th>S2_G_LEAFOFF</th>\n",
       "      <th>S2_B_LEAFOFF</th>\n",
       "      <th>S2_NIR_LEAFOFF</th>\n",
       "      <th>S2_SWIR1_LEAFOFF</th>\n",
       "      <th>S2_SWIR2_LEAFOFF</th>\n",
       "      <th>S2_RE1_LEAFOFF</th>\n",
       "      <th>S2_RE2_LEAFOFF</th>\n",
       "      <th>S2_RE3_LEAFOFF</th>\n",
       "      <th>S2_RE4_LEAFOFF</th>\n",
       "      <th>...</th>\n",
       "      <th>S2_dRE3</th>\n",
       "      <th>S2_dRE4</th>\n",
       "      <th>LT_DUR_NBR</th>\n",
       "      <th>LT_DUR_SWIR1</th>\n",
       "      <th>LT_MAG_NBR</th>\n",
       "      <th>LT_MAG_SWIR1</th>\n",
       "      <th>LT_RATE_NBR</th>\n",
       "      <th>LT_RATE_SWIR1</th>\n",
       "      <th>LT_YSD_NBR</th>\n",
       "      <th>LT_YSD_SWIR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>606.220706</td>\n",
       "      <td>680.174695</td>\n",
       "      <td>583.685829</td>\n",
       "      <td>2284.744952</td>\n",
       "      <td>712.242937</td>\n",
       "      <td>382.819250</td>\n",
       "      <td>930.007975</td>\n",
       "      <td>1876.339342</td>\n",
       "      <td>2130.404596</td>\n",
       "      <td>2290.689006</td>\n",
       "      <td>...</td>\n",
       "      <td>360.229905</td>\n",
       "      <td>417.447696</td>\n",
       "      <td>18.242086</td>\n",
       "      <td>14.270498</td>\n",
       "      <td>80.029190</td>\n",
       "      <td>292.274131</td>\n",
       "      <td>17.226518</td>\n",
       "      <td>319.817462</td>\n",
       "      <td>28.578620</td>\n",
       "      <td>26.555008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1046.496651</td>\n",
       "      <td>1051.096740</td>\n",
       "      <td>1078.714105</td>\n",
       "      <td>958.400257</td>\n",
       "      <td>314.879348</td>\n",
       "      <td>231.979016</td>\n",
       "      <td>1045.904615</td>\n",
       "      <td>984.708696</td>\n",
       "      <td>933.465047</td>\n",
       "      <td>883.204884</td>\n",
       "      <td>...</td>\n",
       "      <td>1021.978121</td>\n",
       "      <td>956.320125</td>\n",
       "      <td>12.275768</td>\n",
       "      <td>14.925823</td>\n",
       "      <td>193.113362</td>\n",
       "      <td>639.630613</td>\n",
       "      <td>36.187498</td>\n",
       "      <td>530.250292</td>\n",
       "      <td>10.468631</td>\n",
       "      <td>9.956267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.797190</td>\n",
       "      <td>26.922216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>348.355904</td>\n",
       "      <td>91.010674</td>\n",
       "      <td>42.869246</td>\n",
       "      <td>70.049804</td>\n",
       "      <td>257.344712</td>\n",
       "      <td>311.294348</td>\n",
       "      <td>379.008394</td>\n",
       "      <td>...</td>\n",
       "      <td>-6641.178021</td>\n",
       "      <td>-6084.150834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-734.000000</td>\n",
       "      <td>-3098.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>-390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>138.312292</td>\n",
       "      <td>226.456214</td>\n",
       "      <td>121.474867</td>\n",
       "      <td>1702.408017</td>\n",
       "      <td>524.966903</td>\n",
       "      <td>248.190720</td>\n",
       "      <td>437.145189</td>\n",
       "      <td>1334.535597</td>\n",
       "      <td>1579.458543</td>\n",
       "      <td>1748.899616</td>\n",
       "      <td>...</td>\n",
       "      <td>145.159584</td>\n",
       "      <td>161.469546</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-125.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>218.161288</td>\n",
       "      <td>312.829155</td>\n",
       "      <td>195.150466</td>\n",
       "      <td>2112.570389</td>\n",
       "      <td>651.075741</td>\n",
       "      <td>312.776245</td>\n",
       "      <td>574.204674</td>\n",
       "      <td>1636.446904</td>\n",
       "      <td>1938.376937</td>\n",
       "      <td>2133.245012</td>\n",
       "      <td>...</td>\n",
       "      <td>475.905524</td>\n",
       "      <td>495.082222</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>546.130330</td>\n",
       "      <td>575.580973</td>\n",
       "      <td>462.785578</td>\n",
       "      <td>2623.662855</td>\n",
       "      <td>815.419255</td>\n",
       "      <td>431.150948</td>\n",
       "      <td>906.008269</td>\n",
       "      <td>2079.902223</td>\n",
       "      <td>2414.413281</td>\n",
       "      <td>2614.498213</td>\n",
       "      <td>...</td>\n",
       "      <td>824.124349</td>\n",
       "      <td>862.869615</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9046.410985</td>\n",
       "      <td>9303.687276</td>\n",
       "      <td>9492.814974</td>\n",
       "      <td>9533.337612</td>\n",
       "      <td>3020.551859</td>\n",
       "      <td>2062.405424</td>\n",
       "      <td>9169.860337</td>\n",
       "      <td>9173.494865</td>\n",
       "      <td>9085.315148</td>\n",
       "      <td>8913.603338</td>\n",
       "      <td>...</td>\n",
       "      <td>4035.860442</td>\n",
       "      <td>4245.146064</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1071.000000</td>\n",
       "      <td>3133.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>3133.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S2_R_LEAFOFF  S2_G_LEAFOFF  S2_B_LEAFOFF  S2_NIR_LEAFOFF  \\\n",
       "count   7708.000000   7708.000000   7708.000000     7708.000000   \n",
       "mean     606.220706    680.174695    583.685829     2284.744952   \n",
       "std     1046.496651   1051.096740   1078.714105      958.400257   \n",
       "min        9.797190     26.922216      1.000000      348.355904   \n",
       "25%      138.312292    226.456214    121.474867     1702.408017   \n",
       "50%      218.161288    312.829155    195.150466     2112.570389   \n",
       "75%      546.130330    575.580973    462.785578     2623.662855   \n",
       "max     9046.410985   9303.687276   9492.814974     9533.337612   \n",
       "\n",
       "       S2_SWIR1_LEAFOFF  S2_SWIR2_LEAFOFF  S2_RE1_LEAFOFF  S2_RE2_LEAFOFF  \\\n",
       "count       7708.000000       7708.000000     7708.000000     7708.000000   \n",
       "mean         712.242937        382.819250      930.007975     1876.339342   \n",
       "std          314.879348        231.979016     1045.904615      984.708696   \n",
       "min           91.010674         42.869246       70.049804      257.344712   \n",
       "25%          524.966903        248.190720      437.145189     1334.535597   \n",
       "50%          651.075741        312.776245      574.204674     1636.446904   \n",
       "75%          815.419255        431.150948      906.008269     2079.902223   \n",
       "max         3020.551859       2062.405424     9169.860337     9173.494865   \n",
       "\n",
       "       S2_RE3_LEAFOFF  S2_RE4_LEAFOFF  ...      S2_dRE3      S2_dRE4  \\\n",
       "count     7708.000000     7708.000000  ...  7708.000000  7708.000000   \n",
       "mean      2130.404596     2290.689006  ...   360.229905   417.447696   \n",
       "std        933.465047      883.204884  ...  1021.978121   956.320125   \n",
       "min        311.294348      379.008394  ... -6641.178021 -6084.150834   \n",
       "25%       1579.458543     1748.899616  ...   145.159584   161.469546   \n",
       "50%       1938.376937     2133.245012  ...   475.905524   495.082222   \n",
       "75%       2414.413281     2614.498213  ...   824.124349   862.869615   \n",
       "max       9085.315148     8913.603338  ...  4035.860442  4245.146064   \n",
       "\n",
       "        LT_DUR_NBR  LT_DUR_SWIR1   LT_MAG_NBR  LT_MAG_SWIR1  LT_RATE_NBR  \\\n",
       "count  7708.000000   7708.000000  7708.000000   7708.000000  7708.000000   \n",
       "mean     18.242086     14.270498    80.029190    292.274131    17.226518   \n",
       "std      12.275768     14.925823   193.113362    639.630613    36.187498   \n",
       "min       1.000000      1.000000  -734.000000  -3098.000000  -116.000000   \n",
       "25%       6.000000      1.000000    -4.000000   -125.250000     0.000000   \n",
       "50%      19.000000      4.000000    42.000000    157.000000     3.000000   \n",
       "75%      27.000000     35.000000   136.000000    553.000000    18.000000   \n",
       "max      36.000000     36.000000  1071.000000   3133.000000   268.000000   \n",
       "\n",
       "       LT_RATE_SWIR1   LT_YSD_NBR  LT_YSD_SWIR1  \n",
       "count    7708.000000  7708.000000   7708.000000  \n",
       "mean      319.817462    28.578620     26.555008  \n",
       "std       530.250292    10.468631      9.956267  \n",
       "min      -390.000000     0.000000      0.000000  \n",
       "25%        -5.000000    27.000000     21.000000  \n",
       "50%        33.000000    34.000000     31.000000  \n",
       "75%       423.000000    35.000000     34.000000  \n",
       "max      3133.000000    35.000000     35.000000  \n",
       "\n",
       "[8 rows x 53 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[S2_COLS + LANDTRENDR_COLS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and targets\n",
    "This is the first step in determining what features we want to use, and what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = S2_COLS + LANDTRENDR_COLS + ['ELEVATION', 'LAT', 'LON'] + ['ECOREGION3'] \n",
    "Y_COLS = ['LIDAR_COVER', 'TOPHT', 'QMD', 'SDI', 'TCUFT', 'ABOVEGROUND_BIOMASS']\n",
    "\n",
    "Y_NAMES = [col.upper() for col in Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIDAR_COVER</th>\n",
       "      <th>TOPHT</th>\n",
       "      <th>QMD</th>\n",
       "      <th>SDI</th>\n",
       "      <th>TCUFT</th>\n",
       "      <th>ABOVEGROUND_BIOMASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>66.853895</td>\n",
       "      <td>96.780747</td>\n",
       "      <td>14.296492</td>\n",
       "      <td>315.817592</td>\n",
       "      <td>7410.013882</td>\n",
       "      <td>181.553321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.918710</td>\n",
       "      <td>39.322274</td>\n",
       "      <td>8.100335</td>\n",
       "      <td>177.210972</td>\n",
       "      <td>6312.855165</td>\n",
       "      <td>121.967801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.427506</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.677923</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>8.730670</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2579.750000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.035060</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>12.820993</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>5871.500000</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>91.370873</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>17.797910</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>10565.000000</td>\n",
       "      <td>246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>82.039021</td>\n",
       "      <td>1307.000000</td>\n",
       "      <td>51330.000000</td>\n",
       "      <td>887.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIDAR_COVER        TOPHT          QMD          SDI         TCUFT  \\\n",
       "count  7708.000000  7708.000000  7708.000000  7708.000000   7708.000000   \n",
       "mean     66.853895    96.780747    14.296492   315.817592   7410.013882   \n",
       "std      30.918710    39.322274     8.100335   177.210972   6312.855165   \n",
       "min       0.000000     6.000000     1.427506     2.000000      1.000000   \n",
       "25%      49.677923    68.000000     8.730670   181.000000   2579.750000   \n",
       "50%      80.035060    94.000000    12.820993   310.000000   5871.500000   \n",
       "75%      91.370873   123.000000    17.797910   435.000000  10565.000000   \n",
       "max     100.000000   267.000000    82.039021  1307.000000  51330.000000   \n",
       "\n",
       "       ABOVEGROUND_BIOMASS  \n",
       "count          7708.000000  \n",
       "mean            181.553321  \n",
       "std             121.967801  \n",
       "min               2.000000  \n",
       "25%              88.000000  \n",
       "50%             165.000000  \n",
       "75%             246.000000  \n",
       "max             887.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[Y_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S2_R_LEAFOFF',\n",
       " 'S2_G_LEAFOFF',\n",
       " 'S2_B_LEAFOFF',\n",
       " 'S2_NIR_LEAFOFF',\n",
       " 'S2_SWIR1_LEAFOFF',\n",
       " 'S2_SWIR2_LEAFOFF',\n",
       " 'S2_RE1_LEAFOFF',\n",
       " 'S2_RE2_LEAFOFF',\n",
       " 'S2_RE3_LEAFOFF',\n",
       " 'S2_RE4_LEAFOFF',\n",
       " 'S2_R_LEAFON',\n",
       " 'S2_G_LEAFON',\n",
       " 'S2_B_LEAFON',\n",
       " 'S2_NIR_LEAFON',\n",
       " 'S2_SWIR1_LEAFON',\n",
       " 'S2_SWIR2_LEAFON',\n",
       " 'S2_RE1_LEAFON',\n",
       " 'S2_RE2_LEAFON',\n",
       " 'S2_RE3_LEAFON',\n",
       " 'S2_RE4_LEAFON',\n",
       " 'S2_NDVI_LEAFON',\n",
       " 'S2_SAVI_LEAFON',\n",
       " 'S2_BRIGHTNESS_LEAFON',\n",
       " 'S2_GREENNESS_LEAFON',\n",
       " 'S2_WETNESS_LEAFON',\n",
       " 'S2_NDVI_LEAFOFF',\n",
       " 'S2_SAVI_LEAFOFF',\n",
       " 'S2_BRIGHTNESS_LEAFOFF',\n",
       " 'S2_GREENNESS_LEAFOFF',\n",
       " 'S2_WETNESS_LEAFOFF',\n",
       " 'S2_dR',\n",
       " 'S2_dG',\n",
       " 'S2_dB',\n",
       " 'S2_dNIR',\n",
       " 'S2_dSWIR1',\n",
       " 'S2_dSWIR2',\n",
       " 'S2_dRE1',\n",
       " 'S2_dRE2',\n",
       " 'S2_dNDVI',\n",
       " 'S2_dSAVI',\n",
       " 'S2_dBRIGHTNESS',\n",
       " 'S2_dGREENNESS',\n",
       " 'S2_dWETNESS',\n",
       " 'S2_dRE3',\n",
       " 'S2_dRE4',\n",
       " 'LT_DUR_NBR',\n",
       " 'LT_DUR_SWIR1',\n",
       " 'LT_MAG_NBR',\n",
       " 'LT_MAG_SWIR1',\n",
       " 'LT_RATE_NBR',\n",
       " 'LT_RATE_SWIR1',\n",
       " 'LT_YSD_NBR',\n",
       " 'LT_YSD_SWIR1',\n",
       " 'ELEVATION',\n",
       " 'LAT',\n",
       " 'LON',\n",
       " 'ECOREGION3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIDAR_COVER</th>\n",
       "      <th>TOPHT</th>\n",
       "      <th>QMD</th>\n",
       "      <th>SDI</th>\n",
       "      <th>TCUFT</th>\n",
       "      <th>ABOVEGROUND_BIOMASS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECOREGION3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>33.0</td>\n",
       "      <td>59.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>108.3</td>\n",
       "      <td>1636.8</td>\n",
       "      <td>44.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>73.2</td>\n",
       "      <td>110.2</td>\n",
       "      <td>16.6</td>\n",
       "      <td>363.6</td>\n",
       "      <td>9718.3</td>\n",
       "      <td>232.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>68.2</td>\n",
       "      <td>84.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>293.9</td>\n",
       "      <td>5827.7</td>\n",
       "      <td>153.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>75.0</td>\n",
       "      <td>102.8</td>\n",
       "      <td>13.8</td>\n",
       "      <td>346.4</td>\n",
       "      <td>7940.3</td>\n",
       "      <td>197.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>72.8</td>\n",
       "      <td>93.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>371.4</td>\n",
       "      <td>7164.3</td>\n",
       "      <td>156.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>49.6</td>\n",
       "      <td>71.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>194.4</td>\n",
       "      <td>3395.5</td>\n",
       "      <td>86.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>47.6</td>\n",
       "      <td>71.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>151.3</td>\n",
       "      <td>2354.3</td>\n",
       "      <td>64.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>38.3</td>\n",
       "      <td>90.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>294.7</td>\n",
       "      <td>6154.1</td>\n",
       "      <td>177.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>81.4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>9516.5</td>\n",
       "      <td>242.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    LIDAR_COVER  TOPHT   QMD  \\\n",
       "ECOREGION3                                                                     \n",
       "blue_mountains                                             33.0   59.5  13.2   \n",
       "coast_range                                                73.2  110.2  16.6   \n",
       "north_cascades                                             68.2   84.7  12.0   \n",
       "cascades                                                   75.0  102.8  13.8   \n",
       "klamath_mountains_california_high_north_coast_r...         72.8   93.6  11.3   \n",
       "eastern_cascades_slopes_and_foothills                      49.6   71.9  12.3   \n",
       "northern_rockies                                           47.6   71.1  11.8   \n",
       "puget_lowland                                              38.3   90.7  12.7   \n",
       "willamette_valley                                          81.4  113.0  19.0   \n",
       "\n",
       "                                                      SDI   TCUFT  \\\n",
       "ECOREGION3                                                          \n",
       "blue_mountains                                      108.3  1636.8   \n",
       "coast_range                                         363.6  9718.3   \n",
       "north_cascades                                      293.9  5827.7   \n",
       "cascades                                            346.4  7940.3   \n",
       "klamath_mountains_california_high_north_coast_r...  371.4  7164.3   \n",
       "eastern_cascades_slopes_and_foothills               194.4  3395.5   \n",
       "northern_rockies                                    151.3  2354.3   \n",
       "puget_lowland                                       294.7  6154.1   \n",
       "willamette_valley                                   328.0  9516.5   \n",
       "\n",
       "                                                    ABOVEGROUND_BIOMASS  \n",
       "ECOREGION3                                                               \n",
       "blue_mountains                                                     44.8  \n",
       "coast_range                                                       232.1  \n",
       "north_cascades                                                    153.8  \n",
       "cascades                                                          197.5  \n",
       "klamath_mountains_california_high_north_coast_r...                156.6  \n",
       "eastern_cascades_slopes_and_foothills                              86.6  \n",
       "northern_rockies                                                   64.8  \n",
       "puget_lowland                                                     177.8  \n",
       "willamette_valley                                                 242.7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIDAR_COVER</th>\n",
       "      <th>TOPHT</th>\n",
       "      <th>QMD</th>\n",
       "      <th>SDI</th>\n",
       "      <th>TCUFT</th>\n",
       "      <th>ABOVEGROUND_BIOMASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "      <td>7708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>66.853895</td>\n",
       "      <td>96.780747</td>\n",
       "      <td>14.296492</td>\n",
       "      <td>315.817592</td>\n",
       "      <td>7410.013882</td>\n",
       "      <td>181.553321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.918710</td>\n",
       "      <td>39.322274</td>\n",
       "      <td>8.100335</td>\n",
       "      <td>177.210972</td>\n",
       "      <td>6312.855165</td>\n",
       "      <td>121.967801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.427506</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.677923</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>8.730670</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2579.750000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.035060</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>12.820993</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>5871.500000</td>\n",
       "      <td>165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>91.370873</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>17.797910</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>10565.000000</td>\n",
       "      <td>246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>82.039021</td>\n",
       "      <td>1307.000000</td>\n",
       "      <td>51330.000000</td>\n",
       "      <td>887.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIDAR_COVER        TOPHT          QMD          SDI         TCUFT  \\\n",
       "count  7708.000000  7708.000000  7708.000000  7708.000000   7708.000000   \n",
       "mean     66.853895    96.780747    14.296492   315.817592   7410.013882   \n",
       "std      30.918710    39.322274     8.100335   177.210972   6312.855165   \n",
       "min       0.000000     6.000000     1.427506     2.000000      1.000000   \n",
       "25%      49.677923    68.000000     8.730670   181.000000   2579.750000   \n",
       "50%      80.035060    94.000000    12.820993   310.000000   5871.500000   \n",
       "75%      91.370873   123.000000    17.797910   435.000000  10565.000000   \n",
       "max     100.000000   267.000000    82.039021  1307.000000  51330.000000   \n",
       "\n",
       "       ABOVEGROUND_BIOMASS  \n",
       "count          7708.000000  \n",
       "mean            181.553321  \n",
       "std             121.967801  \n",
       "min               2.000000  \n",
       "25%              88.000000  \n",
       "50%             165.000000  \n",
       "75%             246.000000  \n",
       "max             887.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_REGIONS = ['blue_mountains', 'coast_range', 'north_cascades', 'cascades',\n",
    "               'klamath_mountains_california_high_north_coast_range', \n",
    "               'eastern_cascades_slopes_and_foothills', 'northern_rockies',\n",
    "               'puget_lowland', 'willamette_valley']\n",
    "display(df.groupby('ECOREGION3')[Y_COLS].mean().round(1).loc[USE_REGIONS])\n",
    "display(df[Y_COLS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "X, Y = df[X_COLS], df[Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7708 entries, 0 to 7707\n",
      "Data columns (total 57 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   S2_R_LEAFOFF           7708 non-null   float64\n",
      " 1   S2_G_LEAFOFF           7708 non-null   float64\n",
      " 2   S2_B_LEAFOFF           7708 non-null   float64\n",
      " 3   S2_NIR_LEAFOFF         7708 non-null   float64\n",
      " 4   S2_SWIR1_LEAFOFF       7708 non-null   float64\n",
      " 5   S2_SWIR2_LEAFOFF       7708 non-null   float64\n",
      " 6   S2_RE1_LEAFOFF         7708 non-null   float64\n",
      " 7   S2_RE2_LEAFOFF         7708 non-null   float64\n",
      " 8   S2_RE3_LEAFOFF         7708 non-null   float64\n",
      " 9   S2_RE4_LEAFOFF         7708 non-null   float64\n",
      " 10  S2_R_LEAFON            7708 non-null   float64\n",
      " 11  S2_G_LEAFON            7708 non-null   float64\n",
      " 12  S2_B_LEAFON            7708 non-null   float64\n",
      " 13  S2_NIR_LEAFON          7708 non-null   float64\n",
      " 14  S2_SWIR1_LEAFON        7708 non-null   float64\n",
      " 15  S2_SWIR2_LEAFON        7708 non-null   float64\n",
      " 16  S2_RE1_LEAFON          7708 non-null   float64\n",
      " 17  S2_RE2_LEAFON          7708 non-null   float64\n",
      " 18  S2_RE3_LEAFON          7708 non-null   float64\n",
      " 19  S2_RE4_LEAFON          7708 non-null   float64\n",
      " 20  S2_NDVI_LEAFON         7708 non-null   float64\n",
      " 21  S2_SAVI_LEAFON         7708 non-null   float64\n",
      " 22  S2_BRIGHTNESS_LEAFON   7708 non-null   float64\n",
      " 23  S2_GREENNESS_LEAFON    7708 non-null   float64\n",
      " 24  S2_WETNESS_LEAFON      7708 non-null   float64\n",
      " 25  S2_NDVI_LEAFOFF        7708 non-null   float64\n",
      " 26  S2_SAVI_LEAFOFF        7708 non-null   float64\n",
      " 27  S2_BRIGHTNESS_LEAFOFF  7708 non-null   float64\n",
      " 28  S2_GREENNESS_LEAFOFF   7708 non-null   float64\n",
      " 29  S2_WETNESS_LEAFOFF     7708 non-null   float64\n",
      " 30  S2_dR                  7708 non-null   float64\n",
      " 31  S2_dG                  7708 non-null   float64\n",
      " 32  S2_dB                  7708 non-null   float64\n",
      " 33  S2_dNIR                7708 non-null   float64\n",
      " 34  S2_dSWIR1              7708 non-null   float64\n",
      " 35  S2_dSWIR2              7708 non-null   float64\n",
      " 36  S2_dRE1                7708 non-null   float64\n",
      " 37  S2_dRE2                7708 non-null   float64\n",
      " 38  S2_dNDVI               7708 non-null   float64\n",
      " 39  S2_dSAVI               7708 non-null   float64\n",
      " 40  S2_dBRIGHTNESS         7708 non-null   float64\n",
      " 41  S2_dGREENNESS          7708 non-null   float64\n",
      " 42  S2_dWETNESS            7708 non-null   float64\n",
      " 43  S2_dRE3                7708 non-null   float64\n",
      " 44  S2_dRE4                7708 non-null   float64\n",
      " 45  LT_DUR_NBR             7708 non-null   int64  \n",
      " 46  LT_DUR_SWIR1           7708 non-null   int64  \n",
      " 47  LT_MAG_NBR             7708 non-null   int64  \n",
      " 48  LT_MAG_SWIR1           7708 non-null   int64  \n",
      " 49  LT_RATE_NBR            7708 non-null   int64  \n",
      " 50  LT_RATE_SWIR1          7708 non-null   int64  \n",
      " 51  LT_YSD_NBR             7708 non-null   int64  \n",
      " 52  LT_YSD_SWIR1           7708 non-null   int64  \n",
      " 53  ELEVATION              7708 non-null   float64\n",
      " 54  LAT                    7708 non-null   float64\n",
      " 55  LON                    7708 non-null   float64\n",
      " 56  ECOREGION3             7708 non-null   object \n",
      "dtypes: float64(48), int64(8), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df[X_COLS].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7708 entries, 0 to 7707\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   LIDAR_COVER          7708 non-null   float64\n",
      " 1   TOPHT                7708 non-null   int64  \n",
      " 2   QMD                  7708 non-null   float64\n",
      " 3   SDI                  7708 non-null   int64  \n",
      " 4   TCUFT                7708 non-null   int64  \n",
      " 5   ABOVEGROUND_BIOMASS  7708 non-null   int64  \n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 361.4 KB\n"
     ]
    }
   ],
   "source": [
    "df[Y_COLS].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets by ecoregion\n",
    "We want to explore model transferability between regions, so we'll train models independently on subsets of the data within a single ecoregion, as well as a model that is trained on all available ecoregions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregions = list(np.sort([reg for reg in pd.unique(df.ECOREGION3) if ecoreg_counts.loc[reg]['UUID'] > 20]))\n",
    "\n",
    "eco_X_idx = [X.loc[X.ECOREGION3 == eco].index.values for eco in ecoregions]\n",
    "\n",
    "eco_X_dfs = [X.loc[X.ECOREGION3 == eco].drop(['ECOREGION3'], axis=1) for eco in ecoregions]\n",
    "eco_Y_dfs = [Y.loc[idx] for idx in eco_X_idx]\n",
    "\n",
    "# append a \"global\" model that contains data from all ecoregions\n",
    "ecoregions.append('all')\n",
    "ecoregion_names = ['_'.join(x.split('_')[0:2]) for x in ecoregions]\n",
    "eco_X_dfs.append(X.drop(['ECOREGION3'], axis=1))\n",
    "eco_Y_dfs.append(Y)\n",
    "\n",
    "ecoregion_display_names = [' '.join(x.upper().split('_')[:2]) for x in ecoregions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_class_bins = [10,40,70,100]\n",
    "cover_class_labels = ['OPEN', 'MODERATE', 'CLOSED']\n",
    "height_class_bins = np.arange(0,300,20)\n",
    "height_class_labels = [f'{x}-{x+20}' for x in height_class_bins[:-1]]\n",
    "diameter_class_bins = [1, 5, 10, 15, 20, 999]\n",
    "diameter_class_labels = ['SEED/SAP', 'SMALL', 'MEDIUM', 'LARGE', 'VERY_LARGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "We'll use Root Mean Square Error to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, pred):\n",
    "    return np.sqrt((np.square(obs-pred)).mean())\n",
    "\n",
    "def nrmse(obs, pred):\n",
    "    return rmse(pred,obs) / obs.mean()\n",
    "\n",
    "def mae(obs, pred):   \n",
    "    return abs(pred - obs).mean()\n",
    "\n",
    "def mape(obs, pred):    \n",
    "    return abs(pred - obs).mean() / obs.mean()\n",
    "\n",
    "def bias(obs, pred):   \n",
    "    return (pred - obs).mean()\n",
    "\n",
    "def rel_bias(obs, pred):\n",
    "    return bias(pred,obs) / obs.mean()\n",
    "\n",
    "def bin_accuracy(obs, pred, bins, fuzzy_tol=0):\n",
    "    pred_binned = np.digitize(pred, bins)\n",
    "    obs_binned = np.digitisze(obs, bins)\n",
    "    diff = abs(pred_binned - obs_binned)\n",
    "    \n",
    "    return (diff <= fuzzy_tol).sum() / len(diff)\n",
    "\n",
    "def confidence_interval_half(X, confidence=0.95):\n",
    "    n = len(X)\n",
    "    se = stats.sem(X)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function will calculate RMSE scores for each regionally-trained model and the global model on each ecoregion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit some models\n",
    "For each type of model, we'll employ cross-validation to tune model hyperparameters, generating a tuned model for each ecoregion as well as a tuned model using all training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'ElasticNet': ElasticNet(selection='random'),\n",
    "    'Lasso': Lasso(), \n",
    "    'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_jobs=-1), \n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(), \n",
    "}\n",
    "\n",
    "FIT_PARAMS = {\n",
    "    'ElasticNet': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "        'l1_ratio': np.arange(0.1, 1.0, 0.1),\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [1,2,3,4,5,10,20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'max_depth': [5, 20, None],\n",
    "        'max_samples': [0.5, None]\n",
    "    },\n",
    "    'HistGradientBoostingRegressor': {\n",
    "        'max_iter': [50, 100, 200],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTER_FOLDS = 5\n",
    "NUM_INNER_FOLDS = 3\n",
    "SCORE_FUNCS = [rmse, nrmse, mae, mape, bias, rel_bias]\n",
    "score_names = [func.__name__ for func in SCORE_FUNCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_insider_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['cv_results'] = {}\n",
    "            for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "                fold_num = fold_idx + 1\n",
    "                results[region][model_name]['cv_results'][fold_num] = {}\n",
    "                results[region][model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "                results[region][model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[region][model_name]['cv_results'][fold_num][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_global_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for model_name in model_names:\n",
    "        results[model_name] = {}\n",
    "        results[model_name]['fitted_model'] = None\n",
    "        results[model_name]['best_params'] = None\n",
    "        results[model_name]['cv_results'] = {}\n",
    "        for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "            fold_num = fold_idx + 1\n",
    "            results[model_name]['cv_results'][fold_num] = {}\n",
    "            results[model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "            results[model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "            for region in regions:\n",
    "                results[model_name]['cv_results'][fold_num][region] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[model_name]['cv_results'][fold_num][region][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_outsider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['predict_time'] = None\n",
    "            for score_func in score_funcs:\n",
    "                score_func_name = score_func.__name__\n",
    "                results[region][model_name][score_func_name] = {\n",
    "                    y: None for y in target_vars\n",
    "                }\n",
    "    return results\n",
    "\n",
    "def build_visiting_insider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for target_region in regions:\n",
    "        results[target_region] = {}\n",
    "        for train_region in [r for r in regions if r != target_region]:\n",
    "            results[target_region][train_region] = {}\n",
    "            for model_name in model_names:\n",
    "                results[target_region][train_region][model_name] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[target_region][train_region][model_name][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_MODELS = '../models/structure_models2023'\n",
    "os.makedirs(OUT_MODELS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_insider_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X = eco_X_dfs[i]\n",
    "        Y = eco_Y_dfs[i]\n",
    "        outer_groups = df.loc[X.index, 'UUID'].values\n",
    "        \n",
    "        outer_fold_num = 1\n",
    "        for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "            X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "            Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "            inner_groups = df.loc[X_train.index, 'UUID'].values\n",
    "            \n",
    "            inner_search = GridSearchCV(pipe, search_params, \n",
    "                                        scoring='neg_mean_squared_error', \n",
    "                                        n_jobs=-1, cv=cv_inner, refit=True)\n",
    "            \n",
    "            inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "            \n",
    "            inner_best_model = inner_result.best_estimator_\n",
    "            start_time = time.time()\n",
    "            Y_pred = inner_best_model.predict(X_test)\n",
    "            end_time = time.time()\n",
    "            total_predict_time = end_time - start_time\n",
    "            avg_predict_time = total_predict_time / len(X_test)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(Y_test, Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    insider_results[ecoregion][model_name]['cv_results'][outer_fold_num][score_func_name][y_var] = scores.loc[y_var]\n",
    "                    \n",
    "            print(outer_fold_num, end='... ')\n",
    "            outer_fold_num += 1\n",
    "        print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "        # done with scoring of models, now time to tune a model using the whole dataset\n",
    "        outer_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_outer, refit=True)\n",
    "        outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "        \n",
    "        # now fit on the entire dataset, not just training set\n",
    "        model = outer_result.best_estimator_\n",
    "        model.set_params(**outer_result.best_params_)\n",
    "        X = df.loc[df.ECOREGION3 == ecoregion, X_COLS].drop(['ECOREGION3'], axis=1)\n",
    "        y = df.loc[df.ECOREGION3 == ecoregion, Y_COLS]\n",
    "        model.fit(X, y)\n",
    "\n",
    "        eco_name = '_'.join(ecoregion.split('_')[:2])\n",
    "        outfile = f'{eco_name}-sentinel-{model_name}-chained.pkl'\n",
    "        outpath = os.path.join(OUT_MODELS, outfile)\n",
    "        with open(outpath, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "        insider_results[ecoregion][model_name]['fitted_model'] = model\n",
    "        insider_results[ecoregion][model_name]['best_params'] = outer_result.best_params_\n",
    "        \n",
    "        cv_results_dict = {ecoregion: insider_results[ecoregion][model_name]['cv_results'] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return cv_results_dict\n",
    "\n",
    "def tune_outsider_model(model_name, num_folds=5):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    groupkfold = GroupKFold(num_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X_train = X.loc[X.ECOREGION3 != ecoregion].drop('ECOREGION3', axis=1)\n",
    "        Y_train = Y.loc[X_train.index]\n",
    "        X_test = X.loc[X.ECOREGION3 == ecoregion].drop('ECOREGION3', axis=1)\n",
    "        Y_test = Y.loc[X_test.index]\n",
    "        groups = df.loc[X_train.index]['ECOREGION3'].values\n",
    "        \n",
    "        search = GridSearchCV(pipe, search_params, \n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1, cv=groupkfold, refit=True)\n",
    "            \n",
    "        result = search.fit(X_train, Y_train, groups=groups)\n",
    "        print('Done fitting, now scoring', end='... ')\n",
    "        outsider_results[ecoregion][model_name]['best_params'] = result.best_params_\n",
    "        outsider_results[ecoregion][model_name]['fitted_model'] = result.best_estimator_\n",
    "        \n",
    "        best_model = result.best_estimator_       \n",
    "        start_time = time.time()\n",
    "        Y_pred = best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        outsider_results[ecoregion][model_name]['predict_time'] = avg_predict_time\n",
    "            \n",
    "        for score_func in SCORE_FUNCS:\n",
    "            score_func_name = score_func.__name__\n",
    "            scores = score_func(Y_test, Y_pred)\n",
    "            for y_var in scores.index:\n",
    "                outsider_results[ecoregion][model_name][score_func_name][y_var] = scores.loc[y_var]\n",
    "        \n",
    "        results_dict = {ecoregion: outsider_results[ecoregion][model_name] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def tune_global_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    print(f'Scoring with {NUM_OUTER_FOLDS} folds... ', end='')\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    test_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    X = df[X_COLS].drop('ECOREGION3', axis=1)\n",
    "    Y = df[Y_COLS]\n",
    "    outer_groups = df['UUID'].values\n",
    "        \n",
    "    outer_fold_num = 1\n",
    "    for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "        X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "        Y_train, Y_test = Y.loc[train_ix], Y.loc[test_ix]\n",
    "        inner_groups = df.loc[train_ix, 'UUID'].values\n",
    "\n",
    "        inner_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "        inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "\n",
    "        inner_best_model = inner_result.best_estimator_\n",
    "        start_time = time.time()\n",
    "        Y_pred = inner_best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "\n",
    "        for ecoregion in test_regions:\n",
    "            region_mask = (df.loc[test_ix, 'ECOREGION3'] == ecoregion).values\n",
    "            regional_X_test = X_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_test = Y_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_pred = inner_best_model.predict(regional_X_test)\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(regional_Y_test, regional_Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    global_results[model_name]['cv_results'][outer_fold_num][ecoregion][score_func_name][y_var] = scores.loc[y_var]\n",
    "\n",
    "        print(outer_fold_num, end='... ')\n",
    "        outer_fold_num += 1\n",
    "    \n",
    "    print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "    # done with scoring of models, now time to tune a model using the whole dataset\n",
    "    outer_search = GridSearchCV(pipe, search_params, \n",
    "                                scoring='neg_mean_squared_error', \n",
    "                                n_jobs=-1, cv=cv_outer, refit=True)\n",
    "    outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "\n",
    "     # now fit on the entire dataset, not just training set\n",
    "    model = outer_result.best_estimator_\n",
    "    model.set_params(**outer_result.best_params_)\n",
    "    X = df[X_COLS].drop(['ECOREGION3'], axis=1)\n",
    "    y = df[Y_COLS]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    outfile = f'global-sentinel-{model_name}-chained.pkl'\n",
    "    outpath = os.path.join(OUT_MODELS, outfile)\n",
    "    with open(outpath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print('All done.')\n",
    "\n",
    "    global_results[model_name]['fitted_model'] = outer_result.best_estimator_\n",
    "    global_results[model_name]['best_params'] = outer_result.best_params_\n",
    "\n",
    "    results_dict = global_results[model_name]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Global Models\n",
    "These models get to see data from every ecoregion during training and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = build_global_results_dictionary(ecoregions[:-1], MODELS.keys(), NUM_OUTER_FOLDS, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_global_results(results):\n",
    "    data = []\n",
    "    for fold in range(NUM_OUTER_FOLDS):\n",
    "        for ecoregion in ecoregions[:-1]:\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold+1, ecoregion, target, score_name, results['cv_results'][fold+1][ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... "
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "elastic_global = tune_global_model('ElasticNet')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"\"\n",
    "lasso_global = tune_global_model('Lasso')\n",
    "knn_global = tune_global_model('KNeighborsRegressor')\n",
    "rf_global = tune_global_model('RandomForestRegressor')\n",
    "gbm_global = tune_global_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_global, lasso_global, knn_global, rf_global, gbm_global]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_global_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_global_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_global_results['ecoregion'] = all_global_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_global_results.columns = [col.upper() for col in all_global_results.columns]\n",
    "# all_global_results = all_global_results.rename({'SCORE': 'MAE'}, axis=1)\n",
    "all_global_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_global_results.to_csv('../data/processed/nestedcv_chained_global_results_satellite_structure_lidar_cover.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Outsider Models\n",
    "These models have data from the ecoregion they're tested on held out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsider_results = build_outsider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_outsider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for target in Y_COLS:\n",
    "            for score_name in score_names:\n",
    "                data.append((np.nan, ecoregion, target, score_name, results[ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "elastic_outsider = tune_outsider_model('ElasticNet')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"\"\n",
    "lasso_outsider = tune_outsider_model('Lasso')\n",
    "knn_outsider = tune_outsider_model('KNeighborsRegressor')\n",
    "rf_outsider = tune_outsider_model('RandomForestRegressor')\n",
    "gbm_outsider = tune_outsider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>rmse</td>\n",
       "      <td>16.072562</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.486345</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>mae</td>\n",
       "      <td>12.739182</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.385479</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>bias</td>\n",
       "      <td>10.434625</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC      SCORE       MODEL\n",
       "0      NaN  Blue Mountains  LIDAR_COVER   rmse  16.072562  ElasticNet\n",
       "1      NaN  Blue Mountains  LIDAR_COVER  nrmse   0.486345  ElasticNet\n",
       "2      NaN  Blue Mountains  LIDAR_COVER    mae  12.739182  ElasticNet\n",
       "3      NaN  Blue Mountains  LIDAR_COVER   mape   0.385479  ElasticNet\n",
       "4      NaN  Blue Mountains  LIDAR_COVER   bias  10.434625  ElasticNet"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_outsider, lasso_outsider, knn_outsider, rf_outsider, gbm_outsider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_outsider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_outsider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_outsider_results['ecoregion'] = all_outsider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_outsider_results.columns = [col.upper() for col in all_outsider_results.columns]\n",
    "all_outsider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outsider_results.to_csv('../data/processed/nestedcv_chained_outsider_results_satellite_structure_lidar_cover.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Insider Models\n",
    "These models are trained with observations from a single ecoregion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_results = build_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), 5, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_insider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for fold_num in results[ecoregion].keys():\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold_num, ecoregion, target, score_name, results[ecoregion][fold_num][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "elastic_insider = tune_insider_model('ElasticNet')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"\"\n",
    "lasso_insider = tune_insider_model('Lasso')\n",
    "knn_insider = tune_insider_model('KNeighborsRegressor')\n",
    "rf_insider = tune_insider_model('RandomForestRegressor')\n",
    "gbm_insider = tune_insider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>rmse</td>\n",
       "      <td>9.404883</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.276058</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>mae</td>\n",
       "      <td>7.190930</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.211072</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>bias</td>\n",
       "      <td>2.102870</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC     SCORE       MODEL\n",
       "0        1  Blue Mountains  LIDAR_COVER   rmse  9.404883  ElasticNet\n",
       "1        1  Blue Mountains  LIDAR_COVER  nrmse  0.276058  ElasticNet\n",
       "2        1  Blue Mountains  LIDAR_COVER    mae  7.190930  ElasticNet\n",
       "3        1  Blue Mountains  LIDAR_COVER   mape  0.211072  ElasticNet\n",
       "4        1  Blue Mountains  LIDAR_COVER   bias  2.102870  ElasticNet"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_insider, lasso_insider, knn_insider, rf_insider, gbm_insider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_insider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_insider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_insider_results['ecoregion'] = all_insider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_insider_results.columns = [col.upper() for col in all_insider_results.columns]\n",
    "all_insider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insider_results.to_csv('../data/processed/nestedcv_chained_insider_results_satellite_structure_lidar_cover.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trained Insider Models to Score Visiting Insider Models\n",
    "These models are trained on a single region, and scored on other regions they've never seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_results = build_visiting_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET_ECOREGION</th>\n",
       "      <th>TRAIN_ECOREGION</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>LIDAR_COVER</td>\n",
       "      <td>27.573275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>TOPHT</td>\n",
       "      <td>33.872656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>QMD</td>\n",
       "      <td>16.857373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>SDI</td>\n",
       "      <td>216.018045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>TCUFT</td>\n",
       "      <td>5116.483351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TARGET_ECOREGION TRAIN_ECOREGION       MODEL METRIC       TARGET  \\\n",
       "0   Blue Mountains        Cascades  ElasticNet   rmse  LIDAR_COVER   \n",
       "1   Blue Mountains        Cascades  ElasticNet   rmse        TOPHT   \n",
       "2   Blue Mountains        Cascades  ElasticNet   rmse          QMD   \n",
       "3   Blue Mountains        Cascades  ElasticNet   rmse          SDI   \n",
       "4   Blue Mountains        Cascades  ElasticNet   rmse        TCUFT   \n",
       "\n",
       "         SCORE  \n",
       "0    27.573275  \n",
       "1    33.872656  \n",
       "2    16.857373  \n",
       "3   216.018045  \n",
       "4  5116.483351  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitor_results = []\n",
    "for target_region in ecoregions[:-1]:\n",
    "    for train_region in [r for r in ecoregions[:-1] if r != target_region]:\n",
    "        for model_name in MODELS.keys():\n",
    "            model = insider_results[train_region][model_name]['fitted_model']\n",
    "            targ_idx = X.loc[X.ECOREGION3 == target_region].index.values\n",
    "            targ_X = X.loc[targ_idx].drop(['ECOREGION3'], axis=1)\n",
    "            pred = model.predict(targ_X)\n",
    "            obs = Y.loc[targ_idx]\n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(obs, pred)\n",
    "                for y, score in scores.iteritems():\n",
    "                    visitor_results.append(\n",
    "                        (' '.join(target_region.title().replace('_',' ').split()),\n",
    "                         ' '.join(train_region.title().replace('_',' ').split()),\n",
    "                         model_name, score_func_name, y, score))\n",
    "visitor_df = pd.DataFrame(visitor_results, \n",
    "                          columns = ['TARGET_ECOREGION', 'TRAIN_ECOREGION', \n",
    "                                     'MODEL', 'METRIC', 'TARGET', 'SCORE'])\n",
    "visitor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_df.to_csv('../data/processed/nestedcv_chained_visitor_results_satellite_structure_lidar_cover.csv', \n",
    "                  header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest_mapping]",
   "language": "python",
   "name": "conda-env-forest_mapping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
