{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep and model-tuning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import (make_scorer, cohen_kappa_score, \n",
    "                             precision_recall_curve, confusion_matrix, \n",
    "                             plot_precision_recall_curve, precision_score, \n",
    "                             recall_score, f1_score)\n",
    "\n",
    "# types of models we'll fit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# for saving things\n",
    "import pickle\n",
    "# skopt generates warnings if the same parameter set is sampled again\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ordinal_classifiers import (RandomForestOrdinalClassifier, \n",
    "                                            LogisticRegressionOrdinalClassifier, \n",
    "                                            SVMOrdinalClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from sklearn.calibration import calibration_curve\n",
    "from imblearn.metrics import classification_report_imbalanced, sensitivity_specificity_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31,599 samples\n",
      "Columns: ['uuid' 'year' 'FVS_TPA' 'FVS_BA' 'FVS_SDI' 'FVS_CCF' 'FVS_QMD'\n",
      " 'FVS_TCUFT' 'FVS_TOPHT' 'FVS_NUMBER_OF_STRATA' 'FVS_TOTAL_COVER'\n",
      " 'FVS_STRUCTURE_CLASS' 'FVS_CANOPY_BASEHEIGHT' 'FVS_CANOPY_BULKDENSITY'\n",
      " 'FVS_ABOVEGROUND_BIOMASS' 'FVS_ABOVEGROUND_CARBON' 'FVS_GS_TPA' 'FVS_AF'\n",
      " 'FVS_AS' 'FVS_BM' 'FVS_BO' 'FVS_CH' 'FVS_CW' 'FVS_DF' 'FVS_DG' 'FVS_ES'\n",
      " 'FVS_GC' 'FVS_GF' 'FVS_IC' 'FVS_JP' 'FVS_KP' 'FVS_LO' 'FVS_LP' 'FVS_MA'\n",
      " 'FVS_MC' 'FVS_MH' 'FVS_NF' 'FVS_OH' 'FVS_OS' 'FVS_OT' 'FVS_PC' 'FVS_PL'\n",
      " 'FVS_PP' 'FVS_PY' 'FVS_RA' 'FVS_RC' 'FVS_RF' 'FVS_SF' 'FVS_SH' 'FVS_SP'\n",
      " 'FVS_SS' 'FVS_TO' 'FVS_WA' 'FVS_WB' 'FVS_WF' 'FVS_WH' 'FVS_WI' 'FVS_WJ'\n",
      " 'FVS_WL' 'FVS_WO' 'FVS_WP' 'FVS_YC' 'FVS_TRUE_FIR' 'FVS_OTHER_HARDWOOD'\n",
      " 'FVS_MAPLE' 'FVS_OAK' 'FVS_DOUGLAS_FIR' 'FVS_SPRUCE' 'FVS_CEDAR'\n",
      " 'FVS_PONDEROSA_PINE' 'FVS_OTHER_SOFTWOOD' 'FVS_LODGEPOLE_PINE'\n",
      " 'FVS_HEMLOCK' 'FVS_RED_ALDER' 'FVS_TANOAK' 'FVS_JUNIPER' 'FVS_LARCH'\n",
      " 'S1_VV_LEAFOFF' 'S1_VH_LEAFOFF' 'S1_VV_LEAFON' 'S1_VH_LEAFON'\n",
      " 'S2_R_LEAFOFF' 'S2_G_LEAFOFF' 'S2_B_LEAFOFF' 'S2_NIR_LEAFOFF'\n",
      " 'S2_SWIR1_LEAFOFF' 'S2_SWIR2_LEAFOFF' 'S2_RE1_LEAFOFF' 'S2_RE2_LEAFOFF'\n",
      " 'S2_RE3_LEAFOFF' 'S2_RE4_LEAFOFF' 'S2_R_LEAFON' 'S2_G_LEAFON'\n",
      " 'S2_B_LEAFON' 'S2_NIR_LEAFON' 'S2_SWIR1_LEAFON' 'S2_SWIR2_LEAFON'\n",
      " 'S2_RE1_LEAFON' 'S2_RE2_LEAFON' 'S2_RE3_LEAFON' 'S2_RE4_LEAFON' 'P_HH'\n",
      " 'P_HV' 'L8_R_LEAFOFF' 'L8_G_LEAFOFF' 'L8_B_LEAFOFF' 'L8_NIR_LEAFOFF'\n",
      " 'L8_SWIR1_LEAFOFF' 'L8_SWIR2_LEAFOFF' 'L8_R_LEAFON' 'L8_G_LEAFON'\n",
      " 'L8_B_LEAFON' 'L8_NIR_LEAFON' 'L8_SWIR1_LEAFON' 'L8_SWIR2_LEAFON'\n",
      " 'LT_DUR_NBR' 'LT_DUR_SWIR1' 'LT_MAG_NBR' 'LT_MAG_SWIR1' 'LT_RATE_NBR'\n",
      " 'LT_RATE_SWIR1' 'LT_YSD_NBR' 'LT_YSD_SWIR1' 'lat' 'lon' 'orig_id'\n",
      " 'source' 'meas_yr' 'ecoregion3' 'agency' 'plot_size_ac' 'ASPECT'\n",
      " 'elevation' 'OVERALL_CURVATURE' 'PLAN_CURVATURE' 'PROFILE_CURVATURE'\n",
      " 'SLOPE' 'SOLAR_RADIATION_INDEX' 'distance_to_water_m']\n"
     ]
    }
   ],
   "source": [
    "DATA = '../data/processed/satellite_annual_training.csv'\n",
    "df = pd.read_csv(DATA).rename({'ELEVATION': 'elevation'}, axis=1)\n",
    "# df = df.dropna()\n",
    "print('{:,d} samples'.format(len(df)))\n",
    "print('Columns:', df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 258 entries, 0 to 257\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   outlier_uuid  258 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "LIDAR_OUTLIERS = '../data/interim/outlier_uuids.csv'\n",
    "outliers = pd.read_csv(LIDAR_OUTLIERS)\n",
    "outliers['outlier_uuid'] = outliers['outlier_uuid'].str[0:8]\n",
    "outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28046 entries, 0 to 31598\n",
      "Columns: 140 entries, uuid to year_diff\n",
      "dtypes: float64(51), int64(83), object(6)\n",
      "memory usage: 30.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[~df.uuid.isin(outliers.outlier_uuid)]\n",
    "df['year_diff'] = (df['year'] - df['meas_yr']).astype(int)\n",
    "df = df.loc[~np.logical_or(df.year_diff > df.LT_YSD_SWIR1, df.year_diff > df.LT_YSD_NBR)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate  few additional indices for sentinel-2 and landsat\n",
    "for season in ['LEAFON', 'LEAFOFF']:\n",
    "    for sensor in ['L8', 'S2']:\n",
    "        R, G, B = f'{sensor}_R_{season}', f'{sensor}_G_{season}', f'{sensor}_B_{season}'\n",
    "        NIR, SWIR1, SWIR2 =  f'{sensor}_NIR_{season}', f'{sensor}_SWIR1_{season}', f'{sensor}_SWIR2_{season}'\n",
    "\n",
    "        NDVI = f'{sensor}_NDVI_{season}'\n",
    "        df[NDVI] = (df[NIR] - df[R])/(df[NIR] + df[R])\n",
    "\n",
    "        ENDVI = f'{sensor}_ENDVI_{season}'\n",
    "        df[NDVI] = (df[NIR] + df[G] - 2*df[B])/(df[NIR] + df[G] + 2*df[B])\n",
    "\n",
    "        SAVI = f'{sensor}_SAVI_{season}'\n",
    "        df[SAVI] = 1.5*(df[NIR] - df[R])/(df[NIR] + df[R] + 0.5)\n",
    "        \n",
    "        BRIGHTNESS = f'{sensor}_BRIGHTNESS_{season}'\n",
    "        df[BRIGHTNESS] = 0.3029*df[B] + 0.2786*df[G] + 0.4733*df[R] + 0.5599*df[NIR] + 0.508*df[SWIR1] + 0.1872*df[SWIR2]\n",
    "        \n",
    "        GREENNESS = f'{sensor}_GREENNESS_{season}'\n",
    "        df[GREENNESS] = -0.2941*df[B] + -0.243*df[G] + -0.5424*df[R] + 0.7276*df[NIR] + 0.0713*df[SWIR1] + -0.1608*df[SWIR2]\n",
    "        \n",
    "        WETNESS = f'{sensor}_WETNESS_{season}'\n",
    "        df[WETNESS] = 0.1511*df[B] + 0.1973*df[G] + 0.3283*df[R] + 0.3407*df[NIR] + -0.7117*df[SWIR1] + -0.4559*df[SWIR2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in ['L8', 'S2']:\n",
    "    bands = np.unique([col.split(f'{sensor}_')[1].split('_')[0] for col in df.columns if col.startswith(sensor)])\n",
    "    for band in bands:\n",
    "        col = f'{sensor}_d{band}'\n",
    "        df[col] = df[f'{sensor}_{band}_LEAFON'] - df[f'{sensor}_{band}_LEAFOFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 19831207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting initial features and targets\n",
    "This is the first step in determining what features we want to use, and what we want to predict. Later, we can still apply some procedures to choose a subset of these features to make simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this threshold to all the species-group-level basal areas for the plots to create a binary presence/absence classification for each species group on each plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANDTRENDR\n",
    "LT_COLS = ['LT_DUR_NBR', 'LT_DUR_SWIR1', 'LT_MAG_NBR', 'LT_MAG_SWIR1', \n",
    "           'LT_RATE_NBR', 'LT_RATE_SWIR1', 'LT_YSD_NBR', 'LT_YSD_SWIR1']\n",
    "# LANDSAT\n",
    "L8_COLS = [\n",
    "    'L8_R_LEAFOFF', 'L8_G_LEAFOFF', 'L8_B_LEAFOFF', \n",
    "    'L8_NIR_LEAFOFF', 'L8_SWIR1_LEAFOFF', 'L8_SWIR2_LEAFOFF', \n",
    "    'L8_NDVI_LEAFOFF', 'L8_SAVI_LEAFOFF', 'L8_BRIGHTNESS_LEAFOFF', 'L8_GREENNESS_LEAFOFF', 'L8_WETNESS_LEAFOFF',\n",
    "    'L8_R_LEAFON', 'L8_G_LEAFON', 'L8_B_LEAFON', \n",
    "    'L8_NIR_LEAFON', 'L8_SWIR1_LEAFON', 'L8_SWIR2_LEAFON',\n",
    "    'L8_NDVI_LEAFON', 'L8_SAVI_LEAFON', 'L8_BRIGHTNESS_LEAFON', 'L8_GREENNESS_LEAFON', 'L8_WETNESS_LEAFON',\n",
    "    'L8_dR', 'L8_dG', 'L8_dB', \n",
    "    'L8_dNIR', 'L8_dSWIR1', 'L8_dSWIR2',\n",
    "    'L8_dNDVI', 'L8_dSAVI', 'L8_dBRIGHTNESS', 'L8_dGREENNESS', 'L8_dWETNESS'\n",
    "]\n",
    "# SENTINEL-1\n",
    "S1_COLS = [\n",
    "    'S1_VV_LEAFOFF', 'S1_VH_LEAFOFF', \n",
    "    'S1_VV_LEAFON', 'S1_VH_LEAFON'\n",
    "]\n",
    "# SENTINEL-2\n",
    "S2_COLS = [\n",
    "    'S2_R_LEAFOFF', 'S2_G_LEAFOFF', 'S2_B_LEAFOFF', \n",
    "    'S2_NIR_LEAFOFF', 'S2_SWIR1_LEAFOFF', 'S2_SWIR2_LEAFOFF', \n",
    "    'S2_RE1_LEAFOFF', 'S2_RE2_LEAFOFF', 'S2_RE3_LEAFOFF', 'S2_RE4_LEAFOFF', \n",
    "    'S2_NDVI_LEAFOFF', 'S2_SAVI_LEAFOFF', 'S2_BRIGHTNESS_LEAFOFF', 'S2_GREENNESS_LEAFOFF', 'S2_WETNESS_LEAFOFF',\n",
    "    'S2_R_LEAFON', 'S2_G_LEAFON', 'S2_B_LEAFON', \n",
    "    'S2_NIR_LEAFON', 'S2_SWIR1_LEAFON', 'S2_SWIR2_LEAFON', \n",
    "    'S2_RE1_LEAFON', 'S2_RE2_LEAFON', 'S2_RE3_LEAFON', 'S2_RE4_LEAFON',\n",
    "    'S2_NDVI_LEAFON', 'S2_SAVI_LEAFON', 'S2_BRIGHTNESS_LEAFON', 'S2_GREENNESS_LEAFON', 'S2_WETNESS_LEAFON',\n",
    "    'S2_dR', 'S2_dG', 'S2_dB', \n",
    "    'S2_dNIR', 'S2_dSWIR1', 'S2_dSWIR2', \n",
    "    'S2_dRE1', 'S2_dRE2', 'S2_dRE3', 'S2_dRE4',\n",
    "    'S2_dNDVI', 'S2_dSAVI', 'S2_dBRIGHTNESS', 'S2_dGREENNESS', 'S2_dWETNESS',\n",
    "]\n",
    "# PALSAR\n",
    "P_COLS = ['P_HH', 'P_HV']\n",
    "\n",
    "\n",
    "AGENCY_COLS = ['agency_BLM', 'agency_USFS', 'agency_WADNR']\n",
    "\n",
    "# ECOREGIONS = [col for col in df.columns if col.startswith('eco_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [x.replace('FVS_', '') for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "BA_COLS = ['TRUE_FIR', 'OTHER_HARDWOOD', 'MAPLE', 'OAK', 'DOUGLAS_FIR', \n",
    "           'SPRUCE', 'CEDAR', 'PONDEROSA_PINE', 'OTHER_SOFTWOOD', \n",
    "           'LODGEPOLE_PINE', 'HEMLOCK', 'RED_ALDER', 'TANOAK', 'JUNIPER',\n",
    "           'LARCH']\n",
    "\n",
    "pres_cols = [x + '_pres' for x in BA_COLS]\n",
    "spp_presence = (df[BA_COLS].values.reshape(-1,1) > 0).reshape(df[BA_COLS].shape)*1\n",
    "spp_presence = pd.DataFrame(data=spp_presence, columns=pres_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following classification of abundance for each species:\n",
    "\n",
    "| label | description | basal area |  \n",
    "| :--: | :--: | :--: |\n",
    "| 0 | absent | 0% |  \n",
    "| 1 | present | (0-33%] |  \n",
    "| 2 | abundant | (33-66%] |\n",
    "| 3 | dominant | (66-100%] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_prop = df[BA_COLS].divide(df[BA_COLS].sum(axis=1) + 1e-9, axis=0)\n",
    "prop_cols = [x + '_prop' for x in BA_COLS]\n",
    "ba_prop.columns = prop_cols\n",
    "\n",
    "SW_SPP = ['TRUE_FIR', 'DOUGLAS_FIR', 'SPRUCE', 'CEDAR', \n",
    "          'PONDEROSA_PINE', 'OTHER_SOFTWOOD', 'LODGEPOLE_PINE', \n",
    "          'HEMLOCK', 'JUNIPER', 'LARCH']\n",
    "SW_PROP_COLS = [spp+'_prop' for spp in SW_SPP]\n",
    "HW_SPP = ['OTHER_HARDWOOD','MAPLE', 'OAK', 'RED_ALDER', 'TANOAK']\n",
    "HW_PROP_COLS = [spp+'_prop' for spp in HW_SPP]\n",
    "\n",
    "ba_prop['ALL_HARDWOOD_prop'] = ba_prop[HW_PROP_COLS].sum(axis=1)\n",
    "ba_prop['ALL_SOFTWOOD_prop'] = ba_prop[SW_PROP_COLS].sum(axis=1)\n",
    "# prop_cols.extend(['ALL_HARDWOOD_prop', 'ALL_SOFTWOOD_prop'])\n",
    "\n",
    "abund_cols = [x + '_abund' for x in BA_COLS]\n",
    "# abund_cols.extend(['ALL_HARDWOOD_abund', 'ALL_SOFTWOOD_abund'])\n",
    "\n",
    "spp_abund = pd.DataFrame(data=np.digitize(ba_prop[prop_cols].values, [0,1/3,2/3], right=True), \n",
    "                         index=ba_prop.index,\n",
    "                         columns=abund_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp_abund['HARDWOOD_SOFTWOOD_abund'] = np.nan\n",
    "\n",
    "# hardwood dominance, no softwood presence\n",
    "spp_abund.loc[(ba_prop['ALL_HARDWOOD_prop'] > 0.66) & \n",
    "              (ba_prop['ALL_SOFTWOOD_prop'] < 0.05),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = -2\n",
    "\n",
    "# hardwood dominance, softwood presence\n",
    "spp_abund.loc[(ba_prop['ALL_HARDWOOD_prop'] > 0.66) & \n",
    "              (ba_prop['ALL_SOFTWOOD_prop'] >= 0.05),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = -1\n",
    "\n",
    "# no basal area recorded, nonstocked\n",
    "spp_abund.loc[np.isclose(ba_prop['ALL_HARDWOOD_prop'],0) & \n",
    "              np.isclose(ba_prop['ALL_SOFTWOOD_prop'],0),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = -9998\n",
    "\n",
    "# softwood dominance, hardwood presence\n",
    "spp_abund.loc[(ba_prop['ALL_SOFTWOOD_prop'] > 0.66) & \n",
    "              (ba_prop['ALL_HARDWOOD_prop'] >= 0.05),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = 1\n",
    "\n",
    "# softwood dominance, no hardwood presence\n",
    "spp_abund.loc[(ba_prop['ALL_SOFTWOOD_prop'] > 0.66) & \n",
    "              (ba_prop['ALL_HARDWOOD_prop'] < 0.05),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = 2\n",
    "\n",
    "# softwood and hardwood equal at 40-50%\n",
    "spp_abund.loc[(ba_prop['ALL_SOFTWOOD_prop'] > 0.33)&\n",
    "              (ba_prop['ALL_HARDWOOD_prop'] > 0.33)& \n",
    "              (ba_prop['ALL_SOFTWOOD_prop'] > 0),\n",
    "              'HARDWOOD_SOFTWOOD_abund'] = 0\n",
    "\n",
    "spp_abund['HARDWOOD_SOFTWOOD_abund'] = spp_abund['HARDWOOD_SOFTWOOD_abund'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRUE_FIR_prop</th>\n",
       "      <th>OTHER_HARDWOOD_prop</th>\n",
       "      <th>MAPLE_prop</th>\n",
       "      <th>OAK_prop</th>\n",
       "      <th>DOUGLAS_FIR_prop</th>\n",
       "      <th>SPRUCE_prop</th>\n",
       "      <th>CEDAR_prop</th>\n",
       "      <th>PONDEROSA_PINE_prop</th>\n",
       "      <th>OTHER_SOFTWOOD_prop</th>\n",
       "      <th>LODGEPOLE_PINE_prop</th>\n",
       "      <th>...</th>\n",
       "      <th>CEDAR_abund</th>\n",
       "      <th>PONDEROSA_PINE_abund</th>\n",
       "      <th>OTHER_SOFTWOOD_abund</th>\n",
       "      <th>LODGEPOLE_PINE_abund</th>\n",
       "      <th>HEMLOCK_abund</th>\n",
       "      <th>RED_ALDER_abund</th>\n",
       "      <th>TANOAK_abund</th>\n",
       "      <th>JUNIPER_abund</th>\n",
       "      <th>LARCH_abund</th>\n",
       "      <th>HARDWOOD_SOFTWOOD_abund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRUE_FIR_prop  OTHER_HARDWOOD_prop  MAPLE_prop  OAK_prop  DOUGLAS_FIR_prop  \\\n",
       "0            0.0                  0.0         0.0       0.0          0.171717   \n",
       "1            0.0                  0.0         0.0       0.0          0.000000   \n",
       "2            0.0                  0.0         0.0       0.0          1.000000   \n",
       "3            0.0                  0.0         0.0       0.0          0.000000   \n",
       "5            0.0                  0.0         0.0       0.0          0.000000   \n",
       "\n",
       "   SPRUCE_prop  CEDAR_prop  PONDEROSA_PINE_prop  OTHER_SOFTWOOD_prop  \\\n",
       "0          0.0         0.0             0.828283                  0.0   \n",
       "1          0.0         0.0             1.000000                  0.0   \n",
       "2          0.0         0.0             0.000000                  0.0   \n",
       "3          0.0         0.0             0.000000                  0.0   \n",
       "5          0.0         0.0             0.000000                  0.0   \n",
       "\n",
       "   LODGEPOLE_PINE_prop  ...  CEDAR_abund  PONDEROSA_PINE_abund  \\\n",
       "0                  0.0  ...            0                     3   \n",
       "1                  0.0  ...            0                     3   \n",
       "2                  0.0  ...            0                     0   \n",
       "3                  0.0  ...            0                     0   \n",
       "5                  0.0  ...            0                     0   \n",
       "\n",
       "   OTHER_SOFTWOOD_abund  LODGEPOLE_PINE_abund  HEMLOCK_abund  RED_ALDER_abund  \\\n",
       "0                     0                     0              0                0   \n",
       "1                     0                     0              0                0   \n",
       "2                     0                     0              0                0   \n",
       "3                     0                     0              0                0   \n",
       "5                     0                     0              0                0   \n",
       "\n",
       "   TANOAK_abund  JUNIPER_abund  LARCH_abund  HARDWOOD_SOFTWOOD_abund  \n",
       "0             0              0            0                        2  \n",
       "1             0              0            0                        2  \n",
       "2             0              0            0                        2  \n",
       "3             0              0            0                    -9998  \n",
       "5             3              0            0                       -2  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp_df = pd.concat([ba_prop, spp_abund], axis=1)\n",
    "spp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>year</th>\n",
       "      <th>TPA</th>\n",
       "      <th>BA</th>\n",
       "      <th>SDI</th>\n",
       "      <th>CCF</th>\n",
       "      <th>QMD</th>\n",
       "      <th>TCUFT</th>\n",
       "      <th>TOPHT</th>\n",
       "      <th>NUMBER_OF_STRATA</th>\n",
       "      <th>...</th>\n",
       "      <th>CEDAR_abund</th>\n",
       "      <th>PONDEROSA_PINE_abund</th>\n",
       "      <th>OTHER_SOFTWOOD_abund</th>\n",
       "      <th>LODGEPOLE_PINE_abund</th>\n",
       "      <th>HEMLOCK_abund</th>\n",
       "      <th>RED_ALDER_abund</th>\n",
       "      <th>TANOAK_abund</th>\n",
       "      <th>JUNIPER_abund</th>\n",
       "      <th>LARCH_abund</th>\n",
       "      <th>HARDWOOD_SOFTWOOD_abund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174f5abb</td>\n",
       "      <td>2018</td>\n",
       "      <td>199</td>\n",
       "      <td>100</td>\n",
       "      <td>186</td>\n",
       "      <td>100</td>\n",
       "      <td>9.581292</td>\n",
       "      <td>2113</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3ca58372</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>18.232113</td>\n",
       "      <td>846</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7bd6ffce</td>\n",
       "      <td>2017</td>\n",
       "      <td>445</td>\n",
       "      <td>62</td>\n",
       "      <td>149</td>\n",
       "      <td>125</td>\n",
       "      <td>5.053601</td>\n",
       "      <td>801</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd3731de</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1ee9b438</td>\n",
       "      <td>2016</td>\n",
       "      <td>308</td>\n",
       "      <td>219</td>\n",
       "      <td>352</td>\n",
       "      <td>305</td>\n",
       "      <td>11.410089</td>\n",
       "      <td>8212</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uuid  year  TPA   BA  SDI  CCF        QMD  TCUFT  TOPHT  \\\n",
       "0  174f5abb  2018  199  100  186  100   9.581292   2113     58   \n",
       "1  3ca58372  2016   12   22   31   16  18.232113    846     55   \n",
       "2  7bd6ffce  2017  445   62  149  125   5.053601    801     39   \n",
       "3  cd3731de  2020    0    0    0    0   0.000000      0      0   \n",
       "5  1ee9b438  2016  308  219  352  305  11.410089   8212     86   \n",
       "\n",
       "   NUMBER_OF_STRATA  ...  CEDAR_abund PONDEROSA_PINE_abund  \\\n",
       "0                 1  ...            0                    3   \n",
       "1                 1  ...            0                    3   \n",
       "2                 2  ...            0                    0   \n",
       "3                 0  ...            0                    0   \n",
       "5                 2  ...            0                    0   \n",
       "\n",
       "   OTHER_SOFTWOOD_abund  LODGEPOLE_PINE_abund  HEMLOCK_abund  RED_ALDER_abund  \\\n",
       "0                     0                     0              0                0   \n",
       "1                     0                     0              0                0   \n",
       "2                     0                     0              0                0   \n",
       "3                     0                     0              0                0   \n",
       "5                     0                     0              0                0   \n",
       "\n",
       "   TANOAK_abund  JUNIPER_abund  LARCH_abund  HARDWOOD_SOFTWOOD_abund  \n",
       "0             0              0            0                        2  \n",
       "1             0              0            0                        2  \n",
       "2             0              0            0                        2  \n",
       "3             0              0            0                    -9998  \n",
       "5             3              0            0                       -2  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge these presence and abundance classes into the dataframe\n",
    "df = pd.concat([df, spp_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRUE_FIR_abund',\n",
       " 'OTHER_HARDWOOD_abund',\n",
       " 'MAPLE_abund',\n",
       " 'OAK_abund',\n",
       " 'DOUGLAS_FIR_abund',\n",
       " 'SPRUCE_abund',\n",
       " 'CEDAR_abund',\n",
       " 'PONDEROSA_PINE_abund',\n",
       " 'OTHER_SOFTWOOD_abund',\n",
       " 'LODGEPOLE_PINE_abund',\n",
       " 'HEMLOCK_abund',\n",
       " 'RED_ALDER_abund',\n",
       " 'TANOAK_abund',\n",
       " 'JUNIPER_abund',\n",
       " 'LARCH_abund']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abund_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark plots with canopy cover < 10%, QMD < 1\", and TOPHT < 10' as non-stocked, let's not predict on them\n",
    "df.loc[(df.TOTAL_COVER < 10), 'HARDWOOD_SOFTWOOD_abund'] = -9998\n",
    "df.loc[(df.QMD < 1), 'HARDWOOD_SOFTWOOD_abund'] = -9998\n",
    "df.loc[(df.TOPHT < 10), 'HARDWOOD_SOFTWOOD_abund'] = -9998\n",
    "df = df.loc[df.HARDWOOD_SOFTWOOD_abund > -9998]\n",
    "df['HARDWOOD_SOFTWOOD_abund'] =  df['HARDWOOD_SOFTWOOD_abund'] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots with < 10% canopy cover, <1\" QMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = ['TOTAL_COVER', 'TOPHT', 'QMD', 'TCUFT'] + S2_COLS + LT_COLS + ['elevation', 'lat', 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6500 entries, 0 to 6499\n",
      "Columns: 219 entries, uuid to HARDWOOD_SOFTWOOD_abund\n",
      "dtypes: float64(114), int64(99), object(6)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=X_COLS+abund_cols).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing sets\n",
    "Since some plot_ids have multiple observations, we want all observations for each plot_id to fall entirely in train_set or entirely the training or testing sets because these samples are definitely not independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing sets for each species group\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "\n",
    "# make sure each plot (uuid) only goes into either train or testing set\n",
    "uuids = pd.unique(df['uuid'].values)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "train0_test1 = np.random.choice((0,1), uuids.shape, p=(0.8,0.2))\n",
    "train_test_assign = pd.DataFrame(np.dstack((uuids, train0_test1))[0], columns=['uuid', 'train0_test1'])\n",
    "df_assigned = df.merge(train_test_assign, on='uuid')\n",
    "    \n",
    "train = df_assigned.loc[df_assigned['train0_test1'] == 0]\n",
    "test = df_assigned.loc[df_assigned['train0_test1'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize features\n",
    "Scale all predictive features to have 0 mean and unit variance (subtracting mean, dividing by standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models/composition_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train[X_COLS].copy(), test[X_COLS].copy()\n",
    "Y_train, Y_test = train[spp_df.columns], test[spp_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at Data\n",
    "Be aware that the data are heavily skewed towards particularly widespread species like Douglas-fir, western hemlock, and red alder. There are far fewer examples, particularly of plots with other species where those species are abundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_47968_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >absent</th>\n",
       "      <th class=\"col_heading level0 col1\" >present</th>\n",
       "      <th class=\"col_heading level0 col2\" >abundant</th>\n",
       "      <th class=\"col_heading level0 col3\" >dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row0\" class=\"row_heading level0 row0\" >DOUGLAS_FIR</th>\n",
       "      <td id=\"T_47968_row0_col0\" class=\"data row0 col0\" >1,591</td>\n",
       "      <td id=\"T_47968_row0_col1\" class=\"data row0 col1\" >1,030</td>\n",
       "      <td id=\"T_47968_row0_col2\" class=\"data row0 col2\" >1,194</td>\n",
       "      <td id=\"T_47968_row0_col3\" class=\"data row0 col3\" >2,685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row1\" class=\"row_heading level0 row1\" >HEMLOCK</th>\n",
       "      <td id=\"T_47968_row1_col0\" class=\"data row1 col0\" >3,620</td>\n",
       "      <td id=\"T_47968_row1_col1\" class=\"data row1 col1\" >1,714</td>\n",
       "      <td id=\"T_47968_row1_col2\" class=\"data row1 col2\" >681</td>\n",
       "      <td id=\"T_47968_row1_col3\" class=\"data row1 col3\" >485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row2\" class=\"row_heading level0 row2\" >TRUE_FIR</th>\n",
       "      <td id=\"T_47968_row2_col0\" class=\"data row2 col0\" >4,951</td>\n",
       "      <td id=\"T_47968_row2_col1\" class=\"data row2 col1\" >960</td>\n",
       "      <td id=\"T_47968_row2_col2\" class=\"data row2 col2\" >333</td>\n",
       "      <td id=\"T_47968_row2_col3\" class=\"data row2 col3\" >256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row3\" class=\"row_heading level0 row3\" >RED_ALDER</th>\n",
       "      <td id=\"T_47968_row3_col0\" class=\"data row3 col0\" >4,959</td>\n",
       "      <td id=\"T_47968_row3_col1\" class=\"data row3 col1\" >1,085</td>\n",
       "      <td id=\"T_47968_row3_col2\" class=\"data row3 col2\" >235</td>\n",
       "      <td id=\"T_47968_row3_col3\" class=\"data row3 col3\" >221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row4\" class=\"row_heading level0 row4\" >CEDAR</th>\n",
       "      <td id=\"T_47968_row4_col0\" class=\"data row4 col0\" >5,013</td>\n",
       "      <td id=\"T_47968_row4_col1\" class=\"data row4 col1\" >1,176</td>\n",
       "      <td id=\"T_47968_row4_col2\" class=\"data row4 col2\" >239</td>\n",
       "      <td id=\"T_47968_row4_col3\" class=\"data row4 col3\" >72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row5\" class=\"row_heading level0 row5\" >OTHER_HARDWOOD</th>\n",
       "      <td id=\"T_47968_row5_col0\" class=\"data row5 col0\" >5,078</td>\n",
       "      <td id=\"T_47968_row5_col1\" class=\"data row5 col1\" >1,187</td>\n",
       "      <td id=\"T_47968_row5_col2\" class=\"data row5 col2\" >139</td>\n",
       "      <td id=\"T_47968_row5_col3\" class=\"data row5 col3\" >96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row6\" class=\"row_heading level0 row6\" >PONDEROSA_PINE</th>\n",
       "      <td id=\"T_47968_row6_col0\" class=\"data row6 col0\" >5,693</td>\n",
       "      <td id=\"T_47968_row6_col1\" class=\"data row6 col1\" >306</td>\n",
       "      <td id=\"T_47968_row6_col2\" class=\"data row6 col2\" >192</td>\n",
       "      <td id=\"T_47968_row6_col3\" class=\"data row6 col3\" >309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row7\" class=\"row_heading level0 row7\" >MAPLE</th>\n",
       "      <td id=\"T_47968_row7_col0\" class=\"data row7 col0\" >5,904</td>\n",
       "      <td id=\"T_47968_row7_col1\" class=\"data row7 col1\" >554</td>\n",
       "      <td id=\"T_47968_row7_col2\" class=\"data row7 col2\" >31</td>\n",
       "      <td id=\"T_47968_row7_col3\" class=\"data row7 col3\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row8\" class=\"row_heading level0 row8\" >SPRUCE</th>\n",
       "      <td id=\"T_47968_row8_col0\" class=\"data row8 col0\" >6,079</td>\n",
       "      <td id=\"T_47968_row8_col1\" class=\"data row8 col1\" >297</td>\n",
       "      <td id=\"T_47968_row8_col2\" class=\"data row8 col2\" >79</td>\n",
       "      <td id=\"T_47968_row8_col3\" class=\"data row8 col3\" >45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row9\" class=\"row_heading level0 row9\" >OTHER_SOFTWOOD</th>\n",
       "      <td id=\"T_47968_row9_col0\" class=\"data row9 col0\" >6,163</td>\n",
       "      <td id=\"T_47968_row9_col1\" class=\"data row9 col1\" >320</td>\n",
       "      <td id=\"T_47968_row9_col2\" class=\"data row9 col2\" >7</td>\n",
       "      <td id=\"T_47968_row9_col3\" class=\"data row9 col3\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row10\" class=\"row_heading level0 row10\" >LODGEPOLE_PINE</th>\n",
       "      <td id=\"T_47968_row10_col0\" class=\"data row10 col0\" >6,198</td>\n",
       "      <td id=\"T_47968_row10_col1\" class=\"data row10 col1\" >200</td>\n",
       "      <td id=\"T_47968_row10_col2\" class=\"data row10 col2\" >55</td>\n",
       "      <td id=\"T_47968_row10_col3\" class=\"data row10 col3\" >47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row11\" class=\"row_heading level0 row11\" >LARCH</th>\n",
       "      <td id=\"T_47968_row11_col0\" class=\"data row11 col0\" >6,210</td>\n",
       "      <td id=\"T_47968_row11_col1\" class=\"data row11 col1\" >213</td>\n",
       "      <td id=\"T_47968_row11_col2\" class=\"data row11 col2\" >53</td>\n",
       "      <td id=\"T_47968_row11_col3\" class=\"data row11 col3\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row12\" class=\"row_heading level0 row12\" >OAK</th>\n",
       "      <td id=\"T_47968_row12_col0\" class=\"data row12 col0\" >6,311</td>\n",
       "      <td id=\"T_47968_row12_col1\" class=\"data row12 col1\" >169</td>\n",
       "      <td id=\"T_47968_row12_col2\" class=\"data row12 col2\" >15</td>\n",
       "      <td id=\"T_47968_row12_col3\" class=\"data row12 col3\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row13\" class=\"row_heading level0 row13\" >TANOAK</th>\n",
       "      <td id=\"T_47968_row13_col0\" class=\"data row13 col0\" >6,357</td>\n",
       "      <td id=\"T_47968_row13_col1\" class=\"data row13 col1\" >109</td>\n",
       "      <td id=\"T_47968_row13_col2\" class=\"data row13 col2\" >23</td>\n",
       "      <td id=\"T_47968_row13_col3\" class=\"data row13 col3\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47968_level0_row14\" class=\"row_heading level0 row14\" >JUNIPER</th>\n",
       "      <td id=\"T_47968_row14_col0\" class=\"data row14 col0\" >6,417</td>\n",
       "      <td id=\"T_47968_row14_col1\" class=\"data row14 col1\" >45</td>\n",
       "      <td id=\"T_47968_row14_col2\" class=\"data row14 col2\" >10</td>\n",
       "      <td id=\"T_47968_row14_col3\" class=\"data row14 col3\" >28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f862c2918e0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df[abund_cols].apply(np.bincount, axis=0, minlength=4)\n",
    "counts.columns = [col.split('_abund')[0] for col in counts.columns]\n",
    "counts = counts.rename({0: 'absent', 1:'present', 2:'abundant', 3:'dominant'}, axis=0)\n",
    "counts.T.sort_values(by='absent')\\\n",
    ".style.format('{:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e96ab_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >absent</th>\n",
       "      <th class=\"col_heading level0 col1\" >present</th>\n",
       "      <th class=\"col_heading level0 col2\" >abundant</th>\n",
       "      <th class=\"col_heading level0 col3\" >dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row0\" class=\"row_heading level0 row0\" >DOUGLAS_FIR</th>\n",
       "      <td id=\"T_e96ab_row0_col0\" class=\"data row0 col0\" >339</td>\n",
       "      <td id=\"T_e96ab_row0_col1\" class=\"data row0 col1\" >185</td>\n",
       "      <td id=\"T_e96ab_row0_col2\" class=\"data row0 col2\" >261</td>\n",
       "      <td id=\"T_e96ab_row0_col3\" class=\"data row0 col3\" >548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row1\" class=\"row_heading level0 row1\" >HEMLOCK</th>\n",
       "      <td id=\"T_e96ab_row1_col0\" class=\"data row1 col0\" >729</td>\n",
       "      <td id=\"T_e96ab_row1_col1\" class=\"data row1 col1\" >360</td>\n",
       "      <td id=\"T_e96ab_row1_col2\" class=\"data row1 col2\" >138</td>\n",
       "      <td id=\"T_e96ab_row1_col3\" class=\"data row1 col3\" >106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row2\" class=\"row_heading level0 row2\" >RED_ALDER</th>\n",
       "      <td id=\"T_e96ab_row2_col0\" class=\"data row2 col0\" >991</td>\n",
       "      <td id=\"T_e96ab_row2_col1\" class=\"data row2 col1\" >245</td>\n",
       "      <td id=\"T_e96ab_row2_col2\" class=\"data row2 col2\" >58</td>\n",
       "      <td id=\"T_e96ab_row2_col3\" class=\"data row2 col3\" >39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row3\" class=\"row_heading level0 row3\" >TRUE_FIR</th>\n",
       "      <td id=\"T_e96ab_row3_col0\" class=\"data row3 col0\" >1,030</td>\n",
       "      <td id=\"T_e96ab_row3_col1\" class=\"data row3 col1\" >180</td>\n",
       "      <td id=\"T_e96ab_row3_col2\" class=\"data row3 col2\" >66</td>\n",
       "      <td id=\"T_e96ab_row3_col3\" class=\"data row3 col3\" >57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row4\" class=\"row_heading level0 row4\" >CEDAR</th>\n",
       "      <td id=\"T_e96ab_row4_col0\" class=\"data row4 col0\" >1,031</td>\n",
       "      <td id=\"T_e96ab_row4_col1\" class=\"data row4 col1\" >226</td>\n",
       "      <td id=\"T_e96ab_row4_col2\" class=\"data row4 col2\" >62</td>\n",
       "      <td id=\"T_e96ab_row4_col3\" class=\"data row4 col3\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row5\" class=\"row_heading level0 row5\" >OTHER_HARDWOOD</th>\n",
       "      <td id=\"T_e96ab_row5_col0\" class=\"data row5 col0\" >1,061</td>\n",
       "      <td id=\"T_e96ab_row5_col1\" class=\"data row5 col1\" >225</td>\n",
       "      <td id=\"T_e96ab_row5_col2\" class=\"data row5 col2\" >25</td>\n",
       "      <td id=\"T_e96ab_row5_col3\" class=\"data row5 col3\" >22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row6\" class=\"row_heading level0 row6\" >PONDEROSA_PINE</th>\n",
       "      <td id=\"T_e96ab_row6_col0\" class=\"data row6 col0\" >1,161</td>\n",
       "      <td id=\"T_e96ab_row6_col1\" class=\"data row6 col1\" >59</td>\n",
       "      <td id=\"T_e96ab_row6_col2\" class=\"data row6 col2\" >44</td>\n",
       "      <td id=\"T_e96ab_row6_col3\" class=\"data row6 col3\" >69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row7\" class=\"row_heading level0 row7\" >MAPLE</th>\n",
       "      <td id=\"T_e96ab_row7_col0\" class=\"data row7 col0\" >1,201</td>\n",
       "      <td id=\"T_e96ab_row7_col1\" class=\"data row7 col1\" >124</td>\n",
       "      <td id=\"T_e96ab_row7_col2\" class=\"data row7 col2\" >6</td>\n",
       "      <td id=\"T_e96ab_row7_col3\" class=\"data row7 col3\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row8\" class=\"row_heading level0 row8\" >SPRUCE</th>\n",
       "      <td id=\"T_e96ab_row8_col0\" class=\"data row8 col0\" >1,269</td>\n",
       "      <td id=\"T_e96ab_row8_col1\" class=\"data row8 col1\" >50</td>\n",
       "      <td id=\"T_e96ab_row8_col2\" class=\"data row8 col2\" >8</td>\n",
       "      <td id=\"T_e96ab_row8_col3\" class=\"data row8 col3\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row9\" class=\"row_heading level0 row9\" >OTHER_SOFTWOOD</th>\n",
       "      <td id=\"T_e96ab_row9_col0\" class=\"data row9 col0\" >1,272</td>\n",
       "      <td id=\"T_e96ab_row9_col1\" class=\"data row9 col1\" >58</td>\n",
       "      <td id=\"T_e96ab_row9_col2\" class=\"data row9 col2\" >3</td>\n",
       "      <td id=\"T_e96ab_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row10\" class=\"row_heading level0 row10\" >LARCH</th>\n",
       "      <td id=\"T_e96ab_row10_col0\" class=\"data row10 col0\" >1,272</td>\n",
       "      <td id=\"T_e96ab_row10_col1\" class=\"data row10 col1\" >46</td>\n",
       "      <td id=\"T_e96ab_row10_col2\" class=\"data row10 col2\" >11</td>\n",
       "      <td id=\"T_e96ab_row10_col3\" class=\"data row10 col3\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row11\" class=\"row_heading level0 row11\" >LODGEPOLE_PINE</th>\n",
       "      <td id=\"T_e96ab_row11_col0\" class=\"data row11 col0\" >1,280</td>\n",
       "      <td id=\"T_e96ab_row11_col1\" class=\"data row11 col1\" >40</td>\n",
       "      <td id=\"T_e96ab_row11_col2\" class=\"data row11 col2\" >4</td>\n",
       "      <td id=\"T_e96ab_row11_col3\" class=\"data row11 col3\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row12\" class=\"row_heading level0 row12\" >OAK</th>\n",
       "      <td id=\"T_e96ab_row12_col0\" class=\"data row12 col0\" >1,295</td>\n",
       "      <td id=\"T_e96ab_row12_col1\" class=\"data row12 col1\" >34</td>\n",
       "      <td id=\"T_e96ab_row12_col2\" class=\"data row12 col2\" >2</td>\n",
       "      <td id=\"T_e96ab_row12_col3\" class=\"data row12 col3\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row13\" class=\"row_heading level0 row13\" >TANOAK</th>\n",
       "      <td id=\"T_e96ab_row13_col0\" class=\"data row13 col0\" >1,304</td>\n",
       "      <td id=\"T_e96ab_row13_col1\" class=\"data row13 col1\" >25</td>\n",
       "      <td id=\"T_e96ab_row13_col2\" class=\"data row13 col2\" >2</td>\n",
       "      <td id=\"T_e96ab_row13_col3\" class=\"data row13 col3\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e96ab_level0_row14\" class=\"row_heading level0 row14\" >JUNIPER</th>\n",
       "      <td id=\"T_e96ab_row14_col0\" class=\"data row14 col0\" >1,311</td>\n",
       "      <td id=\"T_e96ab_row14_col1\" class=\"data row14 col1\" >8</td>\n",
       "      <td id=\"T_e96ab_row14_col2\" class=\"data row14 col2\" >6</td>\n",
       "      <td id=\"T_e96ab_row14_col3\" class=\"data row14 col3\" >8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f862c53dd60>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = test[abund_cols].apply(np.bincount, axis=0, minlength=4)\n",
    "counts.columns = [col.split('_abund')[0] for col in counts.columns]\n",
    "counts = counts.rename({0: 'absent', 1:'present', 2:'abundant', 3:'dominant'}, axis=0)\n",
    "counts.T.sort_values(by='absent')\\\n",
    ".style.format('{:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HARDWOOD_SOFTWOOD_abund\n",
       "0     211\n",
       "1     187\n",
       "2     446\n",
       "3    1333\n",
       "4    4323\n",
       "Name: uuid, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['HARDWOOD_SOFTWOOD_abund'])['uuid'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f862c315820>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df.HARDWOOD_SOFTWOOD_abund, df.ALL_HARDWOOD_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "from skopt import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed directly from Jake VanderPlas\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Bayesian generative classification based on KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bandwidth : float\n",
    "        the kernel bandwidth within each class\n",
    "    kernel : str\n",
    "        the kernel name, passed to KernelDensity\n",
    "    \"\"\"\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        training_sets = [X[y == yi] for yi in self.classes_]\n",
    "        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                      kernel=self.kernel).fit(Xi)\n",
    "                        for Xi in training_sets]\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "                           for Xi in training_sets]\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        logprobs = np.array([model.score_samples(X)\n",
    "                             for model in self.models_]).T\n",
    "        result = np.exp(logprobs + self.logpriors_)\n",
    "        return result / result.sum(1, keepdims=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SEARCH_SPACES = {\n",
    "    'rf': {\n",
    "        'n_estimators': Integer(50, 5000, prior='log-uniform'),\n",
    "        'max_depth': Integer(2, 32, prior='log-uniform'),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        'min_samples_split': Integer(2, 20, prior='uniform'),\n",
    "        'min_samples_leaf': Integer(1, 20, prior='uniform'),\n",
    "        'class_weight': Categorical(['balanced', None])\n",
    "    },\n",
    "    'rf_ord': {\n",
    "        'n_estimators': Integer(50, 1000, prior='log-uniform'),\n",
    "        'max_depth': Integer(2, 32, prior='log-uniform'),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        'min_samples_split': Integer(2, 20, prior='uniform'),\n",
    "        'min_samples_leaf': Integer(1, 20, prior='uniform'),\n",
    "        'class_weight': Categorical(['balanced', None])\n",
    "    },\n",
    "    'log': {\n",
    "        'penalty': Categorical(['l1', 'l2', 'elasticnet', 'none']),\n",
    "        'class_weight': Categorical(['balanced', None]),\n",
    "        'multi_class': Categorical(['ovr', 'multinomial']),\n",
    "        'C': Real(1e-4, 1e4, prior='log-uniform'),\n",
    "        'l1_ratio': Real(0, 1, prior='uniform'),\n",
    "    },\n",
    "    'log_ord': {\n",
    "        'penalty': Categorical(['l1', 'l2', 'elasticnet', 'none']),\n",
    "        'class_weight': Categorical(['balanced', None]),\n",
    "        'C': Real(1e-4, 1e4, prior='log-uniform'),\n",
    "        'l1_ratio': Real(0, 1, prior='uniform'),\n",
    "    },\n",
    "    'svc':{\n",
    "        'C': Real(1e-3, 1e2, prior='log-uniform'),\n",
    "        'gamma': Real(1e-3, 1, prior='log-uniform'),\n",
    "        'class_weight': Categorical(['balanced', None])\n",
    "    },\n",
    "    'svc_ord':{\n",
    "        'C': Real(1e-3, 1e2, prior='log-uniform'),\n",
    "        'gamma': Real(1e-3, 1, prior='log-uniform'),\n",
    "        'class_weight': Categorical(['balanced', None])\n",
    "    },\n",
    "    'kde':{\n",
    "        'bandwidth': Real(1e-3, 1e3, prior='log-uniform'),\n",
    "        'kernel': Categorical(['gaussian', 'tophat', 'epanechnikov', \n",
    "                               'exponential', 'linear', 'cosine']),\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': Integer(1, 25, prior='uniform'),\n",
    "        'weights': Categorical(['uniform', 'distance']), \n",
    "        'p': Integer(1, 100, prior='log-uniform')\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_TYPES = {\n",
    "    'rf': RandomForestClassifier(random_state=RANDOM_STATE,\n",
    "                                 n_jobs=-1),\n",
    "    'rf_ord': RandomForestOrdinalClassifier(random_state=RANDOM_STATE,\n",
    "                             n_jobs=-1),\n",
    "    'log': LogisticRegression(random_state=RANDOM_STATE,\n",
    "                              solver='saga',\n",
    "                              max_iter=1e6,\n",
    "                              n_jobs=-1),\n",
    "    'log_ord': LogisticRegressionOrdinalClassifier(random_state=RANDOM_STATE,\n",
    "                          solver='saga',\n",
    "                          max_iter=1e6,                  \n",
    "                          n_jobs=-1),\n",
    "    'svc': SVC(random_state=RANDOM_STATE,\n",
    "               kernel='rbf'),\n",
    "    'svc_ord': SVC(random_state=RANDOM_STATE,\n",
    "                   kernel='rbf'),\n",
    "    'kde': KDEClassifier(),\n",
    "    'knn': KNeighborsClassifier(n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupkfold = GroupKFold(n_splits=3)\n",
    "uuid_groups = df.reindex(X_train.index)['uuid']\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import balanced_accuracy_score, average_precision_score\n",
    "\n",
    "# scorer = make_scorer(geometric_mean_score) \n",
    "# scorer = make_scorer(cohen_kappa_score, weights='linear')\n",
    "\n",
    "def cohen_kappa_adjust(y_true, y_pred, labels=None, weights=None, sample_weight=None):\n",
    "    if y_pred.dtype == 'float' and y_pred.max() <= 1.0: # regression model trained on proportions\n",
    "        y_true = np.digitize(y_true, [0,1/3,2/3], right=True)\n",
    "        y_pred = np.digitize(y_pred, [0,1/3,2/3], right=True)\n",
    "    \n",
    "    elif y_pred.dtype == 'float' and y_pred.max() > 1.0: # regression model trained on abundance integers\n",
    "        y_true = y_true.round().astype(int)\n",
    "        y_pred = y_pred.round().astype(int)\n",
    "        \n",
    "    return cohen_kappa_score(y_true, y_pred, labels=labels, weights=weights, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "\n",
    "def tune_classifier(sp_name, model_type, samples_per_param=100, n_jobs=1, n_points=1):\n",
    "    if model_type[-4:] == '_reg':\n",
    "        y_train = Y_train[sp_name+'_prop']\n",
    "    else:\n",
    "        y_train = Y_train[sp_name+'_abund']\n",
    "    \n",
    "    model = MODEL_TYPES[model_type]\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('model', model)\n",
    "    ])\n",
    "    search_space = MODEL_SEARCH_SPACES[model_type]\n",
    "    search_space = {f'model__{key}': search_space[key] for key in search_space}\n",
    "    \n",
    "    if sp_name == 'HARDWOOD_SOFTWOOD':\n",
    "        LABELS = [0, 1, 2, 3, 4]\n",
    "    else:\n",
    "        LABELS = [0, 1, 2, 3]\n",
    "    \n",
    "    scorer = make_scorer(cohen_kappa_adjust, labels=LABELS, weights='quadratic')\n",
    "    \n",
    "    opt = BayesSearchCV(\n",
    "        pipe,\n",
    "        search_space,\n",
    "        n_iter=samples_per_param,\n",
    "        random_state=RANDOM_STATE,\n",
    "        cv=groupkfold,\n",
    "        scoring=scorer,\n",
    "        verbose=0,\n",
    "        n_jobs=n_jobs,\n",
    "        n_points=n_points,\n",
    "    )\n",
    "    \n",
    "    def on_step(result):\n",
    "        num_rounds = len(result.func_vals)\n",
    "        if num_rounds % 10 == 1:\n",
    "            print('\\n@ sample iteration {}'.format(num_rounds), flush=True)\n",
    "\n",
    "        score = result.fun\n",
    "        best_score = result.func_vals[-1]\n",
    "\n",
    "        if score == best_score:\n",
    "            print('','{:0.3f}'.format(score), end=' ', flush=True)\n",
    "        else:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "    \n",
    "    SAVE_DIR = '../models/composition_models'\n",
    "    check_pkl = '{}_{}_tuned_checkpoints.pkl'.format(sp_name, model_type)\n",
    "    check_path = os.path.join(SAVE_DIR, check_pkl)\n",
    "    checkpoint_saver = CheckpointSaver(check_path)\n",
    "\n",
    "    print('Running {:,d} rounds of cross-validation.'.format(opt.total_iterations))\n",
    "    print('Sampling {} parameters {} times each.'.format(len(search_space), samples_per_param))\n",
    "    print('Displaying best cross-validation scores... ')\n",
    "    \n",
    "    with warnings.catch_warnings():  # don't issue warnings when we test the same parameter values\n",
    "        warnings.filterwarnings(\"ignore\", \n",
    "                                category=UserWarning) \n",
    "        \n",
    "        # executes bayesian optimization\n",
    "        _ = opt.fit(X_train, y_train, \n",
    "                    groups=uuid_groups, \n",
    "                    callback=[on_step, checkpoint_saver])\n",
    "    \n",
    "    opt_pkl = '{}_{}_tuned.pkl'.format(sp_name, model_type)\n",
    "    opt_path = os.path.join(SAVE_DIR, opt_pkl)\n",
    "    dump(opt, opt_path)\n",
    "    print('Done. \\nModel saved at {}\\n'.format(opt_path))\n",
    "    \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(sp_name, model, \n",
    "                                target_names=['absent', 'present', 'abundant', 'dominant']):\n",
    "    print(classification_report_imbalanced(\n",
    "        Y_test[sp_name+'_abund'],\n",
    "        model.predict(X_test),\n",
    "        target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help interpret classification reports:\n",
    "* `tp` is the number of true positives\n",
    "* `fp` is the number of false positives, \n",
    "* `tn` is the number of true negatives \n",
    "* `fn` is the number of false negatives\n",
    "\n",
    "**precision** is the ratio `tp / (tp + fp)`, or **the ability of the classifier not to label as positive a sample that is negative.** \n",
    "\n",
    "**recall** is the ratio `tp / (tp + fn)`, or **the ability of the classifier to find all the positive samples.**\n",
    "\n",
    "**specificity** is the ratio `tn / (tn + fp)`, or **the ability of the classifier to find all the negative samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion(sp_name, model, \n",
    "                      labels=[0, 1, 2, 3],\n",
    "                      display_labels=['Absent', 'Present', 'Abundant', 'Dominant']):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "    plot_confusion_matrix(model, \n",
    "                          X_test, \n",
    "                          Y_test[sp_name+'_abund'],\n",
    "                          labels=labels,\n",
    "                          normalize='true',\n",
    "                          display_labels=display_labels,\n",
    "                          include_values=True,\n",
    "                          values_format='.1%',\n",
    "                          cmap='Greens', ax=axs[0])\n",
    "    \n",
    "    plot_confusion_matrix(model, \n",
    "                          X_test, \n",
    "                          Y_test[sp_name+'_abund'],\n",
    "                          labels=labels,\n",
    "                          display_labels=display_labels,\n",
    "                          include_values=True,\n",
    "                          values_format=',d',\n",
    "                          cmap='Greens', ax=axs[1])\n",
    "    axs[0].grid(False)\n",
    "    axs[1].grid(False)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rf_feature_importances(model):\n",
    "    best_model = model.best_estimator_\n",
    "    importances = best_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in best_model.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)#[::-1]\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(3,10))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.barh(range(X_train.shape[1]), importances[indices],\n",
    "            color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.yticks(range(X_train.shape[1]), X_train.columns[indices])\n",
    "    plt.ylim([-1, X_train.shape[1]])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp_models = {spp:{} for spp in BA_COLS + ['HARDWOOD_SOFTWOOD']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 150 rounds of cross-validation.\n",
      "Sampling 6 parameters 25 times each.\n",
      "Displaying best cross-validation scores... \n",
      "\n",
      "@ sample iteration 1\n",
      " -0.624  -0.671 ........\n",
      "@ sample iteration 11\n",
      " -0.679 .... -0.701 .. -0.720 "
     ]
    }
   ],
   "source": [
    "SP_NAME, MOD_TYPE = 'HARDWOOD_SOFTWOOD', 'rf' \n",
    "\n",
    "TARGET_NAMES = ['HW>>SW', 'HW>SW', 'HW~SW', 'HW<SW', 'HW<<SW']\n",
    "LABELS = [0, 1, 2, 3, 4]\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE], \n",
    "                           target_names=TARGET_NAMES)\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE],\n",
    "                  labels=LABELS, display_labels=TARGET_NAMES)\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])\n",
    "\n",
    "test_score = cohen_kappa_score(Y_test[SP_NAME+'_abund'], \n",
    "                               spp_models[SP_NAME][MOD_TYPE].predict(X_test), \n",
    "                               labels=[0, 1, 2, 3, 4], \n",
    "                               weights='quadratic')\n",
    "print('Score on Test Set: {:.3f}'.format(test_score))\n",
    "\n",
    "plot_convergence(spp_models[SP_NAME][MOD_TYPE].optimizer_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'HARDWOOD_SOFTWOOD', 'rf_ord' \n",
    "\n",
    "TARGET_NAMES = ['HW>>SW', 'HW>SW', 'HW~SW', 'HW<SW', 'HW<<SW']\n",
    "LABELS = [0, 1, 2, 3, 4]\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE], \n",
    "                           target_names=TARGET_NAMES)\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE],\n",
    "                  labels=LABELS, display_labels=TARGET_NAMES)\n",
    "\n",
    "test_score = cohen_kappa_score(Y_test[SP_NAME+'_abund'], \n",
    "                               spp_models[SP_NAME][MOD_TYPE].predict(X_test), \n",
    "                               labels=[0, 1, 2, 3, 4], \n",
    "                               weights='quadratic')\n",
    "print('Score on Test Set: {:.3f}'.format(test_score))\n",
    "\n",
    "plot_convergence(spp_models[SP_NAME][MOD_TYPE].optimizer_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'HARDWOOD_SOFTWOOD', 'log' \n",
    "\n",
    "TARGET_NAMES = ['HW>>SW', 'HW>SW', 'HW~SW', 'HW<SW', 'HW<<SW']\n",
    "LABELS = [0, 1, 2, 3, 4]\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE], \n",
    "                           target_names=TARGET_NAMES)\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE],\n",
    "                  labels=LABELS, display_labels=TARGET_NAMES)\n",
    "\n",
    "test_score = cohen_kappa_score(Y_test[SP_NAME+'_abund'], \n",
    "                               spp_models[SP_NAME][MOD_TYPE].predict(X_test), \n",
    "                               labels=[0, 1, 2, 3, 4], \n",
    "                               weights='quadratic')\n",
    "print('Score on Test Set: {:.3f}'.format(test_score))\n",
    "\n",
    "plot_convergence(spp_models[SP_NAME][MOD_TYPE].optimizer_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'HARDWOOD_SOFTWOOD', 'log_ord' \n",
    "\n",
    "TARGET_NAMES = ['HW>>SW', 'HW>SW', 'HW~SW', 'HW<SW', 'HW<<SW']\n",
    "LABELS = [0, 1, 2, 3, 4]\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE], \n",
    "                           target_names=TARGET_NAMES)\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE],\n",
    "                  labels=LABELS, display_labels=TARGET_NAMES)\n",
    "\n",
    "test_score = cohen_kappa_score(Y_test[SP_NAME+'_abund'], \n",
    "                               spp_models[SP_NAME][MOD_TYPE].predict(X_test), \n",
    "                               labels=[0, 1, 2, 3, 4], \n",
    "                               weights='quadratic')\n",
    "print('Score on Test Set: {:.3f}'.format(test_score))\n",
    "\n",
    "plot_convergence(spp_models[SP_NAME][MOD_TYPE].optimizer_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'DOUGLAS_FIR', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'HEMLOCK', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'RED_ALDER', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'CEDAR', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'MAPLE', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'TRUE_FIR', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'PONDEROSA_PINE', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'LODGEPOLE_PINE', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'OAK', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'JUNIPER', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'SPRUCE', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'TANOAK', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'LARCH', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'OTHER_HARDWOOD', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_NAME, MOD_TYPE = 'OTHER_SOFTWOOD', 'rf' \n",
    "\n",
    "spp_models[SP_NAME][MOD_TYPE] = tune_classifier(SP_NAME, MOD_TYPE, samples_per_param=25)\n",
    "print_classification_report(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "display_confusion(SP_NAME, spp_models[SP_NAME][MOD_TYPE])\n",
    "plot_rf_feature_importances(spp_models[SP_NAME][MOD_TYPE])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest_mapping]",
   "language": "python",
   "name": "conda-env-forest_mapping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
