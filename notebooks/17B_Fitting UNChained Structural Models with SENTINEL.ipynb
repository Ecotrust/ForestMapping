{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# data prep and model-tuning\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# types of models we'll fit\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "# from sklearn.multioutput import RegressorChain\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_DATA = '../data/processed/plot_features.csv'\n",
    "KEEP_PLOT_COLS = ['uuid', 'lat', 'lon', 'ecoregion3', 'agency', 'distance_to_water_m', 'plot_size_ac', 'meas_yr']\n",
    "plot_data = pd.read_csv(PLOT_DATA)[KEEP_PLOT_COLS]\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIDAR_DATA = '../data/processed/lidar_features.csv'\n",
    "lidar_data = pd.read_csv(LIDAR_DATA)[['uuid', 'elevation']]\n",
    "lidar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = plot_data.merge(lidar_data, left_on=['uuid'], right_on=['uuid'], how='inner').drop_duplicates(subset=['uuid'])\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVENTORY = '../data/processed/inventory_features.csv'\n",
    "inv_data = pd.read_csv(INVENTORY, index_col=['uuid', 'year'])\n",
    "inv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SATELLITE = '../data/processed/satellite_features.csv'\n",
    "sat = pd.read_csv(SATELLITE, index_col=['uuid', 'year'])\n",
    "S2_COLS = [col for col in sat.columns if col.startswith('S2')]\n",
    "LANDTRENDR_COLS = [col for col in sat.columns if col.startswith('LT')]\n",
    "sat = sat[S2_COLS + LANDTRENDR_COLS].dropna()\n",
    "sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out some of the training data\n",
    "We can exclude some of the training data based on how far separated the inventory data (interpolated using FVS simulations) is from the year the imagery was collected. Similarly, we can screen out training examples that had relatively low density of lidar returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_and_inv = sat.merge(inv_data, how='inner', left_index=True, right_index=True).reset_index()\n",
    "sat_and_inv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sat_and_inv.merge(plot_data, how='inner', left_on=['uuid'], right_on=['uuid']).dropna()\n",
    "print('{:,d} samples'.format(len(df)))\n",
    "print('Columns:', df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLIERS = '../data/interim/outlier_uuids.csv'\n",
    "outliers = pd.read_csv(OUTLIERS)\n",
    "# filter out the height outliers\n",
    "df = df[~df.uuid.isin(outliers.outlier_uuid)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df.topht > 0)&(df.total_cover >= 10)&(df.qmd > 0)]\n",
    "df.loc[df.qmd > 50, 'qmd'] = 50\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect how many samples we have for different years, regions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=['year'])[['uuid']].count().rename({'uuid':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, \n",
    "               values='uuid', \n",
    "               aggfunc='count', \n",
    "               index=['meas_yr'], \n",
    "               columns=['year'], \n",
    "               fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoreg_counts = df.groupby(by=['ecoregion3'])[['uuid', 'year', 'plot_size_ac']].nunique()\n",
    "ecoreg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available features\n",
    "The different types of predictor variables we can use to predict a forest attribute, including climate, lidar-derived, soil, and satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[S2_COLS + LANDTRENDR_COLS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and targets\n",
    "This is the first step in determining what features we want to use, and what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = S2_COLS + LANDTRENDR_COLS + ['elevation', 'lat', 'lon'] + ['ecoregion3'] \n",
    "Y_COLS = ['total_cover', 'topht', 'qmd', 'tcuft']\n",
    "\n",
    "Y_NAMES = [col.upper() for col in Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_REGIONS = ['blue_mountains', 'coast_range', 'north_cascades', 'cascades',\n",
    "               'klamath_mountains_california_high_north_coast_range', \n",
    "               'eastern_cascades_slopes_and_foothills', 'northern_rockies',\n",
    "               'puget_lowland', 'willamette_valley']\n",
    "display(df.groupby('ecoregion3')[Y_COLS].mean().round(1).loc[USE_REGIONS])\n",
    "display(df[Y_COLS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "X, Y = df[X_COLS], df[Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[X_COLS].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets by ecoregion\n",
    "We want to explore model transferability between regions, so we'll train models independently on subsets of the data within a single ecoregion, as well as a model that is trained on all available ecoregions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregions = list(np.sort([reg for reg in pd.unique(df.ecoregion3) if ecoreg_counts.loc[reg]['uuid'] > 20]))\n",
    "\n",
    "eco_X_idx = [X.loc[X.ecoregion3 == eco].index.values for eco in ecoregions]\n",
    "\n",
    "eco_X_dfs = [X.loc[X.ecoregion3 == eco].drop(['ecoregion3'], axis=1) for eco in ecoregions]\n",
    "eco_Y_dfs = [Y.loc[idx] for idx in eco_X_idx]\n",
    "\n",
    "# append a \"global\" model that contains data from all ecoregions\n",
    "ecoregions.append('all')\n",
    "ecoregion_names = ['_'.join(x.split('_')[0:2]) for x in ecoregions]\n",
    "eco_X_dfs.append(X.drop(['ecoregion3'], axis=1))\n",
    "eco_Y_dfs.append(Y)\n",
    "\n",
    "ecoregion_display_names = [' '.join(x.upper().split('_')[:2]) for x in ecoregions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_class_bins = [10,40,70,100]\n",
    "cover_class_labels = ['OPEN', 'MODERATE', 'CLOSED']\n",
    "height_class_bins = np.arange(0,300,20)\n",
    "height_class_labels = [f'{x}-{x+20}' for x in height_class_bins[:-1]]\n",
    "diameter_class_bins = [1, 5, 10, 15, 20, 999]\n",
    "diameter_class_labels = ['SEED/SAP', 'SMALL', 'MEDIUM', 'LARGE', 'VERY_LARGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "We'll use Root Mean Square Error to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, pred):\n",
    "    return np.sqrt((np.square(obs-pred)).mean())\n",
    "\n",
    "def nrmse(obs, pred):\n",
    "    return rmse(pred,obs) / obs.mean()\n",
    "\n",
    "def mae(obs, pred):   \n",
    "    return abs(pred - obs).mean()\n",
    "\n",
    "def mape(obs, pred):    \n",
    "    return abs(pred - obs).mean() / obs.mean()\n",
    "\n",
    "def bias(obs, pred):   \n",
    "    return (pred - obs).mean()\n",
    "\n",
    "def rel_bias(obs, pred):\n",
    "    return bias(pred,obs) / obs.mean()\n",
    "\n",
    "def bin_accuracy(obs, pred, bins, fuzzy_tol=0):\n",
    "    pred_binned = np.digitize(pred, bins)\n",
    "    obs_binned = np.digitisze(obs, bins)\n",
    "    diff = abs(pred_binned - obs_binned)\n",
    "    \n",
    "    return (diff <= fuzzy_tol).sum() / len(diff)\n",
    "\n",
    "def confidence_interval_half(X, confidence=0.95):\n",
    "    n = len(X)\n",
    "    se = stats.sem(X)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit some models\n",
    "For each type of model, we'll employ cross-validation to tune model hyperparameters, generating a tuned model for each ecoregion as well as a tuned model using all training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Lasso': Lasso(), \n",
    "    'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_jobs=-1), \n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(), \n",
    "}\n",
    "\n",
    "FIT_PARAMS = {\n",
    "    'ElasticNet': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "        'l1_ratio': np.arange(0.0, 1.0, 0.1),\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [1,2,3,4,5,10,20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'max_depth': [5, 20, None],\n",
    "        'max_samples': [0.5, None]\n",
    "    },\n",
    "    'HistGradientBoostingRegressor': {\n",
    "        'max_iter': [50, 100, 200],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTER_FOLDS = 5\n",
    "NUM_INNER_FOLDS = 3\n",
    "SCORE_FUNCS = [rmse, nrmse, mae, mape, bias, rel_bias]\n",
    "score_names = [func.__name__ for func in SCORE_FUNCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_insider_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            for y_col in target_vars:\n",
    "                results[region][model_name][y_col] = {}\n",
    "                results[region][model_name][y_col]['fitted_model'] = None\n",
    "                results[region][model_name][y_col]['best_params'] = None\n",
    "                results[region][model_name][y_col]['cv_results'] = {}\n",
    "                for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "                    fold_num = fold_idx + 1\n",
    "                    results[region][model_name][y_col]['cv_results'][fold_num] = {}\n",
    "                    results[region][model_name][y_col]['cv_results'][fold_num]['best_params'] = None \n",
    "                    results[region][model_name][y_col]['cv_results'][fold_num]['predict_time'] = None\n",
    "                    for score_func in score_funcs:\n",
    "                        score_func_name = score_func.__name__\n",
    "                        results[region][model_name][y_col]['cv_results'][fold_num][score_func_name] = None\n",
    "    return results\n",
    "\n",
    "def parse_insider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for target in Y_COLS:\n",
    "            for fold_num in results[ecoregion][target].keys():\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold_num, ecoregion, target, score_name, results[ecoregion][target][fold_num][score_name]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])\n",
    "\n",
    "def build_global_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for model_name in model_names:\n",
    "        results[model_name] = {}\n",
    "        for y_col in target_vars:\n",
    "            results[model_name][y_col] = {}\n",
    "            results[model_name][y_col]['fitted_model'] = None\n",
    "            results[model_name][y_col]['best_params'] = None\n",
    "            results[model_name][y_col]['cv_results'] = {}\n",
    "            for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "                fold_num = fold_idx + 1\n",
    "                results[model_name][y_col]['cv_results'][fold_num] = {}\n",
    "                results[model_name][y_col]['cv_results'][fold_num]['best_params'] = None \n",
    "                results[model_name][y_col]['cv_results'][fold_num]['predict_time'] = None\n",
    "                for region in regions:\n",
    "                    results[model_name][y_col]['cv_results'][fold_num][region] = {}\n",
    "                    for score_func in score_funcs:\n",
    "                        score_func_name = score_func.__name__\n",
    "                        results[model_name][y_col]['cv_results'][fold_num][region][score_func_name] = None\n",
    "    return results\n",
    "\n",
    "def build_outsider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            for y_col in target_vars:\n",
    "                results[region][model_name][y_col] = {}\n",
    "                results[region][model_name][y_col]['fitted_model'] = None\n",
    "                results[region][model_name][y_col]['best_params'] = None\n",
    "                results[region][model_name][y_col]['predict_time'] = None\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[region][model_name][y_col][score_func_name] = None\n",
    "    return results\n",
    "\n",
    "def build_visiting_insider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for target_region in regions:\n",
    "        results[target_region] = {}\n",
    "        for train_region in [r for r in regions if r != target_region]:\n",
    "            results[target_region][train_region] = {}\n",
    "            for model_name in model_names:\n",
    "                results[target_region][train_region][model_name] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[target_region][train_region][model_name][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_insider_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', clone(model)),\n",
    "    ])\n",
    "    search_params = {f'model__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}')\n",
    "        for y_col in Y_COLS:\n",
    "            print(f'    {y_col}', end='... ')\n",
    "            X = eco_X_dfs[i]\n",
    "            Y = eco_Y_dfs[i][y_col]\n",
    "            outer_groups = df.loc[X.index, 'uuid'].values\n",
    "        \n",
    "            outer_fold_num = 1\n",
    "            for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "                X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "                Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "                inner_groups = df.loc[X_train.index, 'uuid'].values\n",
    "\n",
    "                inner_search = GridSearchCV(pipe, search_params, \n",
    "                                            scoring='neg_mean_squared_error', \n",
    "                                            n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "                inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "                insider_results[ecoregion][model_name][y_col]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "\n",
    "                inner_best_model = inner_result.best_estimator_\n",
    "                start_time = time.time()\n",
    "                Y_pred = inner_best_model.predict(X_test)\n",
    "                end_time = time.time()\n",
    "                total_predict_time = end_time - start_time\n",
    "                avg_predict_time = total_predict_time / len(X_test)\n",
    "                insider_results[ecoregion][model_name][y_col]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "\n",
    "                for score_func in SCORE_FUNCS:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    score = score_func(Y_test, Y_pred)\n",
    "                    insider_results[ecoregion][model_name][y_col]['cv_results'][outer_fold_num][score_func_name] = score\n",
    "                    \n",
    "                print(outer_fold_num, end='... ')\n",
    "                outer_fold_num += 1\n",
    "            print('Done scoring.', end='... ')\n",
    "            \n",
    "            # done with scoring of models, now time to tune a model using the whole dataset\n",
    "            outer_search = GridSearchCV(pipe, search_params, \n",
    "                                        scoring='neg_mean_squared_error', \n",
    "                                        n_jobs=-1, cv=cv_outer, refit=True)\n",
    "            outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "            \n",
    "            # now fit on the entire dataset, not just training set\n",
    "            model = outer_result.best_estimator_\n",
    "            model.set_params(**outer_result.best_params_)\n",
    "            X = df.loc[df.ecoregion3 == ecoregion, X_COLS].drop(['ecoregion3'], axis=1)\n",
    "            y = df.loc[df.ecoregion3 == ecoregion, y_col]\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            eco_name = '_'.join(ecoregion.split('_')[:2])\n",
    "            outfile = f'{eco_name}-sentinel-{model_name}-{y_col}.pkl'\n",
    "            outpath = os.path.join('../models/structure_models', outfile)\n",
    "            with open(outpath, 'wb') as file:\n",
    "                pickle.dump(model, file)\n",
    "            \n",
    "            insider_results[ecoregion][model_name][y_col]['fitted_model'] = model\n",
    "            insider_results[ecoregion][model_name][y_col]['best_params'] = outer_result.best_params_\n",
    "            print('All done.')\n",
    "    \n",
    "        cv_results_dict = {ecoregion: {y_col: insider_results[ecoregion][model_name][y_col]['cv_results'] for y_col in Y_COLS} \n",
    "                           for ecoregion in train_regions}\n",
    "    \n",
    "    return cv_results_dict\n",
    "\n",
    "def tune_outsider_model(model_name, num_folds=5):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', clone(model)),\n",
    "    ])\n",
    "    search_params = {f'model__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    groupkfold = GroupKFold(num_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}')\n",
    "        for y_col in Y_COLS:\n",
    "            print(f'    {y_col}', end='... ')\n",
    "            X_train = df.loc[df.ecoregion3 != ecoregion, X_COLS].drop('ecoregion3', axis=1)\n",
    "            Y_train = Y.loc[X_train.index][y_col]\n",
    "            X_test = df.loc[df.ecoregion3 == ecoregion, X_COLS].drop('ecoregion3', axis=1)\n",
    "            Y_test = Y.loc[X_test.index][y_col]\n",
    "            groups = df.loc[X_train.index]['ecoregion3'].values\n",
    "\n",
    "            search = GridSearchCV(pipe, search_params, \n",
    "                                  scoring='neg_mean_squared_error',\n",
    "                                  n_jobs=-1, cv=groupkfold, refit=True)\n",
    "\n",
    "            result = search.fit(X_train, Y_train, groups=groups)\n",
    "            print('Done fitting, now scoring', end='... ')\n",
    "            outsider_results[ecoregion][model_name][y_col]['best_params'] = result.best_params_\n",
    "            outsider_results[ecoregion][model_name][y_col]['fitted_model'] = result.best_estimator_\n",
    "\n",
    "            best_model = result.best_estimator_       \n",
    "            start_time = time.time()\n",
    "            Y_pred = best_model.predict(X_test)\n",
    "            end_time = time.time()\n",
    "            total_predict_time = end_time - start_time\n",
    "            avg_predict_time = total_predict_time / len(X_test)\n",
    "            outsider_results[ecoregion][model_name][y_col]['predict_time'] = avg_predict_time\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                score = score_func(Y_test, Y_pred)\n",
    "                outsider_results[ecoregion][model_name][y_col][score_func_name] = score\n",
    "            print('All done.')\n",
    "        \n",
    "        results_dict = {ecoregion: {y_col: outsider_results[ecoregion][model_name][y_col] for y_col in Y_COLS} for ecoregion in train_regions}\n",
    "        \n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def tune_global_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "#     print(f'Scoring with {NUM_OUTER_FOLDS} folds... ', end='')\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    test_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', clone(model)),\n",
    "    ])\n",
    "    search_params = {f'model__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    X = df[X_COLS].drop('ecoregion3', axis=1)\n",
    "    Y = df[Y_COLS]\n",
    "    outer_groups = df['uuid'].values\n",
    "\n",
    "    for y_col in Y_COLS:\n",
    "        outer_fold_num = 1\n",
    "        print(f'{y_col}', end='... ')\n",
    "        for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "            X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "            Y_train, Y_test = Y.loc[train_ix][y_col], Y.loc[test_ix][y_col]\n",
    "            inner_groups = df.loc[train_ix, 'uuid'].values\n",
    "\n",
    "            inner_search = GridSearchCV(pipe, search_params, \n",
    "                                        scoring='neg_mean_squared_error', \n",
    "                                        n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "            inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "            global_results[model_name][y_col]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "\n",
    "            inner_best_model = inner_result.best_estimator_\n",
    "            start_time = time.time()\n",
    "            Y_pred = inner_best_model.predict(X_test)\n",
    "            end_time = time.time()\n",
    "            total_predict_time = end_time - start_time\n",
    "            avg_predict_time = total_predict_time / len(X_test)\n",
    "            global_results[model_name][y_col]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "\n",
    "            for ecoregion in test_regions:\n",
    "                region_mask = (df.loc[test_ix, 'ecoregion3'] == ecoregion).values\n",
    "                regional_X_test = X_test.loc[test_ix[region_mask]]\n",
    "                regional_Y_test = Y_test.loc[test_ix[region_mask]]\n",
    "                regional_Y_pred = inner_best_model.predict(regional_X_test)\n",
    "\n",
    "                for score_func in SCORE_FUNCS:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    score = score_func(regional_Y_test, regional_Y_pred)\n",
    "                    global_results[model_name][y_col]['cv_results'][outer_fold_num][ecoregion][score_func_name] = score\n",
    "\n",
    "            print(outer_fold_num, end='... ')\n",
    "            outer_fold_num += 1\n",
    "\n",
    "        print('Done scoring. Now fitting a final model', end='... ')\n",
    "\n",
    "        # done with scoring of models, now time to tune a model using the whole dataset\n",
    "        outer_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_outer, refit=True)\n",
    "        outer_result = outer_search.fit(X, Y[y_col], groups=outer_groups)\n",
    "        \n",
    "        # now fit on the entire dataset, not just training set\n",
    "        model = outer_result.best_estimator_\n",
    "        model.set_params(**outer_result.best_params_)\n",
    "        X = df[X_COLS].drop(['ecoregion3'], axis=1)\n",
    "        y = df[y_col]\n",
    "        model.fit(X, y)\n",
    "\n",
    "        outfile = f'global-sentinel-{model_name}-{y_col}.pkl'\n",
    "        outpath = os.path.join('../models/structure_models', outfile)\n",
    "        with open(outpath, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        print('All done.')\n",
    "        \n",
    "        global_results[model_name][y_col]['fitted_model'] = model\n",
    "        global_results[model_name][y_col]['best_params'] = outer_result.best_params_\n",
    "\n",
    "    results_dict = global_results[model_name]\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_global_results(results):\n",
    "    data = []\n",
    "    for fold in range(NUM_OUTER_FOLDS):\n",
    "        for ecoregion in ecoregions[:-1]:\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold+1, ecoregion, target, score_name, results[target]['cv_results'][fold+1][ecoregion][score_name]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Global Models\n",
    "These models get to see data from every ecoregion during training and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = build_global_results_dictionary(ecoregions[:-1], MODELS.keys(), NUM_OUTER_FOLDS, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_global = tune_global_model('ElasticNet')\n",
    "lasso_global = tune_global_model('Lasso')\n",
    "knn_global = tune_global_model('KNeighborsRegressor')\n",
    "rf_global = tune_global_model('RandomForestRegressor')\n",
    "gbm_global = tune_global_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_global, lasso_global, knn_global, rf_global, gbm_global]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_global_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_global_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_global_results['ecoregion'] = all_global_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_global_results.columns = [col.upper() for col in all_global_results.columns]\n",
    "all_global_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_global_results.to_csv('../data/processed/nestedcv_unchained_global_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Outsider Models\n",
    "These models have data from the ecoregion they're tested on held out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsider_results = build_outsider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_outsider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for target in Y_COLS:\n",
    "            for score_name in score_names:\n",
    "                data.append((np.nan, ecoregion, target, score_name, results[ecoregion][target][score_name]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_outsider = tune_outsider_model('ElasticNet')\n",
    "lasso_outsider = tune_outsider_model('Lasso')\n",
    "knn_outsider = tune_outsider_model('KNeighborsRegressor')\n",
    "# rf_outsider = tune_outsider_model('RandomForestRegressor')\n",
    "# gbm_outsider = tune_outsider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_outsider = tune_outsider_model('RandomForestRegressor')\n",
    "gbm_outsider = tune_outsider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_outsider, lasso_outsider, knn_outsider, rf_outsider, gbm_outsider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_outsider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_outsider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_outsider_results['ecoregion'] = all_outsider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_outsider_results.columns = [col.upper() for col in all_outsider_results.columns]\n",
    "all_outsider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outsider_results.to_csv('../data/processed/nestedcv_unchained_outsider_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Insider Models\n",
    "These models are trained with observations from a single ecoregion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_results = build_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), 5, SCORE_FUNCS, Y_COLS)\n",
    "\n",
    "elastic_insider = tune_insider_model('ElasticNet')\n",
    "lasso_insider = tune_insider_model('Lasso')\n",
    "knn_insider = tune_insider_model('KNeighborsRegressor')\n",
    "rf_insider = tune_insider_model('RandomForestRegressor')\n",
    "gbm_insider = tune_insider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_insider, lasso_insider, knn_insider, rf_insider, gbm_insider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_insider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_insider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_insider_results['ecoregion'] = all_insider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_insider_results.columns = [col.upper() for col in all_insider_results.columns]\n",
    "all_insider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insider_results.to_csv('../data/processed/nestedcv_unchained_insider_results_satellite_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trained Insider Models to Score Visiting Insider Models\n",
    "These models are trained on a single region, and scored on other regions they've never seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_results = build_visiting_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_results = []\n",
    "for target_region in ecoregions[:-1]:\n",
    "    for train_region in [r for r in ecoregions[:-1] if r != target_region]:\n",
    "        for model_name in MODELS.keys():\n",
    "            for y_col in Y_COLS:\n",
    "                model = insider_results[train_region][model_name][y_col]['fitted_model']\n",
    "                targ_idx = df.loc[df.ecoregion3 == target_region].index.values\n",
    "                targ_X = df.loc[targ_idx, X_COLS].drop(['ecoregion3'], axis=1)\n",
    "                pred = model.predict(targ_X)\n",
    "                obs = df.loc[targ_idx, y_col]\n",
    "                for score_func in SCORE_FUNCS:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    score = score_func(obs, pred)\n",
    "                    visitor_results.append(\n",
    "                        (' '.join(target_region.title().replace('_',' ').split()),\n",
    "                         ' '.join(train_region.title().replace('_',' ').split()),\n",
    "                         model_name, score_func_name, y_col, score))\n",
    "visitor_df = pd.DataFrame(visitor_results, \n",
    "                          columns = ['TARGET_ECOREGION', 'TRAIN_ECOREGION', \n",
    "                                     'MODEL', 'METRIC', 'TARGET', 'SCORE'])\n",
    "visitor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_df.to_csv('../data/processed/nestedcv_unchained_visitor_results_satellite_structure.csv', \n",
    "                  header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest_mapping]",
   "language": "python",
   "name": "conda-env-forest_mapping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
