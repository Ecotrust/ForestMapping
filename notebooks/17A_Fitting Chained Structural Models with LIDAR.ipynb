{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# data prep and model-tuning\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# types of models we'll fit\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.multioutput import RegressorChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5089 entries, 0 to 5088\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   uuid                 5089 non-null   object \n",
      " 1   lat                  5089 non-null   float64\n",
      " 2   lon                  5089 non-null   float64\n",
      " 3   ecoregion3           5089 non-null   object \n",
      " 4   agency               5089 non-null   object \n",
      " 5   distance_to_water_m  5089 non-null   float64\n",
      " 6   plot_size_ac         5089 non-null   float64\n",
      " 7   meas_yr              5089 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 318.2+ KB\n"
     ]
    }
   ],
   "source": [
    "PLOT_DATA = '../data/processed/plot_features.csv'\n",
    "KEEP_PLOT_COLS = ['uuid', 'lat', 'lon', 'ecoregion3', 'agency', 'distance_to_water_m', 'plot_size_ac', 'meas_yr']\n",
    "plot_data = pd.read_csv(PLOT_DATA)[KEEP_PLOT_COLS]\n",
    "plot_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "LIDAR_DATA = '../data/processed/lidar_features.csv'\n",
    "lidar_data = pd.read_csv(LIDAR_DATA)\n",
    "USE_LIDAR_COLS = ['strat0_return-proportion', 'strat1_return-proportion', \n",
    "                  'strat2_return-proportion', 'strat3_return-proportion', \n",
    "                  'strat4_return-proportion', 'strat5_return-proportion', \n",
    "                  'height_05-percentile',  'height_25-percentile', \n",
    "                  'height_50-percentile', 'height_75-percentile',\n",
    "                  'height_95_percentile', 'cover', \n",
    "                  'potential_volume', 'stddev_height', \n",
    "                  'surface_volume', 'kurtosis', 'skewness',\n",
    "                  'aspect', 'elevation', 'slope'\n",
    "                 ]\n",
    "lidar_data[USE_LIDAR_COLS] = lidar_data[USE_LIDAR_COLS].replace('-1.#IND00', np.nan).astype(float)\n",
    "lidar_data = lidar_data[['uuid', 'lidar_year', 'lidar_acq'] + USE_LIDAR_COLS]\n",
    "float_cols = [col for col in lidar_data.columns if lidar_data[col].dtype == 'float']\n",
    "\n",
    "for col in lidar_data[float_cols].columns[np.any(lidar_data[float_cols].isna(),axis=0).values]:\n",
    "    lidar_data[col] = imputer.fit_transform(lidar_data[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5411 entries, 00027724 to fff7e1c3\n",
      "Data columns (total 22 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   lidar_year                5411 non-null   int64  \n",
      " 1   lidar_acq                 5411 non-null   object \n",
      " 2   strat0_return-proportion  5411 non-null   float64\n",
      " 3   strat1_return-proportion  5411 non-null   float64\n",
      " 4   strat2_return-proportion  5411 non-null   float64\n",
      " 5   strat3_return-proportion  5411 non-null   float64\n",
      " 6   strat4_return-proportion  5411 non-null   float64\n",
      " 7   strat5_return-proportion  5411 non-null   float64\n",
      " 8   height_05-percentile      5411 non-null   float64\n",
      " 9   height_25-percentile      5411 non-null   float64\n",
      " 10  height_50-percentile      5411 non-null   float64\n",
      " 11  height_75-percentile      5411 non-null   float64\n",
      " 12  height_95_percentile      5411 non-null   float64\n",
      " 13  cover                     5411 non-null   float64\n",
      " 14  potential_volume          5411 non-null   float64\n",
      " 15  stddev_height             5411 non-null   float64\n",
      " 16  surface_volume            5411 non-null   float64\n",
      " 17  kurtosis                  5411 non-null   float64\n",
      " 18  skewness                  5411 non-null   float64\n",
      " 19  aspect                    5411 non-null   float64\n",
      " 20  elevation                 5411 non-null   float64\n",
      " 21  slope                     5411 non-null   float64\n",
      "dtypes: float64(20), int64(1), object(1)\n",
      "memory usage: 972.3+ KB\n"
     ]
    }
   ],
   "source": [
    "lidar_data = lidar_data.set_index('uuid')\n",
    "lidar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49410 entries, ba510248 to c4f7f099\n",
      "Data columns (total 76 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   year                 49410 non-null  int64  \n",
      " 1   tpa                  49410 non-null  int64  \n",
      " 2   ba                   49410 non-null  int64  \n",
      " 3   sdi                  49410 non-null  int64  \n",
      " 4   ccf                  49410 non-null  int64  \n",
      " 5   qmd                  49410 non-null  float64\n",
      " 6   tcuft                49410 non-null  int64  \n",
      " 7   topht                49410 non-null  int64  \n",
      " 8   number_of_strata     49410 non-null  int64  \n",
      " 9   total_cover          49410 non-null  int64  \n",
      " 10  structure_class      49410 non-null  object \n",
      " 11  canopy_baseheight    49410 non-null  int64  \n",
      " 12  canopy_bulkdensity   49410 non-null  float64\n",
      " 13  aboveground_biomass  49410 non-null  int64  \n",
      " 14  aboveground_carbon   49410 non-null  int64  \n",
      " 15  gs_tpa               49410 non-null  int64  \n",
      " 16  AF                   49410 non-null  int64  \n",
      " 17  AS                   49410 non-null  int64  \n",
      " 18  BM                   49410 non-null  int64  \n",
      " 19  BO                   49410 non-null  int64  \n",
      " 20  CH                   49410 non-null  int64  \n",
      " 21  CW                   49410 non-null  int64  \n",
      " 22  DF                   49410 non-null  int64  \n",
      " 23  DG                   49410 non-null  int64  \n",
      " 24  ES                   49410 non-null  int64  \n",
      " 25  GC                   49410 non-null  int64  \n",
      " 26  GF                   49410 non-null  int64  \n",
      " 27  IC                   49410 non-null  int64  \n",
      " 28  JP                   49410 non-null  int64  \n",
      " 29  KP                   49410 non-null  int64  \n",
      " 30  LO                   49410 non-null  int64  \n",
      " 31  LP                   49410 non-null  int64  \n",
      " 32  MA                   49410 non-null  int64  \n",
      " 33  MC                   49410 non-null  int64  \n",
      " 34  MH                   49410 non-null  int64  \n",
      " 35  NF                   49410 non-null  int64  \n",
      " 36  OH                   49410 non-null  int64  \n",
      " 37  OS                   49410 non-null  int64  \n",
      " 38  OT                   49410 non-null  int64  \n",
      " 39  PC                   49410 non-null  int64  \n",
      " 40  PL                   49410 non-null  int64  \n",
      " 41  PP                   49410 non-null  int64  \n",
      " 42  PY                   49410 non-null  int64  \n",
      " 43  RA                   49410 non-null  int64  \n",
      " 44  RC                   49410 non-null  int64  \n",
      " 45  RF                   49410 non-null  int64  \n",
      " 46  SF                   49410 non-null  int64  \n",
      " 47  SH                   49410 non-null  int64  \n",
      " 48  SP                   49410 non-null  int64  \n",
      " 49  SS                   49410 non-null  int64  \n",
      " 50  TO                   49410 non-null  int64  \n",
      " 51  WA                   49410 non-null  int64  \n",
      " 52  WB                   49410 non-null  int64  \n",
      " 53  WF                   49410 non-null  int64  \n",
      " 54  WH                   49410 non-null  int64  \n",
      " 55  WI                   49410 non-null  int64  \n",
      " 56  WJ                   49410 non-null  int64  \n",
      " 57  WL                   49410 non-null  int64  \n",
      " 58  WO                   49410 non-null  int64  \n",
      " 59  WP                   49410 non-null  int64  \n",
      " 60  YC                   49410 non-null  int64  \n",
      " 61  TRUE_FIR             49410 non-null  int64  \n",
      " 62  OTHER_HARDWOOD       49410 non-null  int64  \n",
      " 63  MAPLE                49410 non-null  int64  \n",
      " 64  OAK                  49410 non-null  int64  \n",
      " 65  DOUGLAS_FIR          49410 non-null  int64  \n",
      " 66  SPRUCE               49410 non-null  int64  \n",
      " 67  CEDAR                49410 non-null  int64  \n",
      " 68  PONDEROSA_PINE       49410 non-null  int64  \n",
      " 69  OTHER_SOFTWOOD       49410 non-null  int64  \n",
      " 70  LODGEPOLE_PINE       49410 non-null  int64  \n",
      " 71  HEMLOCK              49410 non-null  int64  \n",
      " 72  RED_ALDER            49410 non-null  int64  \n",
      " 73  TANOAK               49410 non-null  int64  \n",
      " 74  JUNIPER              49410 non-null  int64  \n",
      " 75  LARCH                49410 non-null  int64  \n",
      "dtypes: float64(2), int64(73), object(1)\n",
      "memory usage: 29.0+ MB\n"
     ]
    }
   ],
   "source": [
    "INVENTORY = '../data/processed/inventory_features.csv'\n",
    "inv_data = pd.read_csv(INVENTORY)\n",
    "inv_data = inv_data.set_index('uuid')\n",
    "inv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out some of the training data\n",
    "We can exclude some of the training data based on how far separated the inventory data (interpolated using FVS simulations) is from the year the lidar was collected. Similarly, we can screen out training examples that had relatively low density of lidar returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_and_inv = lidar_data.merge(inv_data, how='outer', left_index=True, right_index=True)\n",
    "lidar_and_inv = lidar_and_inv.reset_index()\n",
    "lidar_and_inv = lidar_and_inv.loc[lidar_and_inv.uuid.isin(lidar_data.index.get_level_values(0))]\n",
    "lidar_and_inv = lidar_and_inv.dropna(subset=['year'])\n",
    "lidar_and_inv['year_diff'] = abs(lidar_and_inv['year'] - lidar_and_inv['lidar_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4073 entries, 0 to 60804\n",
      "Data columns (total 100 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   uuid                      4073 non-null   object \n",
      " 1   lidar_year                4073 non-null   int64  \n",
      " 2   lidar_acq                 4073 non-null   object \n",
      " 3   strat0_return-proportion  4073 non-null   float64\n",
      " 4   strat1_return-proportion  4073 non-null   float64\n",
      " 5   strat2_return-proportion  4073 non-null   float64\n",
      " 6   strat3_return-proportion  4073 non-null   float64\n",
      " 7   strat4_return-proportion  4073 non-null   float64\n",
      " 8   strat5_return-proportion  4073 non-null   float64\n",
      " 9   height_05-percentile      4073 non-null   float64\n",
      " 10  height_25-percentile      4073 non-null   float64\n",
      " 11  height_50-percentile      4073 non-null   float64\n",
      " 12  height_75-percentile      4073 non-null   float64\n",
      " 13  height_95_percentile      4073 non-null   float64\n",
      " 14  cover                     4073 non-null   float64\n",
      " 15  potential_volume          4073 non-null   float64\n",
      " 16  stddev_height             4073 non-null   float64\n",
      " 17  surface_volume            4073 non-null   float64\n",
      " 18  kurtosis                  4073 non-null   float64\n",
      " 19  skewness                  4073 non-null   float64\n",
      " 20  aspect                    4073 non-null   float64\n",
      " 21  elevation                 4073 non-null   float64\n",
      " 22  slope                     4073 non-null   float64\n",
      " 23  year                      4073 non-null   float64\n",
      " 24  tpa                       4073 non-null   float64\n",
      " 25  ba                        4073 non-null   float64\n",
      " 26  sdi                       4073 non-null   float64\n",
      " 27  ccf                       4073 non-null   float64\n",
      " 28  qmd                       4073 non-null   float64\n",
      " 29  tcuft                     4073 non-null   float64\n",
      " 30  topht                     4073 non-null   float64\n",
      " 31  number_of_strata          4073 non-null   float64\n",
      " 32  total_cover               4073 non-null   float64\n",
      " 33  structure_class           4073 non-null   object \n",
      " 34  canopy_baseheight         4073 non-null   float64\n",
      " 35  canopy_bulkdensity        4073 non-null   float64\n",
      " 36  aboveground_biomass       4073 non-null   float64\n",
      " 37  aboveground_carbon        4073 non-null   float64\n",
      " 38  gs_tpa                    4073 non-null   float64\n",
      " 39  AF                        4073 non-null   float64\n",
      " 40  AS                        4073 non-null   float64\n",
      " 41  BM                        4073 non-null   float64\n",
      " 42  BO                        4073 non-null   float64\n",
      " 43  CH                        4073 non-null   float64\n",
      " 44  CW                        4073 non-null   float64\n",
      " 45  DF                        4073 non-null   float64\n",
      " 46  DG                        4073 non-null   float64\n",
      " 47  ES                        4073 non-null   float64\n",
      " 48  GC                        4073 non-null   float64\n",
      " 49  GF                        4073 non-null   float64\n",
      " 50  IC                        4073 non-null   float64\n",
      " 51  JP                        4073 non-null   float64\n",
      " 52  KP                        4073 non-null   float64\n",
      " 53  LO                        4073 non-null   float64\n",
      " 54  LP                        4073 non-null   float64\n",
      " 55  MA                        4073 non-null   float64\n",
      " 56  MC                        4073 non-null   float64\n",
      " 57  MH                        4073 non-null   float64\n",
      " 58  NF                        4073 non-null   float64\n",
      " 59  OH                        4073 non-null   float64\n",
      " 60  OS                        4073 non-null   float64\n",
      " 61  OT                        4073 non-null   float64\n",
      " 62  PC                        4073 non-null   float64\n",
      " 63  PL                        4073 non-null   float64\n",
      " 64  PP                        4073 non-null   float64\n",
      " 65  PY                        4073 non-null   float64\n",
      " 66  RA                        4073 non-null   float64\n",
      " 67  RC                        4073 non-null   float64\n",
      " 68  RF                        4073 non-null   float64\n",
      " 69  SF                        4073 non-null   float64\n",
      " 70  SH                        4073 non-null   float64\n",
      " 71  SP                        4073 non-null   float64\n",
      " 72  SS                        4073 non-null   float64\n",
      " 73  TO                        4073 non-null   float64\n",
      " 74  WA                        4073 non-null   float64\n",
      " 75  WB                        4073 non-null   float64\n",
      " 76  WF                        4073 non-null   float64\n",
      " 77  WH                        4073 non-null   float64\n",
      " 78  WI                        4073 non-null   float64\n",
      " 79  WJ                        4073 non-null   float64\n",
      " 80  WL                        4073 non-null   float64\n",
      " 81  WO                        4073 non-null   float64\n",
      " 82  WP                        4073 non-null   float64\n",
      " 83  YC                        4073 non-null   float64\n",
      " 84  TRUE_FIR                  4073 non-null   float64\n",
      " 85  OTHER_HARDWOOD            4073 non-null   float64\n",
      " 86  MAPLE                     4073 non-null   float64\n",
      " 87  OAK                       4073 non-null   float64\n",
      " 88  DOUGLAS_FIR               4073 non-null   float64\n",
      " 89  SPRUCE                    4073 non-null   float64\n",
      " 90  CEDAR                     4073 non-null   float64\n",
      " 91  PONDEROSA_PINE            4073 non-null   float64\n",
      " 92  OTHER_SOFTWOOD            4073 non-null   float64\n",
      " 93  LODGEPOLE_PINE            4073 non-null   float64\n",
      " 94  HEMLOCK                   4073 non-null   float64\n",
      " 95  RED_ALDER                 4073 non-null   float64\n",
      " 96  TANOAK                    4073 non-null   float64\n",
      " 97  JUNIPER                   4073 non-null   float64\n",
      " 98  LARCH                     4073 non-null   float64\n",
      " 99  year_diff                 4073 non-null   float64\n",
      "dtypes: float64(96), int64(1), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "lidar_and_inv = lidar_and_inv.loc[lidar_and_inv.groupby(by=['uuid', 'lidar_year'])['year_diff'].idxmin().astype(int)]\n",
    "lidar_and_inv.index = lidar_and_inv.index.astype(int)\n",
    "lidar_and_inv = lidar_and_inv.loc[lidar_and_inv.year_diff <= 5]\n",
    "lidar_and_inv['lidar_year'] = lidar_and_inv['lidar_year'].astype(int)\n",
    "lidar_and_inv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,073 samples\n",
      "Columns: ['uuid' 'lidar_year' 'lidar_acq' 'strat0_return-proportion'\n",
      " 'strat1_return-proportion' 'strat2_return-proportion'\n",
      " 'strat3_return-proportion' 'strat4_return-proportion'\n",
      " 'strat5_return-proportion' 'height_05-percentile' 'height_25-percentile'\n",
      " 'height_50-percentile' 'height_75-percentile' 'height_95_percentile'\n",
      " 'cover' 'potential_volume' 'stddev_height' 'surface_volume' 'kurtosis'\n",
      " 'skewness' 'aspect' 'elevation' 'slope' 'year' 'tpa' 'ba' 'sdi' 'ccf'\n",
      " 'qmd' 'tcuft' 'topht' 'number_of_strata' 'total_cover' 'structure_class'\n",
      " 'canopy_baseheight' 'canopy_bulkdensity' 'aboveground_biomass'\n",
      " 'aboveground_carbon' 'gs_tpa' 'AF' 'AS' 'BM' 'BO' 'CH' 'CW' 'DF' 'DG'\n",
      " 'ES' 'GC' 'GF' 'IC' 'JP' 'KP' 'LO' 'LP' 'MA' 'MC' 'MH' 'NF' 'OH' 'OS'\n",
      " 'OT' 'PC' 'PL' 'PP' 'PY' 'RA' 'RC' 'RF' 'SF' 'SH' 'SP' 'SS' 'TO' 'WA'\n",
      " 'WB' 'WF' 'WH' 'WI' 'WJ' 'WL' 'WO' 'WP' 'YC' 'TRUE_FIR' 'OTHER_HARDWOOD'\n",
      " 'MAPLE' 'OAK' 'DOUGLAS_FIR' 'SPRUCE' 'CEDAR' 'PONDEROSA_PINE'\n",
      " 'OTHER_SOFTWOOD' 'LODGEPOLE_PINE' 'HEMLOCK' 'RED_ALDER' 'TANOAK'\n",
      " 'JUNIPER' 'LARCH' 'year_diff' 'lat' 'lon' 'ecoregion3' 'agency'\n",
      " 'distance_to_water_m' 'plot_size_ac' 'meas_yr']\n"
     ]
    }
   ],
   "source": [
    "df = lidar_and_inv.merge(plot_data, how='outer', left_on=['uuid'], right_on=['uuid']).dropna()\n",
    "df['lidar_year'] = df['lidar_year'].astype(int)\n",
    "print('{:,d} samples'.format(len(df)))\n",
    "print('Columns:', df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4073 entries, 0 to 4072\n",
      "Columns: 107 entries, uuid to meas_yr\n",
      "dtypes: float64(100), int64(2), object(5)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "OUTLIERS = '../data/interim/outlier_uuids.csv'\n",
    "outliers = pd.read_csv(OUTLIERS)\n",
    "# filter out the height outliers\n",
    "df = df[~df.uuid.isin(outliers.outlier_uuid)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3834 entries, 1 to 4072\n",
      "Columns: 107 entries, uuid to meas_yr\n",
      "dtypes: float64(100), int64(2), object(5)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[(df.topht > 0)&(df.total_cover >= 10)&(df.qmd > 0)]\n",
    "df.loc[df.qmd > 50, 'qmd'] = 50\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/lidar_structure_training_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect how many samples we have for different years, regions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010.0</th>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011.0</th>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012.0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013.0</th>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014.0</th>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015.0</th>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016.0</th>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017.0</th>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018.0</th>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "year         \n",
       "2010.0    487\n",
       "2011.0    392\n",
       "2012.0      5\n",
       "2013.0    574\n",
       "2014.0    366\n",
       "2015.0    309\n",
       "2016.0    762\n",
       "2017.0    581\n",
       "2018.0    358"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['year'])[['uuid']].count().rename({'uuid':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2010.0</th>\n",
       "      <th>2011.0</th>\n",
       "      <th>2012.0</th>\n",
       "      <th>2013.0</th>\n",
       "      <th>2014.0</th>\n",
       "      <th>2015.0</th>\n",
       "      <th>2016.0</th>\n",
       "      <th>2017.0</th>\n",
       "      <th>2018.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meas_yr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>487</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>392</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>574</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>317</td>\n",
       "      <td>42</td>\n",
       "      <td>92</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>244</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year     2010.0  2011.0  2012.0  2013.0  2014.0  2015.0  2016.0  2017.0  \\\n",
       "meas_yr                                                                   \n",
       "2010        487       0       4       0       2       9       0       0   \n",
       "2011          0     392       1       0       2       3       0       0   \n",
       "2013          0       0       0     574       3      11      14      27   \n",
       "2014          0       0       0       0     317      42      92      35   \n",
       "2015          0       0       0       0      42     244     112      29   \n",
       "2016          0       0       0       0       0       0     544      34   \n",
       "2017          0       0       0       0       0       0       0     456   \n",
       "2018          0       0       0       0       0       0       0       0   \n",
       "\n",
       "year     2018.0  \n",
       "meas_yr          \n",
       "2010          0  \n",
       "2011          0  \n",
       "2013          3  \n",
       "2014          0  \n",
       "2015          5  \n",
       "2016         25  \n",
       "2017          0  \n",
       "2018        325  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df, \n",
    "               values='uuid', \n",
    "               aggfunc='count', \n",
    "               index=['meas_yr'], \n",
    "               columns=['year'], \n",
    "               fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>year</th>\n",
       "      <th>plot_size_ac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecoregion3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>657</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>1692</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>columbia_plateau</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>288</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>405</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>116</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    uuid  year  plot_size_ac\n",
       "ecoregion3                                                                  \n",
       "blue_mountains                                        85     2             1\n",
       "cascades                                             657     6             3\n",
       "coast_range                                         1692     9             2\n",
       "columbia_plateau                                       7     3             1\n",
       "eastern_cascades_slopes_and_foothills                198     5             2\n",
       "klamath_mountains_california_high_north_coast_r...   288     6             2\n",
       "north_cascades                                       405     5             2\n",
       "northern_rockies                                     101     4             1\n",
       "puget_lowland                                        116     6             1\n",
       "willamette_valley                                     45     3             3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoreg_counts = df.groupby(by=['ecoregion3'])[['uuid', 'year', 'plot_size_ac']].nunique()\n",
    "ecoreg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available features\n",
    "The different types of predictor variables we can use to predict a forest attribute, including climate, lidar-derived, soil, and satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strat0_return-proportion</th>\n",
       "      <th>strat1_return-proportion</th>\n",
       "      <th>strat2_return-proportion</th>\n",
       "      <th>strat3_return-proportion</th>\n",
       "      <th>strat4_return-proportion</th>\n",
       "      <th>strat5_return-proportion</th>\n",
       "      <th>height_05-percentile</th>\n",
       "      <th>height_25-percentile</th>\n",
       "      <th>height_50-percentile</th>\n",
       "      <th>height_75-percentile</th>\n",
       "      <th>height_95_percentile</th>\n",
       "      <th>cover</th>\n",
       "      <th>potential_volume</th>\n",
       "      <th>stddev_height</th>\n",
       "      <th>surface_volume</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.160452</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>0.101705</td>\n",
       "      <td>0.209455</td>\n",
       "      <td>0.177241</td>\n",
       "      <td>0.197785</td>\n",
       "      <td>1.259226</td>\n",
       "      <td>9.648884</td>\n",
       "      <td>17.329583</td>\n",
       "      <td>24.133792</td>\n",
       "      <td>32.197671</td>\n",
       "      <td>77.478997</td>\n",
       "      <td>3292.205637</td>\n",
       "      <td>6.416928</td>\n",
       "      <td>1989.334152</td>\n",
       "      <td>4.217114</td>\n",
       "      <td>0.054266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.181026</td>\n",
       "      <td>0.070151</td>\n",
       "      <td>0.113448</td>\n",
       "      <td>0.188833</td>\n",
       "      <td>0.176981</td>\n",
       "      <td>0.257019</td>\n",
       "      <td>5.408052</td>\n",
       "      <td>10.842344</td>\n",
       "      <td>13.567674</td>\n",
       "      <td>15.229965</td>\n",
       "      <td>16.861420</td>\n",
       "      <td>20.973250</td>\n",
       "      <td>1694.542669</td>\n",
       "      <td>4.150131</td>\n",
       "      <td>1365.548481</td>\n",
       "      <td>13.982405</td>\n",
       "      <td>1.382902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>-3.370587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.042195</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.061247</td>\n",
       "      <td>0.027158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>5.886250</td>\n",
       "      <td>12.610000</td>\n",
       "      <td>19.723500</td>\n",
       "      <td>68.906629</td>\n",
       "      <td>2041.125000</td>\n",
       "      <td>3.228653</td>\n",
       "      <td>888.140396</td>\n",
       "      <td>1.955420</td>\n",
       "      <td>-0.753138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.041918</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>0.158348</td>\n",
       "      <td>0.126651</td>\n",
       "      <td>0.051950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.290000</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>22.760000</td>\n",
       "      <td>30.107250</td>\n",
       "      <td>85.362201</td>\n",
       "      <td>3117.000000</td>\n",
       "      <td>5.399556</td>\n",
       "      <td>1856.541138</td>\n",
       "      <td>2.602262</td>\n",
       "      <td>-0.102241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.204104</td>\n",
       "      <td>0.081076</td>\n",
       "      <td>0.139455</td>\n",
       "      <td>0.298456</td>\n",
       "      <td>0.278370</td>\n",
       "      <td>0.363624</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>33.267499</td>\n",
       "      <td>42.927376</td>\n",
       "      <td>92.714566</td>\n",
       "      <td>4339.500000</td>\n",
       "      <td>8.679241</td>\n",
       "      <td>2837.768494</td>\n",
       "      <td>3.900820</td>\n",
       "      <td>0.648880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.626115</td>\n",
       "      <td>0.837302</td>\n",
       "      <td>0.933384</td>\n",
       "      <td>0.866815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.510002</td>\n",
       "      <td>108.769997</td>\n",
       "      <td>116.809998</td>\n",
       "      <td>119.860001</td>\n",
       "      <td>121.559998</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12180.000000</td>\n",
       "      <td>28.338732</td>\n",
       "      <td>11849.857422</td>\n",
       "      <td>400.029964</td>\n",
       "      <td>18.986166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       strat0_return-proportion  strat1_return-proportion  \\\n",
       "count               3834.000000               3834.000000   \n",
       "mean                   0.160452                  0.064758   \n",
       "std                    0.181026                  0.070151   \n",
       "min                    0.000000                  0.000000   \n",
       "25%                    0.042195                  0.020416   \n",
       "50%                    0.086286                  0.041918   \n",
       "75%                    0.204104                  0.081076   \n",
       "max                    1.000000                  0.626115   \n",
       "\n",
       "       strat2_return-proportion  strat3_return-proportion  \\\n",
       "count               3834.000000               3834.000000   \n",
       "mean                   0.101705                  0.209455   \n",
       "std                    0.113448                  0.188833   \n",
       "min                    0.000000                  0.000000   \n",
       "25%                    0.021888                  0.061247   \n",
       "50%                    0.059361                  0.158348   \n",
       "75%                    0.139455                  0.298456   \n",
       "max                    0.837302                  0.933384   \n",
       "\n",
       "       strat4_return-proportion  strat5_return-proportion  \\\n",
       "count               3834.000000               3834.000000   \n",
       "mean                   0.177241                  0.197785   \n",
       "std                    0.176981                  0.257019   \n",
       "min                    0.000000                  0.000000   \n",
       "25%                    0.027158                  0.000000   \n",
       "50%                    0.126651                  0.051950   \n",
       "75%                    0.278370                  0.363624   \n",
       "max                    0.866815                  1.000000   \n",
       "\n",
       "       height_05-percentile  height_25-percentile  height_50-percentile  \\\n",
       "count           3834.000000           3834.000000           3834.000000   \n",
       "mean               1.259226              9.648884             17.329583   \n",
       "std                5.408052             10.842344             13.567674   \n",
       "min                0.000000              0.000000              0.000000   \n",
       "25%                0.000000              0.390000              5.886250   \n",
       "50%                0.000000              6.290000             16.240000   \n",
       "75%                0.290000             15.800000             25.600000   \n",
       "max               98.510002            108.769997            116.809998   \n",
       "\n",
       "       height_75-percentile  height_95_percentile        cover  \\\n",
       "count           3834.000000           3834.000000  3834.000000   \n",
       "mean              24.133792             32.197671    77.478997   \n",
       "std               15.229965             16.861420    20.973250   \n",
       "min                0.000000              0.000000     0.000000   \n",
       "25%               12.610000             19.723500    68.906629   \n",
       "50%               22.760000             30.107250    85.362201   \n",
       "75%               33.267499             42.927376    92.714566   \n",
       "max              119.860001            121.559998   100.000000   \n",
       "\n",
       "       potential_volume  stddev_height  surface_volume     kurtosis  \\\n",
       "count       3834.000000    3834.000000     3834.000000  3834.000000   \n",
       "mean        3292.205637       6.416928     1989.334152     4.217114   \n",
       "std         1694.542669       4.150131     1365.548481    13.982405   \n",
       "min            0.000000       0.000000        0.000000     1.103300   \n",
       "25%         2041.125000       3.228653      888.140396     1.955420   \n",
       "50%         3117.000000       5.399556     1856.541138     2.602262   \n",
       "75%         4339.500000       8.679241     2837.768494     3.900820   \n",
       "max        12180.000000      28.338732    11849.857422   400.029964   \n",
       "\n",
       "          skewness  \n",
       "count  3834.000000  \n",
       "mean      0.054266  \n",
       "std       1.382902  \n",
       "min      -3.370587  \n",
       "25%      -0.753138  \n",
       "50%      -0.102241  \n",
       "75%       0.648880  \n",
       "max      18.986166  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_LIDAR_COLS = ['strat0_return-proportion', 'strat1_return-proportion', \n",
    "                  'strat2_return-proportion', 'strat3_return-proportion', \n",
    "                  'strat4_return-proportion', 'strat5_return-proportion', \n",
    "                  'height_05-percentile',  'height_25-percentile', \n",
    "                  'height_50-percentile', 'height_75-percentile',\n",
    "                  'height_95_percentile', 'cover', \n",
    "                  'potential_volume', 'stddev_height', \n",
    "                  'surface_volume', 'kurtosis', 'skewness']\n",
    "df[USE_LIDAR_COLS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features and targets\n",
    "This is the first step in determining what features we want to use, and what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = USE_LIDAR_COLS + ['elevation', 'lat', 'lon'] + ['ecoregion3'] \n",
    "Y_COLS = ['total_cover', 'topht', 'qmd', 'tcuft']\n",
    "\n",
    "Y_NAMES = [col.upper() for col in Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_cover</th>\n",
       "      <th>topht</th>\n",
       "      <th>qmd</th>\n",
       "      <th>tcuft</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecoregion3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue_mountains</th>\n",
       "      <td>32.4</td>\n",
       "      <td>59.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1851.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast_range</th>\n",
       "      <td>68.6</td>\n",
       "      <td>107.8</td>\n",
       "      <td>14.7</td>\n",
       "      <td>9386.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north_cascades</th>\n",
       "      <td>63.0</td>\n",
       "      <td>87.3</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5976.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cascades</th>\n",
       "      <td>64.1</td>\n",
       "      <td>102.2</td>\n",
       "      <td>13.3</td>\n",
       "      <td>7774.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klamath_mountains_california_high_north_coast_range</th>\n",
       "      <td>56.2</td>\n",
       "      <td>87.2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6567.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eastern_cascades_slopes_and_foothills</th>\n",
       "      <td>44.5</td>\n",
       "      <td>76.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3902.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern_rockies</th>\n",
       "      <td>43.4</td>\n",
       "      <td>74.3</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2740.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puget_lowland</th>\n",
       "      <td>68.0</td>\n",
       "      <td>93.3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>6288.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>willamette_valley</th>\n",
       "      <td>64.1</td>\n",
       "      <td>111.4</td>\n",
       "      <td>17.4</td>\n",
       "      <td>9277.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    total_cover  topht   qmd  \\\n",
       "ecoregion3                                                                     \n",
       "blue_mountains                                             32.4   59.5  12.5   \n",
       "coast_range                                                68.6  107.8  14.7   \n",
       "north_cascades                                             63.0   87.3  11.9   \n",
       "cascades                                                   64.1  102.2  13.3   \n",
       "klamath_mountains_california_high_north_coast_r...         56.2   87.2  10.1   \n",
       "eastern_cascades_slopes_and_foothills                      44.5   76.9  12.0   \n",
       "northern_rockies                                           43.4   74.3  11.1   \n",
       "puget_lowland                                              68.0   93.3  12.4   \n",
       "willamette_valley                                          64.1  111.4  17.4   \n",
       "\n",
       "                                                     tcuft  \n",
       "ecoregion3                                                  \n",
       "blue_mountains                                      1851.8  \n",
       "coast_range                                         9386.4  \n",
       "north_cascades                                      5976.9  \n",
       "cascades                                            7774.7  \n",
       "klamath_mountains_california_high_north_coast_r...  6567.8  \n",
       "eastern_cascades_slopes_and_foothills               3902.4  \n",
       "northern_rockies                                    2740.8  \n",
       "puget_lowland                                       6288.1  \n",
       "willamette_valley                                   9277.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_cover</th>\n",
       "      <th>topht</th>\n",
       "      <th>qmd</th>\n",
       "      <th>tcuft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "      <td>3834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.158842</td>\n",
       "      <td>98.537559</td>\n",
       "      <td>13.424433</td>\n",
       "      <td>7697.220918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.894079</td>\n",
       "      <td>41.156739</td>\n",
       "      <td>7.520646</td>\n",
       "      <td>7028.290334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>8.014735</td>\n",
       "      <td>2522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>12.228978</td>\n",
       "      <td>5831.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>17.253404</td>\n",
       "      <td>10610.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>52138.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_cover        topht          qmd         tcuft\n",
       "count  3834.000000  3834.000000  3834.000000   3834.000000\n",
       "mean     63.158842    98.537559    13.424433   7697.220918\n",
       "std      20.894079    41.156739     7.520646   7028.290334\n",
       "min      10.000000     9.000000     1.000000      1.000000\n",
       "25%      50.000000    68.000000     8.014735   2522.000000\n",
       "50%      67.000000    97.000000    12.228978   5831.000000\n",
       "75%      79.000000   126.000000    17.253404  10610.750000\n",
       "max     100.000000   256.000000    50.000000  52138.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "USE_REGIONS = ['blue_mountains', 'coast_range', 'north_cascades', 'cascades',\n",
    "               'klamath_mountains_california_high_north_coast_range', \n",
    "               'eastern_cascades_slopes_and_foothills', 'northern_rockies',\n",
    "               'puget_lowland', 'willamette_valley']\n",
    "display(df.groupby('ecoregion3')[Y_COLS].mean().round(1).loc[USE_REGIONS])\n",
    "display(df[Y_COLS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "X, Y = df[X_COLS], df[Y_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3834 entries, 0 to 3833\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   strat0_return-proportion  3834 non-null   float64\n",
      " 1   strat1_return-proportion  3834 non-null   float64\n",
      " 2   strat2_return-proportion  3834 non-null   float64\n",
      " 3   strat3_return-proportion  3834 non-null   float64\n",
      " 4   strat4_return-proportion  3834 non-null   float64\n",
      " 5   strat5_return-proportion  3834 non-null   float64\n",
      " 6   height_05-percentile      3834 non-null   float64\n",
      " 7   height_25-percentile      3834 non-null   float64\n",
      " 8   height_50-percentile      3834 non-null   float64\n",
      " 9   height_75-percentile      3834 non-null   float64\n",
      " 10  height_95_percentile      3834 non-null   float64\n",
      " 11  cover                     3834 non-null   float64\n",
      " 12  potential_volume          3834 non-null   float64\n",
      " 13  stddev_height             3834 non-null   float64\n",
      " 14  surface_volume            3834 non-null   float64\n",
      " 15  kurtosis                  3834 non-null   float64\n",
      " 16  skewness                  3834 non-null   float64\n",
      " 17  elevation                 3834 non-null   float64\n",
      " 18  lat                       3834 non-null   float64\n",
      " 19  lon                       3834 non-null   float64\n",
      " 20  ecoregion3                3834 non-null   object \n",
      "dtypes: float64(20), object(1)\n",
      "memory usage: 629.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df[X_COLS].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets by ecoregion\n",
    "We want to explore model transferability between regions, so we'll train models independently on subsets of the data within a single ecoregion, as well as a model that is trained on all available ecoregions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoregions = list(np.sort([reg for reg in pd.unique(df.ecoregion3) if ecoreg_counts.loc[reg]['uuid'] > 20]))\n",
    "\n",
    "eco_X_idx = [X.loc[X.ecoregion3 == eco].index.values for eco in ecoregions]\n",
    "\n",
    "eco_X_dfs = [X.loc[X.ecoregion3 == eco].drop(['ecoregion3'], axis=1) for eco in ecoregions]\n",
    "eco_Y_dfs = [Y.loc[idx] for idx in eco_X_idx]\n",
    "\n",
    "# append a \"global\" model that contains data from all ecoregions\n",
    "ecoregions.append('all')\n",
    "ecoregion_names = ['_'.join(x.split('_')[0:2]) for x in ecoregions]\n",
    "eco_X_dfs.append(X.drop(['ecoregion3'], axis=1))\n",
    "eco_Y_dfs.append(Y)\n",
    "\n",
    "ecoregion_display_names = [' '.join(x.upper().split('_')[:2]) for x in ecoregions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_class_bins = [10,40,70,100]\n",
    "cover_class_labels = ['OPEN', 'MODERATE', 'CLOSED']\n",
    "height_class_bins = np.arange(0,300,20)\n",
    "height_class_labels = [f'{x}-{x+20}' for x in height_class_bins[:-1]]\n",
    "diameter_class_bins = [1, 5, 10, 15, 20, 999]\n",
    "diameter_class_labels = ['SEED/SAP', 'SMALL', 'MEDIUM', 'LARGE', 'VERY_LARGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "We'll use Root Mean Square Error to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(obs, pred):\n",
    "    return np.sqrt((np.square(obs-pred)).mean())\n",
    "\n",
    "def nrmse(obs, pred):\n",
    "    return rmse(pred,obs) / obs.mean()\n",
    "\n",
    "def mae(obs, pred):   \n",
    "    return abs(pred - obs).mean()\n",
    "\n",
    "def mape(obs, pred):    \n",
    "    return abs(pred - obs).mean() / obs.mean()\n",
    "\n",
    "def bias(obs, pred):   \n",
    "    return (pred - obs).mean()\n",
    "\n",
    "def rel_bias(obs, pred):\n",
    "    return bias(pred,obs) / obs.mean()\n",
    "\n",
    "def bin_accuracy(obs, pred, bins, fuzzy_tol=0):\n",
    "    pred_binned = np.digitize(pred, bins)\n",
    "    obs_binned = np.digitisze(obs, bins)\n",
    "    diff = abs(pred_binned - obs_binned)\n",
    "    \n",
    "    return (diff <= fuzzy_tol).sum() / len(diff)\n",
    "\n",
    "def confidence_interval_half(X, confidence=0.95):\n",
    "    n = len(X)\n",
    "    se = stats.sem(X)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit some models\n",
    "For each type of model, we'll employ cross-validation to tune model hyperparameters, generating a tuned model for each ecoregion as well as a tuned model using all training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Lasso': Lasso(), \n",
    "    'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_jobs=-1), \n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(), \n",
    "}\n",
    "\n",
    "FIT_PARAMS = {\n",
    "    'ElasticNet': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "        'l1_ratio': np.arange(0.0, 1.0, 0.1),\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-4,2,7),\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [1,2,3,4,5,10,20],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'max_depth': [5, 20, None],\n",
    "        'max_samples': [0.5, None]\n",
    "    },\n",
    "    'HistGradientBoostingRegressor': {\n",
    "        'max_iter': [50, 100, 200],\n",
    "        'min_samples_leaf': [5, 10, 20],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTER_FOLDS = 5\n",
    "NUM_INNER_FOLDS = 3\n",
    "SCORE_FUNCS = [rmse, nrmse, mae, mape, bias, rel_bias]\n",
    "score_names = [func.__name__ for func in SCORE_FUNCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_insider_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['cv_results'] = {}\n",
    "            for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "                fold_num = fold_idx + 1\n",
    "                results[region][model_name]['cv_results'][fold_num] = {}\n",
    "                results[region][model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "                results[region][model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[region][model_name]['cv_results'][fold_num][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_global_results_dictionary(regions, model_names, num_outer_folds, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for model_name in model_names:\n",
    "        results[model_name] = {}\n",
    "        results[model_name]['fitted_model'] = None\n",
    "        results[model_name]['best_params'] = None\n",
    "        results[model_name]['cv_results'] = {}\n",
    "        for fold_idx in range(num_outer_folds):  # results from each outer loop of nested CV\n",
    "            fold_num = fold_idx + 1\n",
    "            results[model_name]['cv_results'][fold_num] = {}\n",
    "            results[model_name]['cv_results'][fold_num]['best_params'] = None \n",
    "            results[model_name]['cv_results'][fold_num]['predict_time'] = None\n",
    "            for region in regions:\n",
    "                results[model_name]['cv_results'][fold_num][region] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[model_name]['cv_results'][fold_num][region][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "def build_outsider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for region in regions:\n",
    "        results[region] = {}\n",
    "        for model_name in model_names:\n",
    "            results[region][model_name] = {}\n",
    "            results[region][model_name]['fitted_model'] = None\n",
    "            results[region][model_name]['best_params'] = None\n",
    "            results[region][model_name]['predict_time'] = None\n",
    "            for score_func in score_funcs:\n",
    "                score_func_name = score_func.__name__\n",
    "                results[region][model_name][score_func_name] = {\n",
    "                    y: None for y in target_vars\n",
    "                }\n",
    "    return results\n",
    "\n",
    "def build_visiting_insider_results_dictionary(regions, model_names, score_funcs, target_vars):\n",
    "    results = {}\n",
    "    for target_region in regions:\n",
    "        results[target_region] = {}\n",
    "        for train_region in [r for r in regions if r != target_region]:\n",
    "            results[target_region][train_region] = {}\n",
    "            for model_name in model_names:\n",
    "                results[target_region][train_region][model_name] = {}\n",
    "                for score_func in score_funcs:\n",
    "                    score_func_name = score_func.__name__\n",
    "                    results[target_region][train_region][model_name][score_func_name] = {\n",
    "                        y: None for y in target_vars\n",
    "                    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_insider_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X = eco_X_dfs[i]\n",
    "        Y = eco_Y_dfs[i]\n",
    "        outer_groups = df.loc[X.index, 'uuid'].values\n",
    "        \n",
    "        outer_fold_num = 1\n",
    "        for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "            X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "            Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n",
    "            inner_groups = df.loc[X_train.index, 'uuid'].values\n",
    "            \n",
    "            inner_search = GridSearchCV(pipe, search_params, \n",
    "                                        scoring='neg_mean_squared_error', \n",
    "                                        n_jobs=-1, cv=cv_inner, refit=True)\n",
    "            \n",
    "            inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "            \n",
    "            inner_best_model = inner_result.best_estimator_\n",
    "            start_time = time.time()\n",
    "            Y_pred = inner_best_model.predict(X_test)\n",
    "            end_time = time.time()\n",
    "            total_predict_time = end_time - start_time\n",
    "            avg_predict_time = total_predict_time / len(X_test)\n",
    "            insider_results[ecoregion][model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(Y_test, Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    insider_results[ecoregion][model_name]['cv_results'][outer_fold_num][score_func_name][y_var] = scores.loc[y_var]\n",
    "                    \n",
    "            print(outer_fold_num, end='... ')\n",
    "            outer_fold_num += 1\n",
    "        print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "        # done with scoring of models, now time to tune a model using the whole dataset\n",
    "        outer_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_outer, refit=True)\n",
    "        outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "        \n",
    "        # now fit on the entire dataset, not just training set\n",
    "        model = outer_result.best_estimator_\n",
    "        model.set_params(**outer_result.best_params_)\n",
    "        X = df.loc[df.ecoregion3 == ecoregion, X_COLS].drop(['ecoregion3'], axis=1)\n",
    "        y = df.loc[df.ecoregion3 == ecoregion, Y_COLS]\n",
    "        model.fit(X, y)\n",
    "\n",
    "        eco_name = '_'.join(ecoregion.split('_')[:2])\n",
    "        outfile = f'{eco_name}-lidar-{model_name}-chained.pkl'\n",
    "        outpath = os.path.join('../models/structure_models', outfile)\n",
    "        with open(outpath, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "        insider_results[ecoregion][model_name]['fitted_model'] = model\n",
    "        insider_results[ecoregion][model_name]['best_params'] = outer_result.best_params_\n",
    "        \n",
    "        cv_results_dict = {ecoregion: insider_results[ecoregion][model_name]['cv_results'] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return cv_results_dict\n",
    "\n",
    "def tune_outsider_model(model_name, num_folds=5):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    train_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    groupkfold = GroupKFold(num_folds)\n",
    "    \n",
    "    for i, ecoregion in enumerate(train_regions):\n",
    "        ecoregion_name = ecoregion_display_names[i]\n",
    "        print(f'Starting on {ecoregion_name}', end='... ')\n",
    "        X_train = X.loc[X.ecoregion3 != ecoregion].drop('ecoregion3', axis=1)\n",
    "        Y_train = Y.loc[X_train.index]\n",
    "        X_test = X.loc[X.ecoregion3 == ecoregion].drop('ecoregion3', axis=1)\n",
    "        Y_test = Y.loc[X_test.index]\n",
    "        groups = df.loc[X_train.index]['ecoregion3'].values\n",
    "        \n",
    "        search = GridSearchCV(pipe, search_params, \n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1, cv=groupkfold, refit=True)\n",
    "            \n",
    "        result = search.fit(X_train, Y_train, groups=groups)\n",
    "        print('Done fitting, now scoring', end='... ')\n",
    "        outsider_results[ecoregion][model_name]['best_params'] = result.best_params_\n",
    "        outsider_results[ecoregion][model_name]['fitted_model'] = result.best_estimator_\n",
    "        \n",
    "        best_model = result.best_estimator_       \n",
    "        start_time = time.time()\n",
    "        Y_pred = best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        outsider_results[ecoregion][model_name]['predict_time'] = avg_predict_time\n",
    "            \n",
    "        for score_func in SCORE_FUNCS:\n",
    "            score_func_name = score_func.__name__\n",
    "            scores = score_func(Y_test, Y_pred)\n",
    "            for y_var in scores.index:\n",
    "                outsider_results[ecoregion][model_name][score_func_name][y_var] = scores.loc[y_var]\n",
    "        \n",
    "        results_dict = {ecoregion: outsider_results[ecoregion][model_name] for ecoregion in train_regions}\n",
    "        print('All done.')\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def tune_global_model(model_name, num_outer_folds=NUM_OUTER_FOLDS, num_inner_folds=NUM_INNER_FOLDS):\n",
    "    print(model_name)\n",
    "    print('-'*len(model_name))\n",
    "    print(f'Scoring with {NUM_OUTER_FOLDS} folds... ', end='')\n",
    "    model = MODELS[model_name]\n",
    "    fit_params = FIT_PARAMS[model_name]\n",
    "    test_regions = [x for x in ecoregions if x.upper() != 'ALL']\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RegressorChain(base_estimator=model)),\n",
    "    ])\n",
    "    search_params = {f'model__base_estimator__{key}': value for key, value in fit_params.items()}\n",
    "    \n",
    "    cv_outer = GroupKFold(num_outer_folds)\n",
    "    cv_inner = GroupKFold(num_inner_folds)\n",
    "    \n",
    "    X = df[X_COLS].drop('ecoregion3', axis=1)\n",
    "    Y = df[Y_COLS]\n",
    "    outer_groups = df['uuid'].values\n",
    "        \n",
    "    outer_fold_num = 1\n",
    "    for train_ix, test_ix in cv_outer.split(X, groups=outer_groups):\n",
    "        X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "        Y_train, Y_test = Y.loc[train_ix], Y.loc[test_ix]\n",
    "        inner_groups = df.loc[train_ix, 'uuid'].values\n",
    "\n",
    "        inner_search = GridSearchCV(pipe, search_params, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "        inner_result = inner_search.fit(X_train, Y_train, groups=inner_groups)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['best_params'] = inner_result.best_params_\n",
    "\n",
    "        inner_best_model = inner_result.best_estimator_\n",
    "        start_time = time.time()\n",
    "        Y_pred = inner_best_model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        total_predict_time = end_time - start_time\n",
    "        avg_predict_time = total_predict_time / len(X_test)\n",
    "        global_results[model_name]['cv_results'][outer_fold_num]['predict_time'] = avg_predict_time\n",
    "\n",
    "        for ecoregion in test_regions:\n",
    "            region_mask = (df.loc[test_ix, 'ecoregion3'] == ecoregion).values\n",
    "            regional_X_test = X_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_test = Y_test.loc[test_ix[region_mask]]\n",
    "            regional_Y_pred = inner_best_model.predict(regional_X_test)\n",
    "            \n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(regional_Y_test, regional_Y_pred)\n",
    "                for y_var in scores.index:\n",
    "                    global_results[model_name]['cv_results'][outer_fold_num][ecoregion][score_func_name][y_var] = scores.loc[y_var]\n",
    "\n",
    "        print(outer_fold_num, end='... ')\n",
    "        outer_fold_num += 1\n",
    "    \n",
    "    print('Done scoring. Now fitting a final model', end='... ')\n",
    "        \n",
    "    # done with scoring of models, now time to tune a model using the whole dataset\n",
    "    outer_search = GridSearchCV(pipe, search_params, \n",
    "                                scoring='neg_mean_squared_error', \n",
    "                                n_jobs=-1, cv=cv_outer, refit=True)\n",
    "    outer_result = outer_search.fit(X, Y, groups=outer_groups)\n",
    "\n",
    "     # now fit on the entire dataset, not just training set\n",
    "    model = outer_result.best_estimator_\n",
    "    model.set_params(**outer_result.best_params_)\n",
    "    X = df[X_COLS].drop(['ecoregion3'], axis=1)\n",
    "    y = df[Y_COLS]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    outfile = f'global-lidar-{model_name}-chained.pkl'\n",
    "    outpath = os.path.join('../models/structure_models', outfile)\n",
    "    with open(outpath, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print('All done.')\n",
    "\n",
    "    global_results[model_name]['fitted_model'] = outer_result.best_estimator_\n",
    "    global_results[model_name]['best_params'] = outer_result.best_params_\n",
    "\n",
    "    results_dict = global_results[model_name]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_global_results(results):\n",
    "    data = []\n",
    "    for fold in range(NUM_OUTER_FOLDS):\n",
    "        for ecoregion in ecoregions[:-1]:\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold+1, ecoregion, target, score_name, results['cv_results'][fold+1][ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Global Models\n",
    "These models get to see data from every ecoregion during training and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = build_global_results_dictionary(ecoregions[:-1], MODELS.keys(), NUM_OUTER_FOLDS, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Scoring with 5 folds... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... "
     ]
    }
   ],
   "source": [
    "elastic_global = tune_global_model('ElasticNet')\n",
    "lasso_global = tune_global_model('Lasso')\n",
    "knn_global = tune_global_model('KNeighborsRegressor')\n",
    "rf_global = tune_global_model('RandomForestRegressor')\n",
    "gbm_global = tune_global_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_global, lasso_global, knn_global, rf_global, gbm_global]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_global_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_global_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_global_results['ecoregion'] = all_global_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_global_results.columns = [col.upper() for col in all_global_results.columns]\n",
    "# all_global_results = all_global_results.rename({'SCORE': 'MAE'}, axis=1)\n",
    "all_global_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_global_results.to_csv('../data/processed/nestedcv_chained_global_results_lidar_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Insider Models\n",
    "These models are trained with observations from a single ecoregion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_results = build_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), 5, SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_insider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for fold_num in results[ecoregion].keys():\n",
    "            for target in Y_COLS:\n",
    "                for score_name in score_names:\n",
    "                    data.append((fold_num, ecoregion, target, score_name, results[ecoregion][fold_num][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Starting on BLUE MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on COAST RANGE... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on EASTERN CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on KLAMATH MOUNTAINS... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTH CASCADES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on NORTHERN ROCKIES... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on PUGET LOWLAND... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n",
      "Starting on WILLAMETTE VALLEY... 1... 2... 3... 4... 5... Done scoring. Now fitting a final model... All done.\n"
     ]
    }
   ],
   "source": [
    "elastic_insider = tune_insider_model('ElasticNet')\n",
    "lasso_insider = tune_insider_model('Lasso')\n",
    "knn_insider = tune_insider_model('KNeighborsRegressor')\n",
    "rf_insider = tune_insider_model('RandomForestRegressor')\n",
    "gbm_insider = tune_insider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>rmse</td>\n",
       "      <td>6.889208</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.202975</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mae</td>\n",
       "      <td>5.652041</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.166525</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>bias</td>\n",
       "      <td>-0.321978</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC     SCORE       MODEL\n",
       "0        1  Blue Mountains  total_cover   rmse  6.889208  ElasticNet\n",
       "1        1  Blue Mountains  total_cover  nrmse  0.202975  ElasticNet\n",
       "2        1  Blue Mountains  total_cover    mae  5.652041  ElasticNet\n",
       "3        1  Blue Mountains  total_cover   mape  0.166525  ElasticNet\n",
       "4        1  Blue Mountains  total_cover   bias -0.321978  ElasticNet"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_insider, lasso_insider, knn_insider, rf_insider, gbm_insider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_insider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_insider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_insider_results['ecoregion'] = all_insider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_insider_results.columns = [col.upper() for col in all_insider_results.columns]\n",
    "all_insider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_insider_results.to_csv('../data/processed/nestedcv_chained_insider_results_lidar_structure.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trained Insider Models to Score Visiting Insider Models\n",
    "These models are trained on a single region, and scored on other regions they've never seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_results = build_visiting_insider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET_ECOREGION</th>\n",
       "      <th>TRAIN_ECOREGION</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>16.037629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>topht</td>\n",
       "      <td>27.830946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>qmd</td>\n",
       "      <td>6.141004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>rmse</td>\n",
       "      <td>tcuft</td>\n",
       "      <td>3568.030775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>Cascades</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>0.494809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TARGET_ECOREGION TRAIN_ECOREGION       MODEL METRIC       TARGET  \\\n",
       "0   Blue Mountains        Cascades  ElasticNet   rmse  total_cover   \n",
       "1   Blue Mountains        Cascades  ElasticNet   rmse        topht   \n",
       "2   Blue Mountains        Cascades  ElasticNet   rmse          qmd   \n",
       "3   Blue Mountains        Cascades  ElasticNet   rmse        tcuft   \n",
       "4   Blue Mountains        Cascades  ElasticNet  nrmse  total_cover   \n",
       "\n",
       "         SCORE  \n",
       "0    16.037629  \n",
       "1    27.830946  \n",
       "2     6.141004  \n",
       "3  3568.030775  \n",
       "4     0.494809  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitor_results = []\n",
    "for target_region in ecoregions[:-1]:\n",
    "    for train_region in [r for r in ecoregions[:-1] if r != target_region]:\n",
    "        for model_name in MODELS.keys():\n",
    "            model = insider_results[train_region][model_name]['fitted_model']\n",
    "            targ_idx = X.loc[X.ecoregion3 == target_region].index.values\n",
    "            targ_X = X.loc[targ_idx].drop(['ecoregion3'], axis=1)\n",
    "            pred = model.predict(targ_X)\n",
    "            obs = Y.loc[targ_idx]\n",
    "            for score_func in SCORE_FUNCS:\n",
    "                score_func_name = score_func.__name__\n",
    "                scores = score_func(obs, pred)\n",
    "                for y, score in scores.iteritems():\n",
    "                    visitor_results.append(\n",
    "                        (' '.join(target_region.title().replace('_',' ').split()),\n",
    "                         ' '.join(train_region.title().replace('_',' ').split()),\n",
    "                         model_name, score_func_name, y, score))\n",
    "visitor_df = pd.DataFrame(visitor_results, \n",
    "                          columns = ['TARGET_ECOREGION', 'TRAIN_ECOREGION', \n",
    "                                     'MODEL', 'METRIC', 'TARGET', 'SCORE'])\n",
    "visitor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_df.to_csv('../data/processed/nestedcv_chained_visitor_results_lidar_structure.csv', \n",
    "                  header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Outsider Models\n",
    "These models have data from the ecoregion they're tested on held out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsider_results = build_outsider_results_dictionary(ecoregions[:-1], MODELS.keys(), SCORE_FUNCS, Y_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_outsider_results(results):\n",
    "    data = []\n",
    "    for ecoregion in ecoregions[:-1]:\n",
    "        for target in Y_COLS:\n",
    "            for score_name in score_names:\n",
    "                data.append((np.nan, ecoregion, target, score_name, results[ecoregion][score_name][target]))\n",
    "    return pd.DataFrame(data, columns=['cv_fold', 'ecoregion', 'target', 'metric', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "----------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "Lasso\n",
      "-----\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "KNeighborsRegressor\n",
      "-------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n",
      "HistGradientBoostingRegressor\n",
      "-----------------------------\n",
      "Starting on BLUE MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on COAST RANGE... Done fitting, now scoring... All done.\n",
      "Starting on EASTERN CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on KLAMATH MOUNTAINS... Done fitting, now scoring... All done.\n",
      "Starting on NORTH CASCADES... Done fitting, now scoring... All done.\n",
      "Starting on NORTHERN ROCKIES... Done fitting, now scoring... All done.\n",
      "Starting on PUGET LOWLAND... Done fitting, now scoring... All done.\n",
      "Starting on WILLAMETTE VALLEY... Done fitting, now scoring... All done.\n"
     ]
    }
   ],
   "source": [
    "elastic_outsider = tune_outsider_model('ElasticNet')\n",
    "lasso_outsider = tune_outsider_model('Lasso')\n",
    "knn_outsider = tune_outsider_model('KNeighborsRegressor')\n",
    "rf_outsider = tune_outsider_model('RandomForestRegressor')\n",
    "gbm_outsider = tune_outsider_model('HistGradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_FOLD</th>\n",
       "      <th>ECOREGION</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>METRIC</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>MODEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>rmse</td>\n",
       "      <td>7.594567</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>nrmse</td>\n",
       "      <td>0.234315</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mae</td>\n",
       "      <td>5.788852</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>mape</td>\n",
       "      <td>0.178603</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue Mountains</td>\n",
       "      <td>total_cover</td>\n",
       "      <td>bias</td>\n",
       "      <td>-2.248044</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CV_FOLD       ECOREGION       TARGET METRIC     SCORE       MODEL\n",
       "0      NaN  Blue Mountains  total_cover   rmse  7.594567  ElasticNet\n",
       "1      NaN  Blue Mountains  total_cover  nrmse  0.234315  ElasticNet\n",
       "2      NaN  Blue Mountains  total_cover    mae  5.788852  ElasticNet\n",
       "3      NaN  Blue Mountains  total_cover   mape  0.178603  ElasticNet\n",
       "4      NaN  Blue Mountains  total_cover   bias -2.248044  ElasticNet"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_TO_CONCAT = [elastic_outsider, lasso_outsider, knn_outsider, rf_outsider, gbm_outsider]\n",
    "NAMES = ['ElasticNet', 'Lasso', 'kNN', 'RF', 'GBM']\n",
    "dfs_to_concat = []\n",
    "for res, name in zip(RESULTS_TO_CONCAT, NAMES):\n",
    "    tmp_df = parse_outsider_results(res)\n",
    "    tmp_df['model'] = name\n",
    "    dfs_to_concat.append(tmp_df)\n",
    "all_outsider_results = pd.concat(dfs_to_concat, axis=0, ignore_index=True)\n",
    "all_outsider_results['ecoregion'] = all_outsider_results['ecoregion'].apply(lambda x: ' '.join(x.title().replace('_',' ').split()[:2]))\n",
    "all_outsider_results.columns = [col.upper() for col in all_outsider_results.columns]\n",
    "all_outsider_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outsider_results.to_csv('../data/processed/nestedcv_chained_outsider_results_lidar_structure.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forest_mapping]",
   "language": "python",
   "name": "conda-env-forest_mapping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
